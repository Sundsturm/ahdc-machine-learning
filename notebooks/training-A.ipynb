{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0754acf-0680-4466-a089-e966f2a14c33",
   "metadata": {},
   "source": [
    "# training-A.ipynb\n",
    "1. This code is intended for training three models with *windowed features* data from MIT-BIH arrhythmia database and additional ECG data with a format of .bin\n",
    "2. Classes/labels: N, S, V, F, and/without Q\n",
    "3. The data only utilizes single-lead from MIT-BIH, which is MLII"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e32954",
   "metadata": {},
   "source": [
    "## **LIBRARY IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0b717-46b3-4967-962f-6d5f2eae8848",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import cudf\n",
    "import os\n",
    "import joblib\n",
    "import pywt\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb  # For reading MIT-BIH data\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import neurokit2 as nk\n",
    "\n",
    "# Scikit-learn and Imbalanced-learn imports\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "from scipy.signal import find_peaks, resample, butter, filtfilt, iirnotch, spectrogram\n",
    "from sklearn.utils import class_weight\n",
    "from glob import glob\n",
    "\n",
    "# Model imports\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Dropout, Add, GlobalAveragePooling1D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Additional setups\n",
    "# Checking cUML\n",
    "print(cudf.Series([1, 2, 3]))\n",
    "\n",
    "# Setting TensorFlow flags\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Checking GPU\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"TensorFlow has detected {len(gpu_devices)} GPU(s):\")\n",
    "    for device in gpu_devices:\n",
    "        print(f\"- {device}\")\n",
    "else:\n",
    "    print(\"TensorFlow did not detect any GPUs. Training will run on the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348ca32-352e-4791-a764-9a1dedd1af04",
   "metadata": {},
   "source": [
    "## **DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb05f79-645a-4ff3-b8f5-44e867b01a39",
   "metadata": {},
   "source": [
    "### DATA PREPARATION FUNCTIONS\n",
    "1. Database: MIT-BIH Arrhythmia Database & additional ECG data with a format of .bin files and from heartbeat simulator\n",
    "2. Preparation: *Windowed Features* of RR Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a11cb1-fc53-445e-933c-49f337cb0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "label_map = { 'N': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0,  # Normal Beats (N)\n",
    "              'V': 1, 'E': 1,                          # Ventricular Ectopic (VEB)\n",
    "              'S': 2, 'A': 2, 'a': 2, 'J': 2,          # Supraventricular Ectopic (SVEB)\n",
    "              'F': 3}                                  # Fusion Beat (F)\n",
    "\n",
    "# Membagi data menjadi sumbu X dan sumbu Y\n",
    "def create_windowed_features(rr_intervals, labels, window_size):\n",
    "    \"\"\"RR intervals features extraction\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(rr_intervals) - window_size):\n",
    "        segment = rr_intervals[i:i+window_size]\n",
    "        # Label sesuai dengan detak di akhir jendela\n",
    "        label = labels[i + window_size - 1]\n",
    "        X.append(segment)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2280f36-f477-42a6-ac58-db67d3b4157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mitbih_data(record_names, db_path):\n",
    "    \"\"\"\n",
    "    Loads ECG signals and annotations from MIT-BIH data.\n",
    "    Args:\n",
    "        record_names (list): List of record names (e.g., ['100', '101']).\n",
    "        db_path (str): Database directory on PhysioNet (e.g., 'mit-bih').\n",
    "\n",
    "    Return:\n",
    "        Returns the RAW ECG data from each record as a list of numpy arrays (signals),\n",
    "        and the annotations as a list of annotation objects.\n",
    "        tuple: (signals, annotations)\n",
    "    \"\"\"\n",
    "    signals, annotations = [], []\n",
    "    for record in record_names:\n",
    "        record_path = f'{db_path}/{record}'\n",
    "        # Read signal from the first channel (usually MLII)\n",
    "        signal = wfdb.rdrecord(record_path, channels=[0]).p_signal.flatten()\n",
    "        annotation = wfdb.rdann(record_path, 'atr')\n",
    "        signals.append(signal)\n",
    "        annotations.append(annotation)\n",
    "    return signals, annotations\n",
    "\n",
    "# Read annotations and labels from each loaded data\n",
    "def extract_rr_intervals_and_labels(annotations):\n",
    "    \"\"\"Extracts RR intervals and corresponding heartbeat labels.\"\"\"\n",
    "    all_rr, labels = [], []\n",
    "    for ann in annotations:\n",
    "        r_peaks = ann.sample\n",
    "        beat_symbols = ann.symbol\n",
    "        # Need at least two R-peaks to calculate an interval\n",
    "        for i in range(1, len(r_peaks)):\n",
    "            symbol = beat_symbols[i]\n",
    "            if symbol in label_map:\n",
    "                # Use the record-specific sampling frequency\n",
    "                rr_interval = (r_peaks[i] - r_peaks[i-1]) / ann.fs\n",
    "                all_rr.append(rr_interval)\n",
    "                labels.append(label_map[symbol])\n",
    "    return np.array(all_rr), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d55906-2b36-4dd9-b9db-dc6663f9a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ecg_from_bin(file_path, dtype=np.int16):\n",
    "    \"\"\"\n",
    "    Loads a raw ECG signal from a binary file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .bin file.\n",
    "        dtype (numpy.dtype): Data type of the signal in the binary file.\n",
    "\n",
    "    Return:\n",
    "        numpy.ndarray: The ECG signal as a numpy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the binary file and convert it into a numpy array\n",
    "        signal = np.fromfile(file_path, dtype=dtype)\n",
    "        print(f\"Successfully read {len(signal)} samples from {file_path}\")\n",
    "        return signal\n",
    "    except IOError as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_r_peaks(signal, fs):\n",
    "    \"\"\"\n",
    "    Detects R-peaks from an ECG signal.\n",
    "\n",
    "    Args:\n",
    "        signal (numpy.ndarray): The raw ECG signal.\n",
    "        fs (int): The sampling frequency of the signal.\n",
    "\n",
    "    Return:\n",
    "        numpy.ndarray: An array containing the locations (indices) of the detected R-peaks.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 1: Detecting R-Peaks ---\")\n",
    "    # The 'height' and 'distance' parameters can be adjusted for your signal\n",
    "    height_threshold = np.max(signal) * 0.5\n",
    "    distance_threshold = fs * 0.4  # Minimum distance between beats\n",
    "\n",
    "    r_peaks, _ = find_peaks(signal, height=height_threshold, distance=distance_threshold)\n",
    "\n",
    "    print(f\"Detected {len(r_peaks)} R-peaks.\")\n",
    "    return r_peaks\n",
    "\n",
    "def extract_rr_and_apply_label_ecg_bin(r_peaks, fs, record_label):\n",
    "    \"\"\"\n",
    "    Calculates RR intervals from a single record and assigns the same label\n",
    "    to all of them.\n",
    "\n",
    "    Args:\n",
    "        r_peaks (numpy.ndarray): Array of R-peak locations (in sample indices) from a single record.\n",
    "        fs (int): The sampling frequency of the signal.\n",
    "        record_label (any): A single label (e.g., string or integer) to be\n",
    "                            applied to this entire record.\n",
    "\n",
    "    Return:\n",
    "        tuple: A tuple containing (rr_intervals, labels).\n",
    "               - rr_intervals (numpy.ndarray): Array of RR intervals in seconds.\n",
    "               - labels (numpy.ndarray): Array containing the same label for each RR interval.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Step 2: RR Extraction and Labeling for the Record ---\")\n",
    "\n",
    "    # Ensure there are enough R-peaks to calculate at least one interval\n",
    "    if len(r_peaks) < 2:\n",
    "        print(\"Warning: Not enough R-peaks to calculate RR intervals.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Calculate all RR intervals in seconds\n",
    "    rr_intervals = np.diff(r_peaks) / fs\n",
    "\n",
    "    # Create a label array where each element is 'record_label'\n",
    "    # The size of this label array is the same as the number of calculated RR intervals\n",
    "    num_rr_intervals = len(rr_intervals)\n",
    "    labels = np.full(shape=num_rr_intervals, fill_value=record_label)\n",
    "\n",
    "    return rr_intervals, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c0215-637d-49cd-8049-379035e45e3f",
   "metadata": {},
   "source": [
    "### DATA PREPARATION EXECUTION\n",
    "1. For .bin data, the program will detect R-peaks first, create RR interval with those R-peaks, and then create windowed features that has multiple RR intervals for each window\n",
    "2. For MIT-BIH data, the program only read labels and annotations from the ECG data and then create windowed features that has multiple RR intervals for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82e85c-06ca-4dd4-9d1b-cb0b177484bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # --- 0. INITIAL PARAMETERS FOR .BIN AND MIT-BIH DATA PREPARATION ---\n",
    "    mitbih_dir = '../data/raw/MIT-BIH/mit-bih-arrhythmia-database-1.0.0/mit-bih-arrhythmia-database-1.0.0/'\n",
    "    window_size = 10\n",
    "    \n",
    "    # DS1 is used for training\n",
    "    ds1 = ['101', '106', '108', '109', '112', '114', '115', '116', '118', '119',\n",
    "           '122', '124', '201', '203', '205', '207', '208', '209', '215', '220',\n",
    "           '223', '230'] \n",
    "    # DS2 is used for evaluation\n",
    "    ds2 = ['100', '103', '105', '111', '113', '117', '121', '123', '200', '202',\n",
    "           '210', '212', '213', '214', '219', '221', '222', '228', '231', '232',\n",
    "           '233', '234'] \n",
    "           \n",
    "    FS_CUSTOM = 500  # IMPORTANT: Adjust to the sampling frequency of your .bin data\n",
    "    custom_file_paths = {\n",
    "        'Arrhythmia': '../data/raw/Arrhythmia/ECG_WAVE.bin',\n",
    "        'Normal': '../data/raw/Normal/ecg_normal.bin'\n",
    "    }\n",
    "    custom_file_labels = {'Arrhythmia': 2, 'Normal': 0}\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"ðŸš€ STARTING DATASET PREPARATION PROCESS ðŸš€\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- 1. PROCESS TRAINING DATA (ds1) ---\n",
    "    print(\"\\n--- [Step 1/5] Processing Training Data (ds1) ---\")\n",
    "    signals_train, annotations_train = load_mitbih_data(ds1, mitbih_dir)\n",
    "    rr_train, labels_train = extract_rr_intervals_and_labels(annotations_train)\n",
    "    X_train, y_train = create_windowed_features(rr_train, labels_train, window_size)\n",
    "    print(f\"Raw training data ready: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "\n",
    "    # --- 2. PROCESS TESTING DATA (Combination of ds2 and .bin) ---\n",
    "    print(\"\\n--- [Step 2/5] Processing Testing Data ---\")\n",
    "\n",
    "    # Part A: Process testing data from MIT-BIH (ds2)\n",
    "    print(\"\\nProcessing testing part 1 (ds2)...\")\n",
    "    signals_test_mitbih, annotations_test_mitbih = load_mitbih_data(ds2, mitbih_dir)\n",
    "    rr_test_mitbih, labels_test_mitbih = extract_rr_intervals_and_labels(annotations_test_mitbih)\n",
    "    X_test_mitbih, y_test_mitbih = create_windowed_features(rr_test_mitbih, labels_test_mitbih, window_size)\n",
    "    print(f\"MIT-BIH testing data ready: X_test_mitbih={X_test_mitbih.shape}, y_test_mitbih={y_test_mitbih.shape}\")\n",
    "\n",
    "    # Part B: Process testing data from .bin files\n",
    "    print(\"\\nProcessing testing part 2 (.bin)...\")\n",
    "    all_rr_custom = []\n",
    "    all_labels_custom = []\n",
    "    for category, path in custom_file_paths.items():\n",
    "        signal_custom = load_ecg_from_bin(path)\n",
    "        if signal_custom is not None:\n",
    "            r_peaks_custom = detect_r_peaks(signal_custom, fs=FS_CUSTOM)\n",
    "            rr_intervals_c, labels_c = extract_rr_and_apply_label_ecg_bin(\n",
    "                r_peaks_custom, fs=FS_CUSTOM, record_label=custom_file_labels[category]\n",
    "            )\n",
    "            all_rr_custom.append(rr_intervals_c)\n",
    "            all_labels_custom.append(labels_c)\n",
    "\n",
    "    rr_test_custom = np.concatenate(all_rr_custom)\n",
    "    labels_test_custom = np.concatenate(all_labels_custom)\n",
    "    X_test_custom, y_test_custom = create_windowed_features(rr_test_custom, labels_test_custom, window_size)\n",
    "\n",
    "    # --- 3. SCALING & COMBINING TESTING DATA ---\n",
    "    print(\"\\n--- [Step 3/5] Scaling and Finalizing Data ---\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    print(\"Scaler trained on training data.\")\n",
    "\n",
    "    # Apply the scaler to all parts of the testing data\n",
    "    X_test_mitbih_scaled = scaler.transform(X_test_mitbih)\n",
    "    X_test_custom_scaled = scaler.transform(X_test_custom)\n",
    "    print(\"Scaler applied to all testing data.\")\n",
    "\n",
    "    # Combine all scaled testing data\n",
    "    X_test_final = np.concatenate((X_test_mitbih_scaled, X_test_custom_scaled), axis=0)\n",
    "    y_test_final = np.concatenate((y_test_mitbih, y_test_custom), axis=0)\n",
    "    print(f\"Final testing data combined: X_test_final={X_test_final.shape}, y_test_final={y_test_final.shape}\")\n",
    "\n",
    "    # --- 4. TRAINING SET SPLIT & OVERSAMPLING (SMOTE) ---\n",
    "    print(\"\\n--- [Step 4/5] Finalizing Training Data (Split & SMOTE) ---\")\n",
    "    print(\"Creating validation set from training data (80/20)...\")\n",
    "    X_train_fold, X_val, y_train_fold, y_val = train_test_split(\n",
    "        X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    print(\"Applying SMOTE only to the training fold...\")\n",
    "    print(\"Training class distribution before SMOTE:\", Counter(y_train_fold))\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "    print(\"Training class distribution after SMOTE:\", Counter(y_train_resampled))\n",
    "\n",
    "    # --- 5. FINAL DATA PREPARATION FOR MODELS ---\n",
    "    print(\"\\n--- [Step 5/5] Preparing Final Datasets for Models ---\")\n",
    "\n",
    "    # Define output_dim based on the number of unique classes in the original training data\n",
    "    output_dim = len(np.unique(y_train))\n",
    "\n",
    "    # One-hot encode labels for Keras\n",
    "    y_train_encoded = to_categorical(y_train_resampled, num_classes=output_dim)\n",
    "    y_val_encoded = to_categorical(y_val, num_classes=output_dim)\n",
    "    y_test_final_encoded = to_categorical(y_test_final, num_classes=output_dim)\n",
    "\n",
    "    # ðŸ§  Data for MLP\n",
    "    X_train_mlp, y_train_mlp = X_train_resampled, y_train_encoded\n",
    "    X_val_mlp, y_val_mlp = X_val, y_val_encoded\n",
    "    X_test_mlp, y_test_mlp = X_test_final, y_test_final_encoded\n",
    "\n",
    "    # âš¡ Data for 1D-CNN\n",
    "    X_train_cnn = X_train_mlp.reshape((X_train_mlp.shape[0], X_train_mlp.shape[1], 1))\n",
    "    X_val_cnn = X_val_mlp.reshape((X_val_mlp.shape[0], X_val_mlp.shape[1], 1))\n",
    "    X_test_cnn = X_test_mlp.reshape((X_test_mlp.shape[0], X_test_mlp.shape[1], 1))\n",
    "    y_train_cnn, y_val_cnn, y_test_cnn = y_train_mlp, y_val_mlp, y_test_mlp\n",
    "\n",
    "    # ðŸ“Š Data for RandomForest\n",
    "    X_train_rf, y_train_rf = X_train_resampled, y_train_resampled\n",
    "    X_val_rf, y_val_rf = X_val, y_val\n",
    "    X_test_rf, y_test_rf = X_test_final, y_test_final\n",
    "\n",
    "    # --- FINAL RESULTS ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… DATA PREPARATION COMPLETE âœ…\")\n",
    "    print(\"The following variables are ready for training and evaluation:\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"\\n--- For MLP ---\")\n",
    "    print(f\"  Training:   X_train_mlp: {X_train_mlp.shape}, y_train_mlp: {y_train_mlp.shape}\")\n",
    "    print(f\"  Validation: X_val_mlp: {X_val_mlp.shape}, y_val_mlp: {y_val_mlp.shape}\")\n",
    "    print(f\"  Testing:    X_test_mlp: {X_test_mlp.shape}, y_test_mlp: {y_test_mlp.shape}\")\n",
    "\n",
    "    print(\"\\n--- For 1D-CNN ---\")\n",
    "    print(f\"  Training:   X_train_cnn: {X_train_cnn.shape}, y_train_cnn: {y_train_cnn.shape}\")\n",
    "    print(f\"  Validation: X_val_cnn: {X_val_cnn.shape}, y_val_cnn: {y_val_cnn.shape}\")\n",
    "    print(f\"  Testing:    X_test_cnn: {X_test_cnn.shape}, y_test_cnn: {y_test_cnn.shape}\")\n",
    "\n",
    "    print(\"\\n--- For RandomForest ---\")\n",
    "    print(f\"  Training:   X_train_rf: {X_train_rf.shape}, y_train_rf: {y_train_rf.shape}\")\n",
    "    print(f\"  Validation: X_val_rf: {X_val_rf.shape}, y_val_rf: {y_val_rf.shape}\")\n",
    "    print(f\"  Testing:    X_test_rf: {X_test_rf.shape}, y_test_rf: {y_test_rf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162fbb6-179a-483f-b663-69c1b64dc040",
   "metadata": {},
   "source": [
    "## **MACHINE LEARNING MODEL TRAINING & SAVING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04aa916-ef83-4e8f-ae5d-80d0a49a2129",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28664a07-7fce-4c07-b780-2ffac2105947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_dim, output_dim):\n",
    "    \"\"\"Creates and compiles a Keras MLP model.\"\"\"\n",
    "    model = Sequential([\n",
    "        # Hyperparameters tuning\n",
    "        Dense(512, input_dim=input_dim, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(output_dim, activation='softmax') # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy', # Suitable for one-hot labels\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            # tf.keras.metrics.Precision(name='precision'),\n",
    "            # tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "# 1st CNN model\n",
    "def create_cnn_model(input_shape, output_dim):\n",
    "    \"\"\"Creates and compiles a Keras 1D-CNN model.\"\"\"\n",
    "    # Input shape for CNN must be 3D: (samples, steps, features)\n",
    "    # Example: (10000, 187, 1)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=512, kernel_size=6, activation='relu', # Reduced filters\n",
    "               input_shape=input_shape),\n",
    "        Dropout(0.1),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        Conv1D(filters=512, kernel_size=3, activation='relu'), # Reduced filters\n",
    "        Dropout(0.2),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        Flatten(), # Now flattens a much smaller tensor\n",
    "\n",
    "        Dense(512, activation='relu'), # Reduced dense units\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "# 1D-CNN optimized based on paper\n",
    "def create_cnn_model_optimized(input_shape, output_dim, hp=None):\n",
    "    \"\"\"\n",
    "    Creates and compiles an optimized 1D-CNN model.\n",
    "    If 'hp' is provided, it builds a tunable model for KerasTuner.\n",
    "    Otherwise, it builds a model with default hyperparameters.\n",
    "    \"\"\"\n",
    "    # Define a default hyperparameter object if none is passed\n",
    "    if hp is None:\n",
    "        hp = kt.HyperParameters()\n",
    "        # Set default values for when not tuning\n",
    "        hp.values['conv4_filters'] = 100\n",
    "        hp.values['dense_units'] = 256\n",
    "        hp.values['learning_rate'] = 0.0001\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='valid', name='conv1d_1_freezed')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_1_freezed')(x)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', name='conv1d_2_freezed')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_2_freezed')(x)\n",
    "    x = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same', name='conv1d_3_freezed')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_3_freezed')(x)\n",
    "    # ===============================================\n",
    "    #           Trainable Layers\n",
    "    # ===============================================\n",
    "    x = Conv1D(filters=hp.values['conv4_filters'], kernel_size=3, activation='relu', padding='same', name='conv1d_4_trainable')(x)\n",
    "    x = Flatten(name='flatten_layer')(x)\n",
    "    x = Dense(units=hp.values['dense_units'], activation='relu', name='dense_1_trainable')(x)\n",
    "    outputs = Dense(units=output_dim, activation='softmax', name='output_layer_trainable')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.values['learning_rate']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 1D-ResNet\n",
    "# def create_cnn_model_optimized(input_shape, output_dim, hp=None):\n",
    "#     \"\"\"\n",
    "#     Creates and compiles an optimized 1D-CNN model.\n",
    "#     If 'hp' is provided, it builds a tunable model for KerasTuner.\n",
    "#     Otherwise, it builds a model with default hyperparameters.\n",
    "#     \"\"\"\n",
    "#     # Define a default hyperparameter object if none is passed\n",
    "#     if hp is None:\n",
    "#         hp = kt.HyperParameters()\n",
    "#         # Set default values for when not tuning\n",
    "#         hp.values['initial_filters'] = 384\n",
    "#         hp.values['res_block_1_filters'] = 384\n",
    "#         hp.values['res_block_2_filters'] = 384\n",
    "#         hp.values['kernel_size_initial'] = 7\n",
    "#         hp.values['kernel_size_res'] = 5\n",
    "#         hp.values['dropout_1'] = 0.1\n",
    "#         hp.values['dropout_2'] = 0.3\n",
    "#         hp.values['dense_units'] = 512\n",
    "#         hp.values['dense_dropout'] = 0.5\n",
    "#         hp.values['learning_rate'] = 0.0001\n",
    "\n",
    "#     def residual_block(x, filters, kernel_size):\n",
    "#         y = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "#         y = BatchNormalization()(y)\n",
    "#         y = Activation('relu')(y)\n",
    "#         y = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(y)\n",
    "#         y = BatchNormalization()(y)\n",
    "#         shortcut = Conv1D(filters=filters, kernel_size=1, padding='same')(x) if x.shape[-1] != filters else x\n",
    "#         res_output = Add()([shortcut, y])\n",
    "#         return Activation('relu')(res_output)\n",
    "\n",
    "#     inputs = Input(shape=input_shape)\n",
    "#     x = Conv1D(filters=hp.values['initial_filters'], kernel_size=hp.values['kernel_size_initial'], padding='same')(inputs)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "#     x = residual_block(x, filters=hp.values['res_block_1_filters'], kernel_size=hp.values['kernel_size_res'])\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "#     x = Dropout(hp.values['dropout_1'])(x)\n",
    "#     x = residual_block(x, filters=hp.values['res_block_2_filters'], kernel_size=hp.values['kernel_size_res'])\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "#     x = Dropout(hp.values['dropout_2'])(x)\n",
    "#     x = GlobalAveragePooling1D()(x)\n",
    "#     x = Dense(hp.values['dense_units'], activation='relu')(x)\n",
    "#     x = Dropout(hp.values['dense_dropout'])(x)\n",
    "#     outputs = Dense(output_dim, activation='softmax')(x)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=hp.values['learning_rate']),\n",
    "#         loss='categorical_crossentropy',\n",
    "#         metrics=[\n",
    "#             # 'accuracy',\n",
    "#             tf.keras.metrics.Precision(name='precision'),\n",
    "#             tf.keras.metrics.Recall(name='recall'),\n",
    "#             # Note: F1Score might require a different setup in some TF versions.\n",
    "#             # If it causes issues, consider a custom callback to calculate it.\n",
    "#             tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "#             tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "#         ]\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# Function to create the RandomForest model\n",
    "def create_rf_model():\n",
    "    \"\"\"Creates an instance of the GPU-accelerated RandomForestClassifier model using cuML.\"\"\"\n",
    "    # Hyperparameters are similar to imblearn's\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=30, \n",
    "        random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16e19d-a20a-4308-9848-37de070b26a3",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL TRAINING EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2cd67-b661-4d6b-a3fc-d87dd454416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Multiple Models\n",
    "input_shape_cnn = (X_train_cnn.shape[1], X_train_cnn.shape[2])\n",
    "input_dim = X_train_mlp.shape[1]\n",
    "output_dim = y_train_mlp.shape[1]\n",
    "\n",
    "# Saving/exporting models\n",
    "output_dir = '../models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- MANUAL CLASS WEIGHT CALCULATION ---\n",
    "print(\"--- Manually Calculating Class Weights for Cost-Sensitive Learning ---\")\n",
    "# Ensure y_train_fold contains single integer labels (e.g., [0, 1, 2, 0, ...])\n",
    "# not one-hot encoded vectors.\n",
    "if y_train_fold.ndim > 1 and y_train_fold.shape[1] > 1:\n",
    "    y_labels = np.argmax(y_train_fold, axis=1)\n",
    "else:\n",
    "    y_labels = y_train_fold.flatten() # Ensure it's a 1D array\n",
    "# Count the number of samples in each class.\n",
    "class_counts = np.bincount(y_labels)\n",
    "total_samples = len(y_labels)\n",
    "num_classes = len(np.unique(y_labels))\n",
    "# Calculate weight for each class using the formula:\n",
    "# weight = total_samples / (num_classes * count_for_that_class)\n",
    "class_weights = total_samples / (num_classes * class_counts)\n",
    "# Create the dictionary required by Keras and Scikit-learn.\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Manually Calculated Weights:\", class_weights_dict)\n",
    "\n",
    "models = {\n",
    "    \"1D-CNN\": create_cnn_model_optimized(input_shape_cnn, output_dim),\n",
    "    \"RandomForest\": create_rf_model(),\n",
    "    \"MLP\": create_mlp_model(input_dim, output_dim)\n",
    "}\n",
    "\n",
    "# Dictionary to store the final results\n",
    "results = {}\n",
    "\n",
    "# --- TRAINING AND EVALUATING EACH MODEL ---\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*20} TRAINING MODEL: {name} {'='*20}\")\n",
    "\n",
    "    # ðŸ§  Training\n",
    "    if name == \"1D-CNN\":\n",
    "        model.fit(\n",
    "            X_train_cnn, y_train_cnn,\n",
    "            epochs=150, # Reduced for quick example\n",
    "            batch_size=100,\n",
    "            verbose=1, # Set to 0 to keep output clean\n",
    "            validation_data=(X_val_cnn, y_val_cnn),\n",
    "            class_weight=class_weights_dict\n",
    "        )\n",
    "    elif name == \"MLP\":\n",
    "        model.fit(\n",
    "            X_train_mlp, y_train_mlp,\n",
    "            epochs=150, # Reduced for quick example\n",
    "            batch_size=100,\n",
    "            verbose=1,\n",
    "            validation_data=(X_val_mlp, y_val_mlp),\n",
    "            class_weight=class_weights_dict\n",
    "        )\n",
    "    else: # ðŸ“Š RandomForest\n",
    "        model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "    # âš¡ Prediction on the Test Set\n",
    "    print(f\"Evaluating model {name}...\")\n",
    "    if name in [\"MLP\", \"1D-CNN\"]:\n",
    "        y_pred_raw = model.predict(X_test_mlp if name == \"MLP\" else X_test_cnn)\n",
    "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "    else: # RandomForest\n",
    "        y_pred = model.predict(X_test_rf)\n",
    "\n",
    "    # Store prediction results and ground truth for final evaluation\n",
    "    results[name] = {'y_pred': y_pred, 'y_true': y_test_final}\n",
    "\n",
    "# --- PRINT ALL RESULTS SIMULTANEOUSLY ---\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)']\n",
    "\n",
    "print(f\"\\n{'='*25} FINAL EVALUATION RESULTS {'='*25}\")\n",
    "\n",
    "for name, result_data in results.items():\n",
    "    y_true = result_data['y_true']\n",
    "    y_pred = result_data['y_pred']\n",
    "\n",
    "    print(f\"\\n\\n{'~'*15} REPORT FOR MODEL: {name} {'~'*15}\")\n",
    "    \n",
    "    # --- Classification Report ---\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    # --- Confusion Matrix Visualization ---\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix for {name}', fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    # SAVING MODEL AFTER TRAINING\n",
    "    print(f\"--- Saving model: {name} ---\")\n",
    "    if name in [\"1D-CNN\", \"MLP\"]:\n",
    "        # TensorFlow/Keras models\n",
    "        model_path = os.path.join(output_dir, f\"model_{name.lower()}_saved\")\n",
    "        model.export(model_path) # Saving models\n",
    "        print(f\"âœ… Model {name} has been saved on: {model_path}\")\n",
    "    else: #RandomForest/other scikit-learn models\n",
    "        model_path = os.path.join(output_dir, f\"model_{name.lower()}.joblib\")\n",
    "        joblib.dump(model, model_path) # Saving models\n",
    "        print(f\"âœ… Model {name} has been saved on: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f6a7b-b10a-4e43-89be-2dcaa76b0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Ten-Fold Cross Validation --\n",
    "# --- [Step 1] Combine Pre-processed Data for Cross-Validation ---\n",
    "print(\"--- Combining pre-processed training and validation sets for CV ---\")\n",
    "\n",
    "# Combine the feature sets for MLP/RandomForest\n",
    "X_cv_features = np.concatenate((X_train_rf, X_val_rf), axis=0)\n",
    "\n",
    "# Combine the raw/reshaped data for CNN\n",
    "X_cv_cnn = np.concatenate((X_train_cnn, X_val_cnn), axis=0)\n",
    "\n",
    "# Combine the 1D integer labels. StratifiedKFold needs this format.\n",
    "y_cv_labels = np.concatenate((y_train_rf, y_val_rf), axis=0)\n",
    "\n",
    "print(f\"Total data for Cross-Validation (Features): {X_cv_features.shape}\")\n",
    "print(f\"Total data for Cross-Validation (CNN): {X_cv_cnn.shape}\")\n",
    "print(f\"Total labels for Cross-Validation: {y_cv_labels.shape}\")\n",
    "\n",
    "# --- [Step 2] 10-Fold Cross-Validation Training and Evaluation ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- Starting 10-Fold CV on Pre-Balanced Data ---\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "unique_classes = np.unique(y_cv_labels)\n",
    "output_dim = len(unique_classes)\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)']\n",
    "\n",
    "results = {\n",
    "    'RandomForest': {'accuracy': [], 'recall_macro': [], 'f1_macro': [], 'y_true': [], 'y_pred': []},\n",
    "    'MLP': {'accuracy': [], 'recall_macro': [], 'f1_macro': [], 'y_true': [], 'y_pred': []},\n",
    "    '1D-CNN': {'accuracy': [], 'recall_macro': [], 'f1_macro': [], 'y_true': [], 'y_pred': []}\n",
    "}\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X_cv_features, y_cv_labels)):\n",
    "    print(f\"\\n--- FOLD {fold+1}/{n_splits} ---\")\n",
    "    \n",
    "    X_train_feat_fold, X_val_feat_fold = X_cv_features[train_index], X_cv_features[val_index]\n",
    "    y_train_fold, y_val_fold = y_cv_labels[train_index], y_cv_labels[val_index]\n",
    "    X_train_cnn_fold, X_val_cnn_fold = X_cv_cnn[train_index], X_cv_cnn[val_index]\n",
    "\n",
    "    # --- RandomForest Training & Evaluation ---\n",
    "    print(\"\\nTraining RandomForest...\")\n",
    "    rf_model = create_rf_model()\n",
    "    rf_model.fit(X_train_feat_fold, y_train_fold)\n",
    "    y_pred_rf = rf_model.predict(X_val_feat_fold)\n",
    "    results['RandomForest']['accuracy'].append(accuracy_score(y_val_fold, y_pred_rf))\n",
    "    results['RandomForest']['recall_macro'].append(recall_score(y_val_fold, y_pred_rf, average='macro', zero_division=0))\n",
    "    results['RandomForest']['f1_macro'].append(f1_score(y_val_fold, y_pred_rf, average='macro', zero_division=0))\n",
    "    results['RandomForest']['y_true'].extend(y_val_fold)\n",
    "    results['RandomForest']['y_pred'].extend(y_pred_rf)\n",
    "    print(f\"RandomForest Fold {fold+1} Accuracy: {results['RandomForest']['accuracy'][-1]:.4f}\")\n",
    "\n",
    "    # --- MLP Training & Evaluation ---\n",
    "    print(\"\\nTraining MLP...\")\n",
    "    y_train_mlp_cat = to_categorical(y_train_fold, num_classes=output_dim)\n",
    "    mlp_model = create_mlp_model(input_dim=X_train_feat_fold.shape[1], output_dim=output_dim)\n",
    "    mlp_model.fit(X_train_feat_fold, y_train_mlp_cat, epochs=100, batch_size=200, verbose=0)\n",
    "    y_pred_mlp_prob = mlp_model.predict(X_val_feat_fold)\n",
    "    y_pred_mlp = np.argmax(y_pred_mlp_prob, axis=1)\n",
    "    results['MLP']['accuracy'].append(accuracy_score(y_val_fold, y_pred_mlp))\n",
    "    results['MLP']['recall_macro'].append(recall_score(y_val_fold, y_pred_mlp, average='macro', zero_division=0))\n",
    "    results['MLP']['f1_macro'].append(f1_score(y_val_fold, y_pred_mlp, average='macro', zero_division=0))\n",
    "    results['MLP']['y_true'].extend(y_val_fold)\n",
    "    results['MLP']['y_pred'].extend(y_pred_mlp)\n",
    "    print(f\"MLP Fold {fold+1} Accuracy: {results['MLP']['accuracy'][-1]:.4f}\")\n",
    "\n",
    "    # --- 1D-CNN Training & Evaluation ---\n",
    "    print(\"\\nTraining 1D-CNN...\")\n",
    "    y_train_cnn_cat = to_categorical(y_train_fold, num_classes=output_dim)\n",
    "    cnn_input_shape = (X_train_cnn_fold.shape[1], 1)\n",
    "    cnn_model = create_cnn_model_optimized(input_shape=cnn_input_shape, output_dim=output_dim)\n",
    "    cnn_model.fit(X_train_cnn_fold, y_train_cnn_cat, epochs=150, batch_size=100, verbose=0)\n",
    "    y_pred_cnn_prob = cnn_model.predict(X_val_cnn_fold)\n",
    "    y_pred_cnn = np.argmax(y_pred_cnn_prob, axis=1)\n",
    "    results['1D-CNN']['accuracy'].append(accuracy_score(y_val_fold, y_pred_cnn))\n",
    "    results['1D-CNN']['recall_macro'].append(recall_score(y_val_fold, y_pred_cnn, average='macro', zero_division=0))\n",
    "    results['1D-CNN']['f1_macro'].append(f1_score(y_val_fold, y_pred_cnn, average='macro', zero_division=0))\n",
    "    results['1D-CNN']['y_true'].extend(y_val_fold)\n",
    "    results['1D-CNN']['y_pred'].extend(y_pred_cnn)\n",
    "    print(f\"1D-CNN Fold {fold+1} Accuracy: {results['1D-CNN']['accuracy'][-1]:.4f}\")\n",
    "\n",
    "# --- [Step 3] Final Model Training and Saving ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- Training and Saving Final Models on All Data ---\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "output_dir = '../models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Train and Save RandomForest\n",
    "print(\"\\nTraining final RandomForest model...\")\n",
    "final_rf_model = create_rf_model()\n",
    "final_rf_model.fit(X_cv_features, y_cv_labels)\n",
    "rf_path = os.path.join(output_dir, \"final_randomforest_model.joblib\")\n",
    "joblib.dump(final_rf_model, rf_path)\n",
    "print(f\"âœ… RandomForest model saved to: {rf_path}\")\n",
    "\n",
    "# Prepare labels for Keras models\n",
    "y_cv_keras = to_categorical(y_cv_labels, num_classes=output_dim)\n",
    "\n",
    "# 2. Train and Save MLP\n",
    "print(\"\\nTraining final MLP model...\")\n",
    "final_mlp_model = create_mlp_model(input_dim=X_cv_features.shape[1], output_dim=output_dim)\n",
    "final_mlp_model.fit(X_cv_features, y_cv_keras, epochs=10, batch_size=128, verbose=0)\n",
    "mlp_path = os.path.join(output_dir, \"final_mlp_model.keras\")\n",
    "final_mlp_model.save(mlp_path)\n",
    "print(f\"âœ… MLP model saved to: {mlp_path}\")\n",
    "\n",
    "# 3. Train and Save 1D-CNN\n",
    "print(\"\\nTraining final 1D-CNN model...\")\n",
    "cnn_input_shape = (X_cv_cnn.shape[1], 1)\n",
    "final_cnn_model = create_cnn_model_optimized(input_shape=cnn_input_shape, output_dim=output_dim)\n",
    "final_cnn_model.fit(X_cv_cnn, y_cv_keras, epochs=10, batch_size=128, verbose=0)\n",
    "cnn_path = os.path.join(output_dir, \"final_1d_cnn_model.keras\")\n",
    "final_cnn_model.save(cnn_path)\n",
    "print(f\"âœ… 1D-CNN model saved to: {cnn_path}\")\n",
    "\n",
    "\n",
    "# --- [Step 4] Final Results Summary ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- CROSS-VALIDATION SUMMARY ---\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for model_name, model_results in results.items():\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    for metric in ['accuracy', 'recall_macro', 'f1_macro']:\n",
    "        avg_metric = np.mean(model_results[metric])\n",
    "        std_metric = np.std(model_results[metric])\n",
    "        print(f\"Average {metric.replace('_', ' ').title()}: {avg_metric:.4f} (+/- {std_metric:.4f})\")\n",
    "    \n",
    "    print(\"\\nAggregated Classification Report:\")\n",
    "    aggregated_report = classification_report(\n",
    "        model_results['y_true'], \n",
    "        model_results['y_pred'], \n",
    "        target_names=class_names,\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(aggregated_report)\n",
    "\n",
    "    print(\"Aggregated Confusion Matrix:\")\n",
    "    cm = confusion_matrix(model_results['y_true'], model_results['y_pred'], labels=unique_classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f'{model_name} - Aggregated Confusion Matrix')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
