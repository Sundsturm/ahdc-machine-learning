{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955d89d0-8af4-42bb-af40-65a1f68e54b9",
   "metadata": {},
   "source": [
    "# training-D.ipynb\n",
    "1. This code is intended for training, validating, and testing 1D-CNN model only\n",
    "2. Prepared data is raw ECG data from MIT-BIH and additional ECG data from .bin format\n",
    "3. There are 4 classes/labels: N, V, S, and F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e32954",
   "metadata": {},
   "source": [
    "## **LIBRARY IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0b717-46b3-4967-962f-6d5f2eae8848",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import cudf\n",
    "import os\n",
    "import joblib\n",
    "import pywt\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb  # For reading MIT-BIH data\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import neurokit2 as nk\n",
    "\n",
    "# Scikit-learn and Imbalanced-learn imports\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "from scipy.signal import find_peaks, resample, butter, filtfilt, iirnotch, spectrogram\n",
    "from sklearn.utils import class_weight\n",
    "from glob import glob\n",
    "\n",
    "# Model imports\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Dropout, Add, GlobalAveragePooling1D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Additional setups\n",
    "# Checking cUML\n",
    "print(cudf.Series([1, 2, 3]))\n",
    "\n",
    "# Setting TensorFlow flags\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Checking GPU\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"TensorFlow has detected {len(gpu_devices)} GPU(s):\")\n",
    "    for device in gpu_devices:\n",
    "        print(f\"- {device}\")\n",
    "else:\n",
    "    print(\"TensorFlow did not detect any GPUs. Training will run on the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348ca32-352e-4791-a764-9a1dedd1af04",
   "metadata": {},
   "source": [
    "## **DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb05f79-645a-4ff3-b8f5-44e867b01a39",
   "metadata": {},
   "source": [
    "### DATA PREPARATION FUNCTIONS\n",
    "1. prepare_cnn_data\n",
    "* Purpose: This function is the core processor for the MIT-BIH Arrhythmia Database. It takes the raw ECG signals and their corresponding annotations to generate labeled, windowed segments of the signal, which serve as the direct input for the 1D-CNN.\n",
    "* Key Steps:\n",
    "    1) Signal Denoising: It first applies the preprocess_signal function to remove noise like baseline wander and powerline interference.\n",
    "    2) Resampling: The signal's frequency is resampled from the original 360 Hz to a target of 500 Hz to standardize the data.\n",
    "    3) R-Peak Alignment: It adjusts the locations of the R-peaks (the most prominent part of a heartbeat) to match the new, higher sampling rate.\n",
    "    4) Windowing: For each annotated heartbeat, it extracts a fixed-size window of the signal (400 samples, or 0.8 seconds). The R-peak is used as the central anchor point for this window.\n",
    "    5) Labeling: Each extracted window is assigned a numerical label based on the AAMI class mapping (e.g., 'N' for Normal becomes 0).\n",
    "* Output: The function returns two arrays: one containing the raw signal windows (the data, or X) and another containing their corresponding numerical labels (the targets, or y).\n",
    "\n",
    "2. prepare_cnn_data_from_bin\n",
    "* Purpose: This function is designed to process ECG data from custom binary (.bin) files. It mirrors the functionality of prepare_cnn_data but is adapted for datasets that do not have pre-existing annotation files like the MIT-BIH database.\n",
    "* Key Steps:\n",
    "    1) Signal Denoising: Just like the previous function, it cleans the raw signal to ensure quality.\n",
    "    2) R-Peak Detection: Since no annotation file is available, it uses the detect_r_peaks_robust function to automatically identify the R-peak locations in the signal.\n",
    "    3) Windowing: It segments the signal into windows around each detected R-peak.\n",
    "    4) Uniform Labeling: Unlike the MIT-BIH function, all windows extracted from a single .bin file are assigned the same label, which is provided as an argument to the function (e.g., all beats from ecg_normal.bin are labeled as 0).\n",
    "* Output: It returns arrays of raw signal windows and their corresponding labels, ready to be used as a final testing set.\n",
    "3. Database: MIT-BIH Arrhythmia Database & additional ECG data with a format of .bin files and from heartbeat simulator\n",
    "4. Preparation: Raw data for 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d55906-2b36-4dd9-b9db-dc6663f9a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# AAMI Class Mapping\n",
    "label_map = {\n",
    "    'N': 0, '.': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0,  # Class 0: Normal Beats (N)\n",
    "    'V': 1, 'E': 1,                                  # Class 1: Ventricular Ectopic (VEB)\n",
    "    'S': 2, 'A': 2, 'a': 2, 'J': 2,                  # Class 2: Supraventricular Ectopic (SVEB)\n",
    "    'F': 3                                           # Class 3: Fusion Beat (F)\n",
    "}\n",
    "\n",
    "# Reverse map for plotting labels\n",
    "class_names_map = {\n",
    "    0: 'Normal (N)',\n",
    "    1: 'Ventricular (V)',\n",
    "    2: 'Supraventricular (S)',\n",
    "    3: 'Fusion (F)'\n",
    "}\n",
    "DB_PATH_MIT = '../data/raw/MIT-BIH/mit-bih-arrhythmia-database-1.0.0/mit-bih-arrhythmia-database-1.0.0/'\n",
    "FS_MIT = 360\n",
    "FS_TARGET = 500\n",
    "WINDOW_SIZE = int(FS_TARGET * 0.8)  # 400 samples -> 0.8s * 500Hz\n",
    "\n",
    "# Split MIT-BIH records into training and testing sets to prevent patient data leakage\n",
    "RECORDS_TRAIN = ['101', '106', '108', '109', '112', '114', '115', '116', '118', '119',\n",
    "                 '122', '124', '201', '203', '205', '207', '208', '209', '215', '220',\n",
    "                 '223', '230'] # DS1\n",
    "RECORDS_TEST = ['100', '103', '105', '111', '113', '117', '121', '123', '200', '202',\n",
    "                '210', '212', '213', '214', '219', '221', '222', '228', '231', '232',\n",
    "                '233', '234'] # DS2\n",
    "\n",
    "custom_file_paths = {\n",
    "    'Arrhythmia': '../data/raw/Arrhythmia/ECG_WAVE.bin',\n",
    "    'Normal': '../data/raw/Normal/ecg_normal.bin'\n",
    "}\n",
    "custom_file_labels = {'Arrhythmia': 2, 'Normal': 0} # SVEB and Normal\n",
    "\n",
    "# Define the output directory\n",
    "output_dir_prepare_data = '../data/processed'\n",
    "os.makedirs(output_dir_prepare_data, exist_ok=True)\n",
    "\n",
    "# --- PREPROCESSING & UTILITIES ---\n",
    "def preprocess_signal(signal, fs):\n",
    "    \"\"\"Applies a multi-stage denoising pipeline to a raw ECG signal.\"\"\"\n",
    "    signal = np.array(signal)\n",
    "    nyq = 0.5 * fs\n",
    "    # High-pass filter to remove baseline wander\n",
    "    low_cutoff = 0.6\n",
    "    b, a = butter(2, low_cutoff / nyq, btype='high')\n",
    "    signal_bw_removed = filtfilt(b, a, signal)\n",
    "    # Notch filter to remove powerline interference\n",
    "    powerline_freq = 50\n",
    "    # The quality factor Q is the center frequency divided by the bandwidth.\n",
    "    # A higher Q means a narrower notch.\n",
    "    b, a = iirnotch(powerline_freq, Q=30, fs=fs)\n",
    "    signal_pl_removed = filtfilt(b, a, signal_bw_removed)\n",
    "    # Low-pass filter to remove high-frequency noise\n",
    "    high_cutoff = 100\n",
    "    b, a = butter(4, high_cutoff / nyq, btype='low')\n",
    "    cleaned_signal = filtfilt(b, a, signal_pl_removed)\n",
    "    return cleaned_signal\n",
    "\n",
    "def visualize_preprocessing_result(raw_signal, fs, record_name=\"\"):\n",
    "    \"\"\"\n",
    "    Applies preprocessing and plots the raw vs. cleaned signal for comparison.\n",
    "    \"\"\"\n",
    "    print(f\"Visualizing preprocessing for record: {record_name} (first 10 seconds)...\")\n",
    "    \n",
    "    duration_in_seconds = 10\n",
    "    snippet_length = int(duration_in_seconds * fs)\n",
    "    raw_snippet = raw_signal[:snippet_length]\n",
    "\n",
    "    cleaned_snippet = preprocess_signal(raw_snippet, fs=fs)\n",
    "\n",
    "    time_axis = np.arange(len(raw_snippet)) / fs\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(time_axis, raw_snippet, label='Raw Signal', color='blue', alpha=0.7)\n",
    "    plt.plot(time_axis, cleaned_snippet, label='Cleaned Signal', color='red', linewidth=1.5)\n",
    "    plt.title(f'ECG Signal Preprocessing Result for Record {record_name}')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Amplitude (mV)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# --- DATA LOADING UTILITIES ---\n",
    "def load_mitbih_records(db_path, record_names):\n",
    "    \"\"\"\n",
    "    Loads raw ECG signals (MLII lead only) and their annotations.\n",
    "    \"\"\"\n",
    "    signals, all_annotations = [], []\n",
    "    print(f\"Loading MIT-BIH records (MLII lead only): {', '.join(record_names)}...\")\n",
    "    \n",
    "    for rec_name in record_names:\n",
    "        record_path = os.path.join(db_path, rec_name)\n",
    "        try:\n",
    "            # Read the record, but only channel 0 (usually MLII)\n",
    "            record = wfdb.rdrecord(record_path, channels=[0])\n",
    "            \n",
    "            # Append the signal from the first channel\n",
    "            signals.append(record.p_signal.flatten())\n",
    "            \n",
    "            # Read the annotation\n",
    "            annotation = wfdb.rdann(record_path, 'atr')\n",
    "            all_annotations.append(annotation)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing record {rec_name}: {e}\")\n",
    "            \n",
    "    print(\"Loading complete.\")\n",
    "    return signals, all_annotations\n",
    "\n",
    "def load_ecg_from_bin(file_path, dtype=np.int16):\n",
    "    \"\"\"Loading raw ECG signals from binary files.\"\"\"\n",
    "    try:\n",
    "        signal = np.fromfile(file_path, dtype=dtype)\n",
    "        print(f\"Completed reading {len(signal)} samples from {file_path}\")\n",
    "        return signal\n",
    "    except IOError as e:\n",
    "        print(f\"An error has occurred while reading: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_r_peaks_robust(signal, fs):\n",
    "    \"\"\"Detects R-peaks using a robust algorithm from NeuroKit2.\"\"\"\n",
    "    try:\n",
    "        _, rpeaks_dict = nk.ecg_peaks(signal, sampling_rate=fs)\n",
    "        r_peaks = rpeaks_dict['ECG_R_Peaks']\n",
    "        return r_peaks\n",
    "    except Exception as e:\n",
    "        print(f\"R-peak detection failed: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# --- DATA PREPARATION FUNCTIONS FOR 1D-CNN ---\n",
    "def prepare_cnn_data(signals, annotations, window_size, fs=360, target_fs=500):\n",
    "    \"\"\"Processes single-lead MIT-BIH data to generate raw windows for 1D-CNN.\"\"\"\n",
    "    all_raw_windows, all_labels = [], []\n",
    "    samples_before = window_size // 3\n",
    "    samples_after = window_size - samples_before\n",
    "\n",
    "    for i, raw_signal in enumerate(signals):\n",
    "        ann = annotations[i]\n",
    "        try:\n",
    "            # Preprocess and resample the single signal\n",
    "            cleaned_signal_orig_fs = preprocess_signal(raw_signal, fs=fs)\n",
    "            num_samples_resampled = int(len(cleaned_signal_orig_fs) * (target_fs / fs))\n",
    "            cleaned_signal = resample(cleaned_signal_orig_fs, num_samples_resampled)\n",
    "            \n",
    "            # Resample R-peak locations to match the new sampling rate\n",
    "            r_peaks_resampled = np.round(ann.sample * (target_fs / fs)).astype(int)\n",
    "\n",
    "            # Iterate through R-peaks to extract windows\n",
    "            for j in range(1, len(r_peaks_resampled) - 1):\n",
    "                r_peak_loc = r_peaks_resampled[j]\n",
    "                symbol = ann.symbol[j]\n",
    "                if symbol in label_map:\n",
    "                    start, end = r_peak_loc - samples_before, r_peak_loc + samples_after\n",
    "                    if start >= 0 and end < len(cleaned_signal):\n",
    "                        window = cleaned_signal[start:end]\n",
    "                        all_raw_windows.append(np.expand_dims(window, axis=-1))\n",
    "                        all_labels.append(label_map[symbol])\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process record {ann.record_name}: {e}\")\n",
    "            \n",
    "    return np.array(all_raw_windows), np.array(all_labels)\n",
    "\n",
    "def prepare_cnn_data_from_bin(signal, r_peaks, window_size, label, target_fs=500):\n",
    "    \"\"\"Processes a single-lead .bin file to generate raw windows for 1D-CNN.\"\"\"\n",
    "    all_raw_windows, all_labels = [], []\n",
    "    samples_before = window_size // 3\n",
    "    samples_after = window_size - samples_before\n",
    "    cleaned_signal = preprocess_signal(signal, fs=target_fs)\n",
    "\n",
    "    for j in range(1, len(r_peaks) - 1):\n",
    "        r_peak_loc = r_peaks[j]\n",
    "        start, end = r_peak_loc - samples_before, r_peak_loc + samples_after\n",
    "        if start >= 0 and end < len(cleaned_signal):\n",
    "            window = cleaned_signal[start:end]\n",
    "            all_raw_windows.append(np.expand_dims(window, axis=-1))\n",
    "            all_labels.append(label)\n",
    "            \n",
    "    return np.array(all_raw_windows), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c0215-637d-49cd-8049-379035e45e3f",
   "metadata": {},
   "source": [
    "### DATA PREPARATION EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82e85c-06ca-4dd4-9d1b-cb0b177484bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- [5] MAIN EXECUTION SCRIPT ---\n",
    "if __name__ == '__main__':\n",
    "    # --- [Step 1] Process MIT-BIH Data ---\n",
    "    print(\"--- [Step 1] Processing Single-Lead MIT-BIH Data ---\")\n",
    "    \n",
    "    # Load raw signals and annotations for training and testing sets\n",
    "    train_signals, train_anns = load_mitbih_records(DB_PATH_MIT, RECORDS_TRAIN)\n",
    "    test_signals, test_anns = load_mitbih_records(DB_PATH_MIT, RECORDS_TEST)\n",
    "\n",
    "    # Visualize preprocessing on the first training signal\n",
    "    if train_signals:\n",
    "        visualize_preprocessing_result(train_signals[0], fs=FS_MIT, record_name=RECORDS_TRAIN[0])\n",
    "\n",
    "    # Prepare raw windowed data for the CNN from MIT-BIH records\n",
    "    X_train_raw_base, y_train = prepare_cnn_data(\n",
    "        train_signals, train_anns, WINDOW_SIZE, fs=FS_MIT, target_fs=FS_TARGET\n",
    "    )\n",
    "    X_test_raw_mit, y_test_mitbih = prepare_cnn_data(\n",
    "        test_signals, test_anns, WINDOW_SIZE, fs=FS_MIT, target_fs=FS_TARGET\n",
    "    )\n",
    "\n",
    "    # --- [Step 2] Process Single-Lead Custom .bin Data for TESTING ---\n",
    "    print(\"\\n--- [Step 2] Processing Single-Lead Custom Data for Testing ---\")\n",
    "    X_test_custom_raw_list, y_test_custom_list = [], []\n",
    "    for name, path in custom_file_paths.items():\n",
    "        signal = load_ecg_from_bin(path)\n",
    "        if signal is not None:\n",
    "            # Note: Assuming custom .bin files are already at the target frequency (FS_TARGET)\n",
    "            r_peaks = detect_r_peaks_robust(signal, fs=FS_TARGET)\n",
    "            if r_peaks.size > 2:\n",
    "                raw_single, y_single = prepare_cnn_data_from_bin(\n",
    "                    signal, r_peaks, WINDOW_SIZE, custom_file_labels[name], target_fs=FS_TARGET\n",
    "                )\n",
    "                X_test_custom_raw_list.append(raw_single)\n",
    "                y_test_custom_list.append(y_single)\n",
    "            \n",
    "    if X_test_custom_raw_list:\n",
    "        X_test_custom_raw = np.vstack(X_test_custom_raw_list)\n",
    "        y_test_custom = np.concatenate(y_test_custom_list)\n",
    "    else:\n",
    "        X_test_custom_raw, y_test_custom = np.array([]), np.array([])\n",
    "\n",
    "    # --- [Step 3] Combine Test Datasets ---\n",
    "    print(\"\\n--- [Step 3] Combining Test Datasets ---\")\n",
    "    X_test_final = np.concatenate((X_test_raw_mit, X_test_custom_raw), axis=0)\n",
    "    y_test_final = np.concatenate((y_test_mitbih, y_test_custom), axis=0)\n",
    "    \n",
    "    print(f\"Final Test Data Shapes -> Raw: {X_test_final.shape}, Labels: {y_test_final.shape}\")\n",
    "\n",
    "    # --- [Step 4] Splitting Training Data for Validation and Resampling ---\n",
    "    print(\"\\n--- [Step 4] Splitting and Resampling Training Data ---\")\n",
    "    X_train, X_val, y_train_fold, y_val_fold = train_test_split(\n",
    "        X_train_raw_base, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    print(\"Class distribution before resampling:\", Counter(y_train_fold))\n",
    "    \n",
    "    print(\"Applying SMOTEENN to raw training data...\")\n",
    "    # Reshape for SMOTEENN (flatten the window)\n",
    "    n_samples, window_len, channels = X_train.shape\n",
    "    X_train_reshaped = X_train.reshape(n_samples, -1)\n",
    "    \n",
    "    sampler = SMOTEENN(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = sampler.fit_resample(X_train_reshaped, y_train_fold)\n",
    "    \n",
    "    # Reshape back to 3D for the CNN\n",
    "    X_train_cnn = X_train_resampled.reshape(-1, window_len, channels)\n",
    "    y_train_cnn_1d = y_train_resampled\n",
    "    \n",
    "    print(\"Class distribution after sampling:\", Counter(y_train_cnn_1d))\n",
    "\n",
    "    # --- [Step 5] Final Data Preparation (One-Hot Encoding) ---\n",
    "    print(\"\\n--- [Step 5] Preparing Final Datasets for Models (One-Hot Encoding) ---\")\n",
    "    output_dim = len(np.unique(y_train))\n",
    "\n",
    "    y_train_cnn = to_categorical(y_train_cnn_1d, num_classes=output_dim)\n",
    "    X_val_cnn = X_val\n",
    "    y_val_cnn = to_categorical(y_val_fold, num_classes=output_dim)\n",
    "    X_test_cnn = X_test_final\n",
    "    y_test_cnn = to_categorical(y_test_final, num_classes=output_dim)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ DATA PREPARATION COMPLETE ✅\")\n",
    "    print(f\"Shapes for 1D-CNN -> Train: {X_train_cnn.shape}, Val: {X_val_cnn.shape}, Test: {X_test_cnn.shape}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162fbb6-179a-483f-b663-69c1b64dc040",
   "metadata": {},
   "source": [
    "## **MACHINE LEARNING MODEL TRAINING & SAVING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04aa916-ef83-4e8f-ae5d-80d0a49a2129",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL FUNCTIONS\n",
    "The used model is optimized 1D-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28664a07-7fce-4c07-b780-2ffac2105947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, output_dim, hp=None):\n",
    "    \"\"\"\n",
    "    Creates and compiles an optimized 1D-CNN model.\n",
    "    If 'hp' is provided, it builds a tunable model for KerasTuner.\n",
    "    Otherwise, it builds a model with default hyperparameters.\n",
    "    \"\"\"\n",
    "    # Define a default hyperparameter object if none is passed\n",
    "    if hp is None:\n",
    "        hp = kt.HyperParameters()\n",
    "        # Set default values for when not tuning\n",
    "        hp.values['conv4_filters'] = 64\n",
    "        hp.values['dense_units'] = 512\n",
    "        hp.values['learning_rate'] = 0.0001\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='valid', name='conv1d_1_freezed')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_1_freezed')(x)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', name='conv1d_2_freezed')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_2_freezed')(x)\n",
    "    x = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same', name='conv1d_3_freezed')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_3_freezed')(x)\n",
    "    # ===============================================\n",
    "    #           Trainable Layers\n",
    "    # ===============================================\n",
    "    x = Conv1D(filters=hp.values['conv4_filters'], kernel_size=3, activation='relu', padding='same', name='conv1d_4_trainable')(x)\n",
    "    x = Flatten(name='flatten_layer')(x)\n",
    "    x = Dense(units=hp.values['dense_units'], activation='relu', name='dense_1_trainable')(x)\n",
    "    outputs = Dense(units=output_dim, activation='softmax', name='output_layer_trainable')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.values['learning_rate']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16e19d-a20a-4308-9848-37de070b26a3",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL TRAINING EXECUTION\n",
    "1. Targeted metrics: Precision, Recall, F1-Score, and Specificity\n",
    "2. After training, all models immediately saved and exported into a certain folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e497d-133b-4e34-94dc-b4ea017cc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ten-Fold Cross Validation for CNN model---\n",
    "# Define input and output dimensions for the CNN model\n",
    "# We combine the initial training and validation sets for cross-validation\n",
    "X_full_train = np.concatenate((X_train_cnn, X_val_cnn), axis=0)\n",
    "y_full_train_cat = np.concatenate((y_train_cnn, y_val_cnn), axis=0)\n",
    "y_full_train = np.argmax(y_full_train_cat, axis=1) # Get 1D labels for splitting\n",
    "\n",
    "input_shape_cnn = (X_full_train.shape[1], X_full_train.shape[2])\n",
    "output_dim = y_full_train_cat.shape[1]\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)']\n",
    "\n",
    "# Define the output directory for saving the model\n",
    "output_dir = '../models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- 10-Fold Cross-Validation ---\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store metrics for each fold\n",
    "fold_accuracies = []\n",
    "fold_f1_scores = []\n",
    "fold_losses = []\n",
    "\n",
    "print(f\"\\n{'='*20} STARTING {n_splits}-FOLD CROSS-VALIDATION {'='*20}\")\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X_full_train, y_full_train)):\n",
    "    print(f\"\\n--- FOLD {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "    # Split the data for the current fold\n",
    "    X_train, X_val = X_full_train[train_index], X_full_train[val_index]\n",
    "    y_train_cat, y_val_cat = y_full_train_cat[train_index], y_full_train_cat[val_index]\n",
    "    y_train = y_full_train[train_index] # 1D labels for weight calculation\n",
    "\n",
    "    # --- Manual Class Weights Calculation for the current fold ---\n",
    "    print(\"Calculating class weights for the current fold...\")\n",
    "    n_samples = len(y_train)\n",
    "    class_counts = Counter(y_train)\n",
    "    n_classes = len(class_counts)\n",
    "    \n",
    "    manual_weights = {}\n",
    "    print(\"Manual Calculation Breakdown:\")\n",
    "    for i in range(n_classes):\n",
    "        n_samples_in_class = class_counts.get(i, 0) # Use .get for safety\n",
    "        weight = n_samples / (n_classes * n_samples_in_class)\n",
    "        manual_weights[i] = weight\n",
    "        print(f\"  Weight for Class {i} ({class_names[i]}): {n_samples} / ({n_classes} * {n_samples_in_class}) = {weight:.4f}\")\n",
    "    \n",
    "    class_weights_dict = dict(manual_weights)\n",
    "    print(\"Fold Weights:\", class_weights_dict)\n",
    "\n",
    "    # --- Model Creation (re-create model for each fold) ---\n",
    "    model = create_cnn_model(input_shape_cnn, output_dim)\n",
    "\n",
    "    # --- Model Training for the current fold ---\n",
    "    print(\"Training...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        epochs=150,\n",
    "        batch_size=100,\n",
    "        verbose=0, # Set to 0 to keep output clean during CV\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "    # --- Evaluate the fold ---\n",
    "    # FIX: Unpack all returned values from model.evaluate into a list\n",
    "    # The first value is always loss, the second is the first metric (accuracy)\n",
    "    evaluation_results = model.evaluate(X_val, y_val_cat, verbose=0)\n",
    "    loss = evaluation_results[0]\n",
    "    accuracy = evaluation_results[1]\n",
    "    \n",
    "    # Get predictions to calculate F1-score\n",
    "    y_val_pred_raw = model.predict(X_val, verbose=0)\n",
    "    y_val_pred = np.argmax(y_val_pred_raw, axis=1)\n",
    "    y_val_true = np.argmax(y_val_cat, axis=1)\n",
    "    \n",
    "    # Calculate macro F1-score, which is suitable for imbalanced classes\n",
    "    f1 = f1_score(y_val_true, y_val_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    fold_losses.append(loss)\n",
    "    fold_accuracies.append(accuracy)\n",
    "    fold_f1_scores.append(f1) # Append F1-score\n",
    "    print(f\"Fold {fold + 1} - Val Loss: {loss:.4f}, Val Accuracy: {accuracy:.4f}, Val Macro F1-Score: {f1:.4f}\")\n",
    "\n",
    "# --- Average Cross-Validation Results ---\n",
    "print(f\"\\n{'='*20} CROSS-VALIDATION SUMMARY {'='*20}\")\n",
    "print(f\"Average Validation Accuracy: {np.mean(fold_accuracies):.4f} (+/- {np.std(fold_accuracies):.4f})\")\n",
    "print(f\"Average Validation F1-Score: {np.mean(fold_f1_scores):.4f} (+/- {np.std(fold_f1_scores):.4f})\")\n",
    "print(f\"Average Validation Loss: {np.mean(fold_losses):.4f}\")\n",
    "\n",
    "\n",
    "# --- Final Model Training on ALL Data ---\n",
    "print(f\"\\n{'='*20} TRAINING FINAL MODEL ON ALL DATA {'='*20}\")\n",
    "# Re-calculate weights on the full training dataset\n",
    "full_train_class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_full_train), y=y_full_train)\n",
    "full_train_class_weights_dict = dict(enumerate(full_train_class_weights))\n",
    "print(\"Final Model Weights:\", full_train_class_weights_dict)\n",
    "\n",
    "final_model = create_cnn_model_optimized(input_shape_cnn, output_dim)\n",
    "final_model.fit(\n",
    "    X_full_train, y_full_train_cat,\n",
    "    epochs=150,\n",
    "    batch_size=100,\n",
    "    verbose=1,\n",
    "    class_weight=full_train_class_weights_dict\n",
    ")\n",
    "\n",
    "# --- Final Evaluation on the Hold-Out Test Set ---\n",
    "print(f\"\\n{'='*20} FINAL EVALUATION ON TEST SET {'='*20}\")\n",
    "y_pred_raw = final_model.predict(X_test_cnn)\n",
    "y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "y_true = y_test_final\n",
    "\n",
    "# --- Final Evaluation Report ---\n",
    "print(f\"\\n{'~'*15} REPORT FOR FINAL MODEL {'~'*15}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "# --- Confusion Matrix Visualization ---\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix for Final 1D-CNN Model', fontsize=16)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# --- Saving the Final Model ---\n",
    "print(f\"\\n--- Saving final model: 1D-CNN ---\")\n",
    "model_path = os.path.join(output_dir, \"model_1d-cnn_final.keras\")\n",
    "final_model.save(model_path)\n",
    "print(f\"✅ Final Model 1D-CNN has been saved on: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214cd86a-625a-4637-9678-c23ade8ade9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Single-Fold Training for CNN model only--\n",
    "# Define input and output dimensions for the CNN model\n",
    "input_shape_cnn = (X_train_cnn.shape[1], X_train_cnn.shape[2])\n",
    "output_dim = y_train_cnn.shape[1]\n",
    "\n",
    "# Define the output directory for saving the model\n",
    "output_dir = '../models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Manual Class Weights Calculation for the current fold ---\n",
    "print(\"Calculating class weights for the current fold...\")\n",
    "n_samples = len(y_train)\n",
    "class_counts = Counter(y_train)\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "manual_weights = {}\n",
    "print(\"Manual Calculation Breakdown:\")\n",
    "for i in range(n_classes):\n",
    "    n_samples_in_class = class_counts.get(i, 0) # Use .get for safety\n",
    "    weight = n_samples / (n_classes * n_samples_in_class)\n",
    "    manual_weights[i] = weight\n",
    "    print(f\"  Weight for Class {i} ({class_names[i]}): {n_samples} / ({n_classes} * {n_samples_in_class}) = {weight:.4f}\")\n",
    "\n",
    "class_weights_dict = dict(manual_weights)\n",
    "print(\"Fold Weights:\", class_weights_dict)\n",
    "\n",
    "# --- Model Creation ---\n",
    "print(f\"\\n{'='*20} CREATING 1D-CNN MODEL {'='*20}\")\n",
    "model = create_cnn_model(input_shape_cnn, output_dim)\n",
    "model.summary()\n",
    "\n",
    "# --- Model Training ---\n",
    "print(f\"\\n{'='*20} TRAINING 1D-CNN MODEL {'='*20}\")\n",
    "model.fit(\n",
    "    X_train_cnn, y_train_cnn,\n",
    "    epochs=150,\n",
    "    batch_size=100,\n",
    "    verbose=1,\n",
    "    validation_data=(X_val_cnn, y_val_cnn),\n",
    "    class_weight=class_weights_dict\n",
    ")\n",
    "\n",
    "# --- Model Evaluation ---\n",
    "print(f\"\\n{'='*20} EVALUATING 1D-CNN MODEL {'='*20}\")\n",
    "# Predict on the test set\n",
    "y_pred_raw = model.predict(X_test_cnn)\n",
    "y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "y_true = y_test_final # Use the final, combined ground truth labels\n",
    "\n",
    "# --- Final Evaluation Report ---\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)']\n",
    "print(f\"\\n{'~'*15} REPORT FOR 1D-CNN MODEL {'~'*15}\")\n",
    "\n",
    "# --- Classification Report ---\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)\n",
    "print(report)\n",
    "\n",
    "# --- Confusion Matrix Visualization ---\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix for 1D-CNN', fontsize=16)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# --- Saving the Model ---\n",
    "print(f\"\\n--- Saving model: 1D-CNN ---\")\n",
    "model_path = os.path.join(output_dir, \"model_1d-cnn_saved.keras\")\n",
    "# Use model.save() for Keras models for better compatibility\n",
    "model.save(model_path) \n",
    "print(f\"✅ Model 1D-CNN has been saved on: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
