{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4aeda7b-fbba-4592-b470-cb25390a695a",
   "metadata": {},
   "source": [
    "# training-B.ipynb\n",
    "1. This code is intended for training three models with raw data for 1D-CNN and WPT data for MLP and Random Forest\n",
    "2. The dataset are MIT-BIH Arrhythmia Database and additional ECG data with format of .bin\n",
    "3. Classes/labels: N, S, V, F, and/without Q\n",
    "4. The data utilizes dual-lead based on MIT-BIH Arrhythmia Database so that the ECG data from .bin is duplicated onto the other lead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e32954",
   "metadata": {},
   "source": [
    "## **LIBRARY IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0b717-46b3-4967-962f-6d5f2eae8848",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import cudf\n",
    "import os\n",
    "import joblib\n",
    "import pywt\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb  # For reading MIT-BIH data\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import neurokit2 as nk\n",
    "\n",
    "# Scikit-learn and Imbalanced-learn imports\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "from scipy.signal import find_peaks, resample, butter, filtfilt, iirnotch, spectrogram\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Model imports\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Dropout, Add, GlobalAveragePooling1D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Additional setups\n",
    "# Checking cUML\n",
    "print(cudf.Series([1, 2, 3]))\n",
    "\n",
    "# Setting TensorFlow flags\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Checking GPU\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"TensorFlow has detected {len(gpu_devices)} GPU(s):\")\n",
    "    for device in gpu_devices:\n",
    "        print(f\"- {device}\")\n",
    "else:\n",
    "    print(\"TensorFlow did not detect any GPUs. Training will run on the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348ca32-352e-4791-a764-9a1dedd1af04",
   "metadata": {},
   "source": [
    "## **DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb05f79-645a-4ff3-b8f5-44e867b01a39",
   "metadata": {},
   "source": [
    "### DATA PREPARATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a11cb1-fc53-445e-933c-49f337cb0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFIGURATIONS & ADDITIONAL FUNCTIONS\n",
    "label_map = {\n",
    "    'N': 0, '.': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0,  # Class 0: Normal Beats (N)\n",
    "    'V': 1, 'E': 1,                                  # Class 1: Ventricular Ectopic (VEB)\n",
    "    'S': 2, 'A': 2, 'a': 2, 'J': 2,                  # Class 2: Supraventricular Ectopic (SVEB)\n",
    "    'F': 3                                           # Class 3: Fusion Beat (F)\n",
    "}\n",
    "DB_PATH_MIT = '../data/raw/MIT-BIH/mit-bih-arrhythmia-database-1.0.0/mit-bih-arrhythmia-database-1.0.0/'\n",
    "FS_MIT = 360\n",
    "FS_TARGET = 500\n",
    "WINDOW_SIZE = int(FS_TARGET*0.8)  # 600ms window -> 0.6s * 500Hz\n",
    "# Split MIT-BIH records into training and testing sets to prevent patient data leakage\n",
    "RECORDS_TRAIN = ['101', '106', '108', '109', '112', '114', '115', '116', '118', '119',\n",
    "                 '122', '124', '201', '203', '205', '207', '208', '209', '215', '220',\n",
    "                 '223', '230'] # DS1\n",
    "RECORDS_TEST = ['100', '103', '105', '111', '113', '117', '121', '123', '200', '202',\n",
    "                '210', '212', '213', '214', '219', '221', '222', '228', '231', '232',\n",
    "                '233', '234'] # DS2\n",
    "custom_file_paths = {\n",
    "    'Arrhythmia': '../data/raw/Arrhythmia/ECG_WAVE.bin',\n",
    "    'Normal': '../data/raw/Normal/ecg_normal.bin' \n",
    "}\n",
    "custom_file_labels = {'Arrhythmia': 2, 'Normal': 0} # SVEB and Normal\n",
    "\n",
    "# Wavelet Feature Configuration\n",
    "WAVELET_TYPE = 'db4'\n",
    "WAVELET_LEVEL = 4\n",
    "\n",
    "# Define the output directory\n",
    "output_dir_prepare_data = '../data/processed'\n",
    "os.makedirs(output_dir_prepare_data, exist_ok=True) # This creates the directory if it doesn't exist\n",
    "\n",
    "# Filtering ECG data on a certain frequencies\n",
    "def preprocess_signal(signal, fs=500):\n",
    "    \"\"\"\n",
    "    Applies a multi-stage denoising pipeline to a raw ECG signal.\n",
    "    This pipeline is designed to remove baseline wander, powerline interference,\n",
    "    and high-frequency noise to maximize signal-to-noise ratio.\n",
    "    \"\"\"\n",
    "    # Ensure signal is a numpy array\n",
    "    signal = np.array(signal)\n",
    "    \n",
    "    # Step 1: Remove baseline wander with a high-pass Butterworth filter.\n",
    "    nyq = 0.5 * fs\n",
    "    low_cutoff = 0.6\n",
    "    b, a = butter(2, low_cutoff / nyq, btype='high')\n",
    "    signal_bw_removed = filtfilt(b, a, signal)\n",
    "\n",
    "    # Step 2: Remove powerline interference with a notch filter (50Hz for Indonesia).\n",
    "    powerline_freq = 50\n",
    "    b, a = iirnotch(powerline_freq / nyq, Q=30)\n",
    "    signal_pl_removed = filtfilt(b, a, signal_bw_removed)\n",
    "\n",
    "    # Step 3: Attenuate high-frequency noise with a low-pass Butterworth filter.\n",
    "    high_cutoff = 100\n",
    "    b, a = butter(4, high_cutoff / nyq, btype='low')\n",
    "    cleaned_signal = filtfilt(b, a, signal_pl_removed)\n",
    "    \n",
    "    return cleaned_signal\n",
    "\n",
    "# Extract features of wavelet\n",
    "def extract_wavelet_features(window, wavelet='db4', level=4):\n",
    "    \"\"\"\n",
    "    Extracts statistical and entropy-based features from the wavelet coefficients of an ECG window.\n",
    "    \n",
    "    Args:\n",
    "        window (np.ndarray): A 1D numpy array representing a single ECG beat window.\n",
    "        wavelet (str): The type of wavelet to use.\n",
    "        level (int): The level of wavelet decomposition.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A 1D numpy array containing the extracted features.\n",
    "    \"\"\"\n",
    "    # Decompose the signal\n",
    "    coeffs = pywt.wavedec(window, wavelet, level=level)\n",
    "    \n",
    "    features = []\n",
    "    for c in coeffs:\n",
    "        # Basic statistical features\n",
    "        features.append(np.mean(c))\n",
    "        features.append(np.std(c))\n",
    "        features.append(np.var(c))\n",
    "        \n",
    "        # Energy of the coefficients\n",
    "        features.append(np.sum(np.square(c)))\n",
    "        \n",
    "        # Shannon Entropy of the coefficients\n",
    "        # We use the squared coeffs to represent energy distribution for entropy calculation\n",
    "        # Adding a small epsilon to avoid log(0)\n",
    "        features.append(entropy(np.square(c) + 1e-9))\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "# Wavelet Packet Transformation\n",
    "def extract_wpt_features(window, wavelet='db4', level=4):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from the Wavelet Packet Transform (WPT) \n",
    "    coefficients of an ECG window for a more detailed analysis.\n",
    "    \"\"\"\n",
    "    # Create the wavelet packet object\n",
    "    wp = pywt.WaveletPacket(data=window, wavelet=wavelet, mode='symmetric', maxlevel=level)\n",
    "    \n",
    "    # Get the coefficient nodes at the specified level\n",
    "    nodes = wp.get_level(level, order='natural')\n",
    "    # Extract the data (coefficients) from each node\n",
    "    coeffs = [node.data for node in nodes]\n",
    "    \n",
    "    features = []\n",
    "    for c in coeffs:\n",
    "        # Basic statistical features\n",
    "        features.append(np.mean(c))\n",
    "        features.append(np.std(c))\n",
    "        features.append(np.var(c))\n",
    "        \n",
    "        # Energy of the coefficients\n",
    "        features.append(np.sum(np.square(c)))\n",
    "        \n",
    "        # Shannon Entropy of the coefficients\n",
    "        features.append(entropy(np.square(c) + 1e-9))\n",
    "        \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03deef09-ac69-442e-96e2-a2d354872eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATA LOADING UTILITY OF MIT-BIH DATA\n",
    "# This function efficiently loads the specified records and their annotations.\n",
    "def load_mitbih_records(db_path, record_names):\n",
    "    \"\"\"\n",
    "    Loads raw ECG signals and annotations for specified records from both leads.\n",
    "\n",
    "    Args:\n",
    "        db_path (str): The path to the database directory.\n",
    "        record_names (list): A list of record names as strings.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing three lists: (signals_leadA, signals_leadB, annotations).\n",
    "        - signals_leadA: A list of raw ECG signal arrays for channel 0.\n",
    "        - signals_leadB: A list of raw ECG signal arrays for channel 1.\n",
    "        - annotations: A list of wfdb Annotation objects.\n",
    "    \"\"\"\n",
    "    signals_leadA = []\n",
    "    signals_leadB = []\n",
    "    all_annotations = []\n",
    "    print(f\"Loading records: {', '.join(record_names)}...\")\n",
    "    for rec_name in record_names:\n",
    "        record_path = f'{db_path}/{rec_name}'\n",
    "        try:\n",
    "            # Read both channels (0 and 1)\n",
    "            record = wfdb.rdrecord(record_path, channels=[0, 1])\n",
    "            signals_leadA.append(record.p_signal[:, 0].flatten())\n",
    "            signals_leadB.append(record.p_signal[:, 1].flatten())\n",
    "            \n",
    "            # Annotations are the same for both leads\n",
    "            annotation = wfdb.rdann(record_path, 'atr')\n",
    "            all_annotations.append(annotation)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing record {rec_name}: {e}\")\n",
    "    print(\"Loading complete.\")\n",
    "    return signals_leadA, signals_leadB, all_annotations\n",
    "\n",
    "def prepare_wpt_and_raw_data(signals_A, signals_B, annotations, window_size, fs=360, target_fs=500):\n",
    "    \"\"\"\n",
    "    Processes dual-lead MIT-BIH data once to generate both WPT features and raw windows.\n",
    "    \"\"\"\n",
    "    all_wpt_features, all_raw_windows, all_labels = [], [], []\n",
    "    samples_before = window_size // 3\n",
    "    samples_after = window_size - samples_before\n",
    "\n",
    "    for i, (raw_signal_A, raw_signal_B) in enumerate(zip(signals_A, signals_B)):\n",
    "        ann = annotations[i]\n",
    "        try:\n",
    "            resampled_A = resample(raw_signal_A, int(len(raw_signal_A) * (target_fs / fs)))\n",
    "            cleaned_A = preprocess_signal(resampled_A, fs=target_fs)\n",
    "            resampled_B = resample(raw_signal_B, int(len(raw_signal_B) * (target_fs / fs)))\n",
    "            cleaned_B = preprocess_signal(resampled_B, fs=target_fs)\n",
    "            r_peaks_resampled = np.round(ann.sample * (target_fs / fs)).astype(int)\n",
    "\n",
    "            for j, r_peak_loc in enumerate(r_peaks_resampled):\n",
    "                symbol = ann.symbol[j]\n",
    "                if symbol in label_map:\n",
    "                    start, end = r_peak_loc - samples_before, r_peak_loc + samples_after\n",
    "                    if start >= 0 and end < len(cleaned_A):\n",
    "                        window_A, window_B = cleaned_A[start:end], cleaned_B[start:end]\n",
    "                        \n",
    "                        # WPT Features: Concatenate from both leads\n",
    "                        wpt_A = extract_wpt_features(window_A)\n",
    "                        wpt_B = extract_wpt_features(window_B)\n",
    "                        all_wpt_features.append(np.concatenate((wpt_A, wpt_B)))\n",
    "                        \n",
    "                        # Raw Windows: Stack into a 2-channel array\n",
    "                        all_raw_windows.append(np.stack((window_A, window_B), axis=-1))\n",
    "                        all_labels.append(label_map[symbol])\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process record {ann.record_name}: {e}\")\n",
    "\n",
    "    return np.array(all_wpt_features), np.array(all_raw_windows), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f269e46-0c80-4702-b7e2-bfd77c68fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DATA LOADING FOR .bin FILES\n",
    "# A function to load ECG data\n",
    "def load_ecg_from_bin(file_path, dtype=np.int16):\n",
    "    \"\"\"\n",
    "    Loading raw ECG signals from binary files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .bin file.\n",
    "        dtype (numpy.dtype): Data type of the signal in the .bin file.\n",
    "\n",
    "    Return:\n",
    "        numpy.ndarray: ECG signals as a numpy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        signal = np.fromfile(file_path, dtype=dtype)\n",
    "        print(f\"Completed reading {len(signal)} samples from {file_path}\")\n",
    "        return signal\n",
    "    except IOError as e:\n",
    "        print(f\"An error has occurred while reading: {e}\")\n",
    "        return None\n",
    "\n",
    "# A function to detect R-peaks for labelling\n",
    "def detect_r_peaks_robust(signal, fs):\n",
    "    \"\"\"Detects R-peaks using a robust algorithm from NeuroKit2.\"\"\"\n",
    "    try:\n",
    "        _, rpeaks_dict = nk.ecg_peaks(signal, sampling_rate=fs)\n",
    "        r_peaks = rpeaks_dict['ECG_R_Peaks']\n",
    "        print(f\"Detected {len(r_peaks)} R-peaks.\")\n",
    "        return r_peaks\n",
    "    except Exception as e:\n",
    "        print(f\"Could not detect R-peaks: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def prepare_wpt_and_raw_data_from_bin(signal, r_peaks, window_size, label, wavelet='db4', level=4, target_fs=500):\n",
    "    \"\"\"\n",
    "    Processes a single-lead .bin file to generate both WPT features and raw windows.\n",
    "    Duplicates features to match the dual-lead format.\n",
    "    \"\"\"\n",
    "    all_wpt_features, all_raw_windows, all_labels = [], [], []\n",
    "    samples_before = window_size // 3\n",
    "    samples_after = window_size - samples_before\n",
    "    cleaned_signal = preprocess_signal(signal, fs=target_fs)\n",
    "\n",
    "    for r_peak_loc in r_peaks:\n",
    "        start, end = r_peak_loc - samples_before, r_peak_loc + samples_after\n",
    "        if start >= 0 and end < len(cleaned_signal):\n",
    "            window = cleaned_signal[start:end]\n",
    "            \n",
    "            # WPT Features: Extract and duplicate\n",
    "            wpt_features = extract_wpt_features(window, wavelet=wavelet, level=level)\n",
    "            all_wpt_features.append(np.concatenate((wpt_features, wpt_features)))\n",
    "            \n",
    "            # Raw Windows: Stack duplicated window for 2 channels\n",
    "            all_raw_windows.append(np.stack((window, window), axis=-1))\n",
    "            all_labels.append(label)\n",
    "            \n",
    "    return np.array(all_wpt_features), np.array(all_raw_windows), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c0215-637d-49cd-8049-379035e45e3f",
   "metadata": {},
   "source": [
    "### DATA PREPARATION EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e651c6-c371-4e8c-b986-13eeb677f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"--- [Step 1] Processing Multi-Lead MIT-BIH Data ---\")\n",
    "    print(\"\\nProcessing MIT-BIH Training set (DS1)...\")\n",
    "    train_signals_A, train_signals_B, train_anns = load_mitbih_records(DB_PATH_MIT, RECORDS_TRAIN)\n",
    "    X_train_wpt, X_train_raw, y_train_mit = prepare_wpt_and_raw_data(train_signals_A, train_signals_B, train_anns, WINDOW_SIZE, fs=FS_MIT, target_fs=FS_TARGET)\n",
    "    print(f\"Combined MIT-BIH Training WPT feature matrix shape: {X_train_wpt.shape}\")\n",
    "    print(f\"Combined MIT-BIH Training Raw window matrix shape: {X_train_raw.shape}\")\n",
    "\n",
    "    print(\"\\nProcessing MIT-BIH Testing set (DS2)...\")\n",
    "    test_signals_A, test_signals_B, test_anns = load_mitbih_records(DB_PATH_MIT, RECORDS_TEST)\n",
    "    X_test_wpt_mit, X_test_raw_mit, y_test_mitbih = prepare_wpt_and_raw_data(test_signals_A, test_signals_B, test_anns, WINDOW_SIZE, fs=FS_MIT, target_fs=FS_TARGET)\n",
    "    print(f\"Combined MIT-BIH Testing WPT feature matrix shape: {X_test_wpt_mit.shape}\")\n",
    "    print(f\"Combined MIT-BIH Testing Raw window matrix shape: {X_test_raw_mit.shape}\")\n",
    "\n",
    "    # --- [Step 2] Process Single-Lead Custom Data ---\n",
    "    print(\"\\n--- [Step 2] Processing Single-Lead Custom Data ---\")\n",
    "    X_test_custom_wpt_list, X_test_custom_raw_list, y_test_custom_list = [], [], []\n",
    "    for name, path in custom_file_paths.items():\n",
    "        label = custom_file_labels[name]\n",
    "        signal = load_ecg_from_bin(path)\n",
    "        if signal is not None:\n",
    "            r_peaks = detect_r_peaks_robust(signal, fs=FS_TARGET)\n",
    "            wpt_single, raw_single, y_single = prepare_wpt_and_raw_data_from_bin(\n",
    "                signal, r_peaks, WINDOW_SIZE, label, wavelet=WAVELET_TYPE, level=WAVELET_LEVEL, target_fs=FS_TARGET\n",
    "            )\n",
    "            X_test_custom_wpt_list.append(wpt_single)\n",
    "            X_test_custom_raw_list.append(raw_single)\n",
    "            y_test_custom_list.append(y_single)\n",
    "            \n",
    "    X_test_custom_wpt = np.vstack(X_test_custom_wpt_list)\n",
    "    X_test_custom_raw = np.vstack(X_test_custom_raw_list)\n",
    "    y_test_custom = np.concatenate(y_test_custom_list)\n",
    "    print(f\"Combined Custom Testing WPT feature matrix shape: {X_test_custom_wpt.shape}\")\n",
    "    print(f\"Combined Custom Testing Raw window matrix shape: {X_test_custom_raw.shape}\")\n",
    "\n",
    "    # --- [Step 3] Finalize Datasets and Scale WPT data ---\n",
    "    print(\"\\n--- [Step 3] Scaling and Finalizing Data ---\")\n",
    "    X_train_wpt_base = X_train_wpt\n",
    "    y_train = y_train_mit\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_wpt_scaled = scaler.fit_transform(X_train_wpt_base)\n",
    "    print(\"Scaler trained on WPT training data.\")\n",
    "    \n",
    "    X_test_wpt_mit_scaled = scaler.transform(X_test_wpt_mit)\n",
    "    X_test_wpt_custom_scaled = scaler.transform(X_test_custom_wpt)\n",
    "    \n",
    "    # Combine test sets\n",
    "    X_test_wpt_final = np.concatenate((X_test_wpt_mit_scaled, X_test_wpt_custom_scaled), axis=0)\n",
    "    X_test_raw_final = np.concatenate((X_test_raw_mit, X_test_custom_raw), axis=0)\n",
    "    y_test_final = np.concatenate((y_test_mitbih, y_test_custom), axis=0)\n",
    "    \n",
    "    # --- [Step 4] Splitting & Hybrid Sampling for WPT data ---\n",
    "    print(\"\\n--- [Step 4] Finalizing Training Data (Split & Hybrid Sample) ---\")\n",
    "    X_train_wpt_fold, X_val_wpt_fold, y_train_fold, y_val_fold = train_test_split(\n",
    "        X_train_wpt_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    # Also split the corresponding raw data for the CNN with the same indices\n",
    "    X_train_raw_fold, X_val_raw_fold, _, _ = train_test_split(\n",
    "        X_train_raw, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    print(\"Applying sampling to WPT training data...\")\n",
    "    print(\"Class distribution before sampling:\", Counter(y_train_fold))\n",
    "    sampler = SMOTE(random_state=42)\n",
    "    X_train_wpt_resampled, y_train_wpt_resampled = sampler.fit_resample(X_train_wpt_fold, y_train_fold)\n",
    "    print(\"Class distribution after sampling:\", Counter(y_train_wpt_resampled))\n",
    "\n",
    "    # --- [Step 5] Final Data Preparation for Models ---\n",
    "    print(\"\\n--- [Step 5] Preparing Final Datasets for Models ---\")\n",
    "    output_dim = len(np.unique(y_train))\n",
    "    \n",
    "    # Data for MLP\n",
    "    X_train_mlp, y_train_mlp = X_train_wpt_resampled, to_categorical(y_train_wpt_resampled, num_classes=output_dim)\n",
    "    X_val_mlp, y_val_mlp = X_val_wpt_fold, to_categorical(y_val_fold, num_classes=output_dim)\n",
    "    X_test_mlp, y_test_mlp = X_test_wpt_final, to_categorical(y_test_final, num_classes=output_dim)\n",
    "\n",
    "    # Data for 1D-CNN\n",
    "    # **CORRECTION**: Use the raw window data, not the WPT features\n",
    "    X_train_cnn, y_train_cnn = X_train_raw_fold, to_categorical(y_train_fold, num_classes=output_dim)\n",
    "    X_val_cnn, y_val_cnn = X_val_raw_fold, to_categorical(y_val_fold, num_classes=output_dim)\n",
    "    X_test_cnn, y_test_cnn = X_test_raw_final, to_categorical(y_test_final, num_classes=output_dim)\n",
    "\n",
    "    # Data for RandomForest\n",
    "    X_train_rf, y_train_rf = X_train_wpt_resampled, y_train_wpt_resampled\n",
    "    X_val_rf, y_val_rf = X_val_wpt_fold, y_val_fold\n",
    "    X_test_rf, y_test_rf = X_test_wpt_final, y_test_final\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… DATA PREPARATION COMPLETE âœ…\")\n",
    "    print(f\"Shapes for MLP -> Train: {X_train_mlp.shape}, Val: {X_val_mlp.shape}, Test: {X_test_mlp.shape}\")\n",
    "    print(f\"Shapes for 1D-CNN -> Train: {X_train_cnn.shape}, Val: {X_val_cnn.shape}, Test: {X_test_cnn.shape}\")\n",
    "    print(f\"Shapes for RandomForest -> Train: {X_train_rf.shape}, Val: {X_val_rf.shape}, Test: {X_test_rf.shape}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162fbb6-179a-483f-b663-69c1b64dc040",
   "metadata": {},
   "source": [
    "## **MACHINE LEARNING MODEL TRAINING & SAVING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04aa916-ef83-4e8f-ae5d-80d0a49a2129",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28664a07-7fce-4c07-b780-2ffac2105947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_dim, output_dim):\n",
    "    \"\"\"Creates and compiles a Keras MLP model.\"\"\"\n",
    "    model = Sequential([\n",
    "        # Hyperparameters tuning\n",
    "        Dense(512, input_dim=input_dim, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(output_dim, activation='softmax') # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy', # Suitable for one-hot labels\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            # tf.keras.metrics.Precision(name='precision'),\n",
    "            # tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "# 1st CNN model\n",
    "def create_cnn_model(input_shape, output_dim):\n",
    "    \"\"\"Creates and compiles a Keras 1D-CNN model.\"\"\"\n",
    "    # Input shape for CNN must be 3D: (samples, steps, features)\n",
    "    # Example: (10000, 187, 1)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=512, kernel_size=6, activation='relu', # Reduced filters\n",
    "               input_shape=input_shape),\n",
    "        Dropout(0.1),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        Conv1D(filters=512, kernel_size=3, activation='relu'), # Reduced filters\n",
    "        Dropout(0.2),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        Flatten(), # Now flattens a much smaller tensor\n",
    "\n",
    "        Dense(512, activation='relu'), # Reduced dense units\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "# 1D-CNN optimized based on paper\n",
    "def create_cnn_model_optimized(input_shape, output_dim, hp=None):\n",
    "    \"\"\"\n",
    "    Creates and compiles an optimized 1D-CNN model.\n",
    "    If 'hp' is provided, it builds a tunable model for KerasTuner.\n",
    "    Otherwise, it builds a model with default hyperparameters.\n",
    "    \"\"\"\n",
    "    # Define a default hyperparameter object if none is passed\n",
    "    if hp is None:\n",
    "        hp = kt.HyperParameters()\n",
    "        # Set default values for when not tuning\n",
    "        hp.values['conv4_filters'] = 100\n",
    "        hp.values['dense_units'] = 256\n",
    "        hp.values['learning_rate'] = 0.0001\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='valid', name='conv1d_1_freezed')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_1_freezed')(x)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', name='conv1d_2_freezed')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_2_freezed')(x)\n",
    "    x = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same', name='conv1d_3_freezed')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_3_freezed')(x)\n",
    "    # ===============================================\n",
    "    #           Trainable Layers\n",
    "    # ===============================================\n",
    "    x = Conv1D(filters=hp.values['conv4_filters'], kernel_size=3, activation='relu', padding='same', name='conv1d_4_trainable')(x)\n",
    "    x = Flatten(name='flatten_layer')(x)\n",
    "    x = Dense(units=hp.values['dense_units'], activation='relu', name='dense_1_trainable')(x)\n",
    "    outputs = Dense(units=output_dim, activation='softmax', name='output_layer_trainable')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.values['learning_rate']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 1D-ResNet\n",
    "# def create_cnn_model_optimized(input_shape, output_dim, hp=None):\n",
    "#     \"\"\"\n",
    "#     Creates and compiles an optimized 1D-CNN model.\n",
    "#     If 'hp' is provided, it builds a tunable model for KerasTuner.\n",
    "#     Otherwise, it builds a model with default hyperparameters.\n",
    "#     \"\"\"\n",
    "#     # Define a default hyperparameter object if none is passed\n",
    "#     if hp is None:\n",
    "#         hp = kt.HyperParameters()\n",
    "#         # Set default values for when not tuning\n",
    "#         hp.values['initial_filters'] = 384\n",
    "#         hp.values['res_block_1_filters'] = 384\n",
    "#         hp.values['res_block_2_filters'] = 384\n",
    "#         hp.values['kernel_size_initial'] = 7\n",
    "#         hp.values['kernel_size_res'] = 5\n",
    "#         hp.values['dropout_1'] = 0.1\n",
    "#         hp.values['dropout_2'] = 0.3\n",
    "#         hp.values['dense_units'] = 512\n",
    "#         hp.values['dense_dropout'] = 0.5\n",
    "#         hp.values['learning_rate'] = 0.0001\n",
    "\n",
    "#     def residual_block(x, filters, kernel_size):\n",
    "#         y = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "#         y = BatchNormalization()(y)\n",
    "#         y = Activation('relu')(y)\n",
    "#         y = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(y)\n",
    "#         y = BatchNormalization()(y)\n",
    "#         shortcut = Conv1D(filters=filters, kernel_size=1, padding='same')(x) if x.shape[-1] != filters else x\n",
    "#         res_output = Add()([shortcut, y])\n",
    "#         return Activation('relu')(res_output)\n",
    "\n",
    "#     inputs = Input(shape=input_shape)\n",
    "#     x = Conv1D(filters=hp.values['initial_filters'], kernel_size=hp.values['kernel_size_initial'], padding='same')(inputs)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "#     x = residual_block(x, filters=hp.values['res_block_1_filters'], kernel_size=hp.values['kernel_size_res'])\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "#     x = Dropout(hp.values['dropout_1'])(x)\n",
    "#     x = residual_block(x, filters=hp.values['res_block_2_filters'], kernel_size=hp.values['kernel_size_res'])\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "#     x = Dropout(hp.values['dropout_2'])(x)\n",
    "#     x = GlobalAveragePooling1D()(x)\n",
    "#     x = Dense(hp.values['dense_units'], activation='relu')(x)\n",
    "#     x = Dropout(hp.values['dense_dropout'])(x)\n",
    "#     outputs = Dense(output_dim, activation='softmax')(x)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=hp.values['learning_rate']),\n",
    "#         loss='categorical_crossentropy',\n",
    "#         metrics=[\n",
    "#             # 'accuracy',\n",
    "#             tf.keras.metrics.Precision(name='precision'),\n",
    "#             tf.keras.metrics.Recall(name='recall'),\n",
    "#             # Note: F1Score might require a different setup in some TF versions.\n",
    "#             # If it causes issues, consider a custom callback to calculate it.\n",
    "#             tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "#             tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "#         ]\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# Function to create the RandomForest model\n",
    "def create_rf_model():\n",
    "    \"\"\"Creates an instance of the GPU-accelerated RandomForestClassifier model using cuML.\"\"\"\n",
    "    # Hyperparameters are similar to imblearn's\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=30, \n",
    "        random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16e19d-a20a-4308-9848-37de070b26a3",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL TRAINING EXECUTION\n",
    "1. Targeted metrics: Precision, Recall, F1-Score, and Specificity\n",
    "2. After training, all models immediately saved and exported into a certain folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2cd67-b661-4d6b-a3fc-d87dd454416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Multiple Models: Single-fold validation\n",
    "input_shape_cnn = (X_train_cnn.shape[1], X_train_cnn.shape[2])\n",
    "input_dim = X_train_mlp.shape[1]\n",
    "output_dim = y_train_mlp.shape[1]\n",
    "\n",
    "# Saving/exporting models\n",
    "output_dir = '../models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"--- Manually Calculating Class Weights for Cost-Sensitive Learning ---\")\n",
    "# 1. Count the number of samples in each class using np.bincount.\n",
    "# `y_train_fold` should be a 1D array of integer class labels (e.g., [0, 1, 1, 2, 0]).\n",
    "class_counts = np.bincount(y_train_fold)\n",
    "# 2. Get the total number of samples and classes.\n",
    "n_samples = len(y_train_fold)\n",
    "n_classes = len(class_counts)\n",
    "# 3. Calculate the weight for each class using the standard formula.\n",
    "manual_weights = n_samples / (n_classes * class_counts)\n",
    "# 4. Create the dictionary that Keras expects, mapping class indices to weights.\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(manual_weights)}\n",
    "\n",
    "print(\"Class Counts:\", dict(enumerate(class_counts)))\n",
    "print(\"Manually Calculated Weights:\", class_weights_dict)\n",
    "\n",
    "models = {\n",
    "    \"1D-CNN\": create_cnn_model_optimized(input_shape_cnn, output_dim),\n",
    "    \"RandomForest\": create_rf_model(),\n",
    "    \"MLP\": create_mlp_model(input_dim, output_dim)\n",
    "}\n",
    "\n",
    "# Dictionary to store the final results\n",
    "results = {}\n",
    "\n",
    "# --- TRAINING AND EVALUATING EACH MODEL ---\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*20} TRAINING MODEL: {name} {'='*20}\")\n",
    "\n",
    "    # ðŸ§  Training\n",
    "    if name == \"1D-CNN\":\n",
    "        model.fit(\n",
    "            X_train_cnn, y_train_cnn,\n",
    "            epochs=150, # Reduced for quick example\n",
    "            batch_size=100,\n",
    "            verbose=1, # Set to 0 to keep output clean\n",
    "            validation_data=(X_val_cnn, y_val_cnn),\n",
    "            class_weight=class_weights_dict\n",
    "        )\n",
    "    elif name == \"MLP\":\n",
    "        model.fit(\n",
    "            X_train_mlp, y_train_mlp,\n",
    "            epochs=100, # Reduced for quick example\n",
    "            batch_size=200,\n",
    "            verbose=1,\n",
    "            validation_data=(X_val_mlp, y_val_mlp),\n",
    "            class_weight=class_weights_dict\n",
    "        )\n",
    "    else: # ðŸ“Š RandomForest\n",
    "        model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "    # âš¡ Prediction on the Test Set\n",
    "    print(f\"Evaluating model {name}...\")\n",
    "    if name in [\"MLP\", \"1D-CNN\"]:\n",
    "        y_pred_raw = model.predict(X_test_mlp if name == \"MLP\" else X_test_cnn)\n",
    "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "    else: # RandomForest\n",
    "        y_pred = model.predict(X_test_rf)\n",
    "\n",
    "    # Store prediction results and ground truth for final evaluation\n",
    "    results[name] = {'y_pred': y_pred, 'y_true': y_test_final}\n",
    "\n",
    "# --- PRINT ALL RESULTS SIMULTANEOUSLY ---\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)']\n",
    "\n",
    "print(f\"\\n{'='*25} FINAL EVALUATION RESULTS {'='*25}\")\n",
    "\n",
    "for name, result_data in results.items():\n",
    "    y_true = result_data['y_true']\n",
    "    y_pred = result_data['y_pred']\n",
    "\n",
    "    print(f\"\\n\\n{'~'*15} REPORT FOR MODEL: {name} {'~'*15}\")\n",
    "    \n",
    "    # --- Classification Report ---\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    # --- Confusion Matrix Visualization ---\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix for {name}', fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    # SAVING MODEL AFTER TRAINING\n",
    "    print(f\"--- Saving model: {name} ---\")\n",
    "    if name in [\"1D-CNN\", \"MLP\"]:\n",
    "        # TensorFlow/Keras models\n",
    "        model_path = os.path.join(output_dir, f\"model_{name.lower()}_saved\")\n",
    "        model.export(model_path) # Saving models\n",
    "        print(f\"âœ… Model {name} has been saved on: {model_path}\")\n",
    "    else: #RandomForest/other scikit-learn models\n",
    "        model_path = os.path.join(output_dir, f\"model_{name.lower()}.joblib\")\n",
    "        joblib.dump(model, model_path) # Saving models\n",
    "        print(f\"âœ… Model {name} has been saved on: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4727c1b-c49a-4e5e-b732-89a429590bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Multiple Models: Ten-fold cross validation\n",
    "# --- SETUP ---\n",
    "input_shape_cnn = (X_cnn_full.shape[1], X_cnn_full.shape[2])\n",
    "input_dim = X_mlp_full.shape[1]\n",
    "output_dim = y_full_categorical.shape[1] # Assuming y is one-hot encoded for NN models\n",
    "\n",
    "# Saving/exporting models\n",
    "output_dir = '../models'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Dictionary to store cross-validation results (e.g., accuracies)\n",
    "cv_results = {}\n",
    "# Dictionary to store final predictions on the test set\n",
    "final_results = {}\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)']\n",
    "\n",
    "\n",
    "# --- 10-FOLD CROSS-VALIDATION LOOP ---\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Define model creation functions in a dictionary\n",
    "# This allows re-creating a fresh, untrained model for each fold\n",
    "model_creators = {\n",
    "    \"1D-CNN\": lambda: create_cnn_model_optimized(input_shape_cnn, output_dim),\n",
    "    \"RandomForest\": lambda: create_rf_model(),\n",
    "    \"MLP\": lambda: create_mlp_model(input_dim, output_dim)\n",
    "}\n",
    "\n",
    "# Use the non-categorical labels for splitting to ensure stratification\n",
    "y_labels_for_split = np.argmax(y_full_categorical, axis=1)\n",
    "\n",
    "for name, create_model in model_creators.items():\n",
    "    print(f\"\\n{'='*25} RUNNING {n_splits}-FOLD CV FOR: {name} {'='*25}\")\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    \n",
    "    # Determine the correct dataset for splitting\n",
    "    if name == \"1D-CNN\":\n",
    "        X_data = X_cnn_full\n",
    "    elif name == \"MLP\":\n",
    "        X_data = X_mlp_full\n",
    "    else: # RandomForest\n",
    "        X_data = X_rf_full\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X_data, y_labels_for_split)):\n",
    "        print(f\"\\n--- FOLD {fold + 1}/{n_splits} ---\")\n",
    "\n",
    "        # --- 1. Split Data for Current Fold ---\n",
    "        X_train_fold, X_val_fold = X_data[train_index], X_data[val_index]\n",
    "        \n",
    "        # Handle labels for different model types\n",
    "        if name in [\"1D-CNN\", \"MLP\"]:\n",
    "            y_train_fold, y_val_fold = y_full_categorical[train_index], y_full_categorical[val_index]\n",
    "            y_train_fold_labels = y_labels_for_split[train_index] # For class weights\n",
    "        else: # RandomForest uses 1D labels\n",
    "            y_train_fold, y_val_fold = y_labels_for_split[train_index], y_labels_for_split[val_index]\n",
    "\n",
    "        # --- 2. Calculate Class Weights for this Fold's Training Data ---\n",
    "        class_counts = np.bincount(y_train_fold_labels if name in [\"1D-CNN\", \"MLP\"] else y_train_fold)\n",
    "        n_samples = len(y_train_fold)\n",
    "        n_classes = len(class_counts)\n",
    "        manual_weights = n_samples / (n_classes * class_counts)\n",
    "        class_weights_dict = {i: weight for i, weight in enumerate(manual_weights)}\n",
    "        print(\"Fold Class Weights:\", class_weights_dict)\n",
    "\n",
    "        # --- 3. Create and Train a New Model Instance ---\n",
    "        model = create_model()\n",
    "\n",
    "        if name == \"1D-CNN\":\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                epochs=150, batch_size=100, verbose=0,\n",
    "                validation_data=(X_val_fold, y_val_fold),\n",
    "                class_weight=class_weights_dict\n",
    "            )\n",
    "        elif name == \"MLP\":\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                epochs=100, batch_size=200, verbose=0,\n",
    "                validation_data=(X_val_fold, y_val_fold),\n",
    "                class_weight=class_weights_dict\n",
    "            )\n",
    "        else: # RandomForest\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # --- 4. Evaluate on the Validation Set for this Fold ---\n",
    "        if name in [\"1D-CNN\", \"MLP\"]:\n",
    "            y_pred_raw = model.predict(X_val_fold)\n",
    "            y_pred_fold = np.argmax(y_pred_raw, axis=1)\n",
    "            y_true_fold = np.argmax(y_val_fold, axis=1)\n",
    "        else: # RandomForest\n",
    "            y_pred_fold = model.predict(X_val_fold)\n",
    "            y_true_fold = y_val_fold\n",
    "            \n",
    "        acc = accuracy_score(y_true_fold, y_pred_fold)\n",
    "        fold_accuracies.append(acc)\n",
    "        print(f\"Fold {fold + 1} Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # --- 5. Calculate and Store Average CV Performance ---\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    std_accuracy = np.std(fold_accuracies)\n",
    "    cv_results[name] = {'mean_accuracy': mean_accuracy, 'std_accuracy': std_accuracy}\n",
    "    print(f\"\\nAverage CV Accuracy for {name}: {mean_accuracy:.4f} (+/- {std_accuracy:.4f})\")\n",
    "\n",
    "\n",
    "# --- FINAL TRAINING & EVALUATION ON TEST SET ---\n",
    "print(f\"\\n\\n{'='*25} FINAL MODEL TRAINING & EVALUATION {'='*25}\")\n",
    "\n",
    "for name, create_model in model_creators.items():\n",
    "    print(f\"\\n{'~'*20} TRAINING FINAL MODEL: {name} ON ALL DATA {'~'*20}\")\n",
    "\n",
    "    # --- 1. Create Final Model and Train on the FULL Training Dataset ---\n",
    "    final_model = create_model()\n",
    "    \n",
    "    # Recalculate class weights on the full dataset\n",
    "    full_class_counts = np.bincount(y_labels_for_split)\n",
    "    full_n_samples = len(y_labels_for_split)\n",
    "    full_n_classes = len(full_class_counts)\n",
    "    full_manual_weights = full_n_samples / (full_n_classes * full_class_counts)\n",
    "    full_class_weights_dict = {i: weight for i, weight in enumerate(full_manual_weights)}\n",
    "    print(\"Final Model Class Weights:\", full_class_weights_dict)\n",
    "\n",
    "    if name == \"1D-CNN\":\n",
    "        final_model.fit(\n",
    "            X_cnn_full, y_full_categorical,\n",
    "            epochs=150, batch_size=100, verbose=1,\n",
    "            class_weight=full_class_weights_dict\n",
    "        )\n",
    "    elif name == \"MLP\":\n",
    "        final_model.fit(\n",
    "            X_mlp_full, y_full_categorical,\n",
    "            epochs=100, batch_size=200, verbose=1,\n",
    "            class_weight=full_class_weights_dict\n",
    "        )\n",
    "    else: # RandomForest\n",
    "        final_model.fit(X_rf_full, y_labels_for_split)\n",
    "\n",
    "    # --- 2. Evaluate on the Hold-Out Test Set ---\n",
    "    print(f\"Evaluating final {name} model on the test set...\")\n",
    "    if name in [\"MLP\", \"1D-CNN\"]:\n",
    "        X_test_data = X_test_mlp if name == \"MLP\" else X_test_cnn\n",
    "        y_pred_raw = final_model.predict(X_test_data)\n",
    "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "    else: # RandomForest\n",
    "        y_pred = final_model.predict(X_test_rf)\n",
    "    \n",
    "    # Store results for final report\n",
    "    final_results[name] = {'y_pred': y_pred, 'y_true': y_test_final}\n",
    "\n",
    "    # --- 3. Save the Final Model ---\n",
    "    print(f\"--- Saving final model: {name} ---\")\n",
    "    if name in [\"1D-CNN\", \"MLP\"]:\n",
    "        model_path = os.path.join(output_dir, f\"final_model_{name.lower()}_saved\")\n",
    "        final_model.save(model_path)\n",
    "    else:\n",
    "        model_path = os.path.join(output_dir, f\"final_model_{name.lower()}.joblib\")\n",
    "        joblib.dump(final_model, model_path)\n",
    "    print(f\"âœ… Final model {name} saved to: {model_path}\")\n",
    "\n",
    "# --- PRINT ALL FINAL RESULTS ---\n",
    "\n",
    "print(f\"\\n\\n{'='*25} CROSS-VALIDATION SUMMARY {'='*25}\")\n",
    "for name, metrics in cv_results.items():\n",
    "    print(f\"{name}: Mean Accuracy = {metrics['mean_accuracy']:.4f} (Std Dev = {metrics['std_accuracy']:.4f})\")\n",
    "\n",
    "print(f\"\\n\\n{'='*25} FINAL TEST SET EVALUATION REPORTS {'='*25}\")\n",
    "for name, result_data in final_results.items():\n",
    "    y_true = result_data['y_true']\n",
    "    y_pred = result_data['y_pred']\n",
    "\n",
    "    print(f\"\\n\\n{'~'*15} REPORT FOR FINAL MODEL: {name} {'~'*15}\")\n",
    "    \n",
    "    # --- Classification Report ---\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    # --- Confusion Matrix Visualization ---\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Final Confusion Matrix for {name}', fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
