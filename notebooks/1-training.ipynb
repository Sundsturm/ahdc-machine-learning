{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e32954",
   "metadata": {},
   "source": [
    "## **LIBRARY IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf0b717-46b3-4967-962f-6d5f2eae8848",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to dlopen libcudart.so.12",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import Libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcudf\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/OSOTNAS/Documents/Kean/Others/KP_Xirka-Darma-Persada/machine-learning/.venv/lib/python3.12/site-packages/cudf/__init__.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcudf\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgpu_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_setup\n\u001b[32m     19\u001b[39m _setup_numba()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mvalidate_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m _setup_numba\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m validate_setup\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/OSOTNAS/Documents/Kean/Others/KP_Xirka-Darma-Persada/machine-learning/.venv/lib/python3.12/site-packages/cudf/utils/gpu_utils.py:109\u001b[39m, in \u001b[36mvalidate_setup\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     99\u001b[39m     minor_version = getDeviceAttribute(\n\u001b[32m    100\u001b[39m         cudaDeviceAttr.cudaDevAttrComputeCapabilityMinor, \u001b[32m0\u001b[39m\n\u001b[32m    101\u001b[39m     )\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedCUDAError(\n\u001b[32m    103\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA GPU with NVIDIA Voltaâ„¢ (Compute Capability 7.0) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    104\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor newer architecture is required.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDetected GPU 0: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDetected Compute Capability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmajor_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminor_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m cuda_runtime_version = \u001b[43mruntimeGetVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cuda_runtime_version < \u001b[32m11000\u001b[39m:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# Require CUDA Runtime version 11.0 or greater.\u001b[39;00m\n\u001b[32m    113\u001b[39m     major_version = cuda_runtime_version // \u001b[32m1000\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/OSOTNAS/Documents/Kean/Others/KP_Xirka-Darma-Persada/machine-learning/.venv/lib/python3.12/site-packages/rmm/_cuda/gpu.py:86\u001b[39m, in \u001b[36mruntimeGetVersion\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mruntimeGetVersion\u001b[39m():\n\u001b[32m     77\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03m    Returns the version number of the local CUDA runtime.\u001b[39;00m\n\u001b[32m     79\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m    and status code.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     status, version = \u001b[43mruntime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetLocalRuntimeVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != runtime.cudaError_t.cudaSuccess:\n\u001b[32m     88\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CUDARuntimeError(status)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/OSOTNAS/Documents/Kean/Others/KP_Xirka-Darma-Persada/machine-learning/.venv/lib/python3.12/site-packages/cuda/bindings/runtime.pyx:32426\u001b[39m, in \u001b[36mcuda.bindings.runtime.getLocalRuntimeVersion\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/OSOTNAS/Documents/Kean/Others/KP_Xirka-Darma-Persada/machine-learning/.venv/lib/python3.12/site-packages/cuda/bindings/cyruntime.pyx:953\u001b[39m, in \u001b[36mcuda.bindings.cyruntime.getLocalRuntimeVersion\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to dlopen libcudart.so.12"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import cudf\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb  # For reading MIT-BIH data\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Scikit-learn and Imbalanced-learn imports\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Model imports\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Additional setups\n",
    "# Checking cUML\n",
    "print(cudf.Series([1, 2, 3]))\n",
    "\n",
    "# Setting TensorFlow flags\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Checking GPU\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"TensorFlow has detected {len(gpu_devices)} GPU(s):\")\n",
    "    for device in gpu_devices:\n",
    "        print(f\"- {device}\")\n",
    "else:\n",
    "    print(\"TensorFlow did not detect any GPUs. Training will run on the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348ca32-352e-4791-a764-9a1dedd1af04",
   "metadata": {},
   "source": [
    "## **DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb05f79-645a-4ff3-b8f5-44e867b01a39",
   "metadata": {},
   "source": [
    "### DATA PREPARATION FUNCTIONS\n",
    "There are 3 types of functions:\n",
    "1. Labels and windowed features of the RR intervals\n",
    "2. Data preparation of MIT-BIH dataset as training and validation set\n",
    "3. Data preparation of additional ECG data with a format of .bin as the final testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a11cb1-fc53-445e-933c-49f337cb0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LABELS AND WINDOWED FEATURES\n",
    "label_map = { 'N': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0,  # Normal Beats (N)\n",
    "              'V': 1, 'E': 1,                          # Ventricular Ectopic (VEB)\n",
    "              'S': 2, 'A': 2, 'a': 2, 'J': 2,          # Supraventricular Ectopic (SVEB)\n",
    "              'F': 3,                                  # Fusion Beat (F)\n",
    "              'P': 4, 'f': 4, 'Q': 4, '?': 4, \"U\": 4}          # Unknown Beat (Q)\n",
    "\n",
    "# Dividing data into X and Y axis\n",
    "def create_windowed_features(rr_intervals, labels, window_size):\n",
    "    \"\"\"Making a windowed features of RR intervals\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(rr_intervals) - window_size):\n",
    "        segment = rr_intervals[i:i+window_size]\n",
    "        # The label is according to the beat on the last window\n",
    "        label = labels[i + window_size - 1]\n",
    "        X.append(segment)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03deef09-ac69-442e-96e2-a2d354872eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATA PREPARATION OF MIT-BIH DATASET\n",
    "# Reading MIT-BIH dataset\n",
    "def load_mitbih_data(record_names, db_path):\n",
    "    \"\"\"\n",
    "    Load ECG signals and the annotations from the MIT-BIH dataset\n",
    "    Args:\n",
    "        record_name (str): Record name (e.g.'100').\n",
    "        db_path (str): MIT-BIH datapath (misal, 'mit-bih').\n",
    "\n",
    "    Return:\n",
    "        Returning raw ECG data (signal) and labels (annotation) from each record in an array of numpy\n",
    "        tuple: (signal, annotation, fs)\n",
    "    \"\"\"\n",
    "    signals, annotations = [], []\n",
    "    for record in record_names:\n",
    "        record_path = f'{db_path}/{record}'\n",
    "        # Reading signal from the first channel\n",
    "        signal = wfdb.rdrecord(record_path, channels=[0]).p_signal.flatten() # Other than .dat\n",
    "        annotation = wfdb.rdann(record_path, 'atr') # .dat\n",
    "        signals.append(signal)\n",
    "        annotations.append(annotation)\n",
    "    return signals, annotations\n",
    "\n",
    "# Reading annotations and labels from each data that has been loaded \n",
    "def extract_rr_intervals_and_labels(annotations):\n",
    "    \"\"\"Extracting RR intervals and labels from the corresponding heartbeat.\"\"\"\n",
    "    all_rr, labels = [], []\n",
    "    for ann in annotations:\n",
    "        r_peaks = ann.sample\n",
    "        beat_symbols = ann.symbol\n",
    "        # Butuh setidaknya dua R-peak untuk menghitung interval\n",
    "        for i in range(1, len(r_peaks)):\n",
    "            symbol = beat_symbols[i]\n",
    "            if symbol in label_map:\n",
    "                rr_interval = (r_peaks[i] - r_peaks[i-1]) / ann.fs  # Gunakan frekuensi sampling spesifik rekaman\n",
    "                all_rr.append(rr_interval)\n",
    "                labels.append(label_map[symbol])\n",
    "    return np.array(all_rr), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f269e46-0c80-4702-b7e2-bfd77c68fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DATA PREPARATION OF .bin ECG DATA\n",
    "# Reading ECG data\n",
    "def load_ecg_from_bin(file_path, dtype=np.int16):\n",
    "    \"\"\"\n",
    "    Loading raw ECG signals from binary files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .bin file.\n",
    "        dtype (numpy.dtype): Data type of the signal in the .bin file.\n",
    "\n",
    "    Return:\n",
    "        numpy.ndarray: ECG signals as a numpy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Reading .bin file and converting it into a numpy array\n",
    "        signal = np.fromfile(file_path, dtype=dtype)\n",
    "        print(f\"Completed reading {len(signal)} samples from {file_path}\")\n",
    "        return signal\n",
    "    except IOError as e:\n",
    "        print(f\"An error has occurred while reading: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_r_peaks(signal, fs):\n",
    "    \"\"\"\n",
    "    Detecting R-peaks from the ECG signal.\n",
    "\n",
    "    Args:\n",
    "        signal (numpy.ndarray): Raw ECG signal in a numpy array.\n",
    "        fs (int): Sampling frequency of the ECG signal.\n",
    "\n",
    "    Return:\n",
    "        numpy.ndarray: An array of indices of the detected R-peaks.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 1: R-Peak Detection ---\")\n",
    "    # 'height' and 'distance' can be configured according to the signal.\n",
    "    height_threshold = np.max(signal) * 0.5\n",
    "    distance_threshold = fs * 0.4  # Minimum distance between heartbeats.\n",
    "\n",
    "    r_peaks, _ = find_peaks(signal, height=height_threshold, distance=distance_threshold)\n",
    "\n",
    "    print(f\"Detected {len(r_peaks)} R-peaks.\")\n",
    "    return r_peaks\n",
    "\n",
    "def extract_rr_and_apply_label_ecg_bin(r_peaks, fs, record_label):\n",
    "    \"\"\"\n",
    "    Calculates RR intervals from a single record and assigns the same single label\n",
    "    to all of those intervals.\n",
    "\n",
    "    Args:\n",
    "        r_peaks (numpy.ndarray): Array containing R-peak locations (in sample indices) from a single record.\n",
    "        fs (int): Sampling frequency of the signal.\n",
    "        record_label (any): A single label (e.g., string or integer) that will\n",
    "                            be applied to this entire record.\n",
    "\n",
    "    Return:\n",
    "        tuple: A tuple containing (rr_intervals, labels).\n",
    "               - rr_intervals (numpy.ndarray): Array containing RR intervals in seconds.\n",
    "               - labels (numpy.ndarray): Array containing the same label for each RR interval.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Step 2: RR Extraction and Labeling for the Record ---\")\n",
    "\n",
    "    # Ensure there are enough R-peaks to calculate at least one interval\n",
    "    if len(r_peaks) < 2:\n",
    "        print(\"Warning: Not enough R-peaks to calculate RR intervals.\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Calculate all RR intervals in seconds\n",
    "    rr_intervals = np.diff(r_peaks) / fs\n",
    "\n",
    "    # Create a label array where each element is 'record_label'\n",
    "    # The size of this label array is the same as the number of calculated RR intervals\n",
    "    num_rr_intervals = len(rr_intervals)\n",
    "    labels = np.full(shape=num_rr_intervals, fill_value=record_label)\n",
    "\n",
    "    return rr_intervals, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c0215-637d-49cd-8049-379035e45e3f",
   "metadata": {},
   "source": [
    "### DATA PREPARATION EXECUTION\n",
    "1. Reading all ECG datasets\n",
    "2. Divide all datasets into training dataset and testing dataset\n",
    "3. Standard scaling and combining datasets\n",
    "4. Splitting training dataset into training split and validation split then applying SMOTE algorithm into the training split\n",
    "5. Preparing all the datasets for each machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea82e85c-06ca-4dd4-9d1b-cb0b177484bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸš€ STARTING DATASET PREPARATION PROCESS ðŸš€\n",
      "============================================================\n",
      "\n",
      "--- [Step 1/5] Processing Training Data (ds1) ---\n",
      "Raw training data ready: X_train=(51011, 10), y_train=(51011,)\n",
      "\n",
      "--- [Step 2/5] Processing Testing Data ---\n",
      "\n",
      "Processing testing part 1 (ds2)...\n",
      "MIT-BIH testing data ready: X_test_mitbih=(49702, 10), y_test_mitbih=(49702,)\n",
      "\n",
      "Processing testing part 2 (.bin)...\n",
      "Completed reading 2380000 samples from ../data/raw/Arrhythmia/ECG_WAVE.bin\n",
      "\n",
      "--- Step 1: R-Peak Detection ---\n",
      "Detected 171 R-peaks.\n",
      "\n",
      "--- Step 2: RR Extraction and Labeling for the Record ---\n",
      "Completed reading 2135000 samples from ../data/raw/Normal/ecg_normal.bin\n",
      "\n",
      "--- Step 1: R-Peak Detection ---\n",
      "Detected 154 R-peaks.\n",
      "\n",
      "--- Step 2: RR Extraction and Labeling for the Record ---\n",
      "\n",
      "--- [Step 3/5] Scaling and Finalizing Data ---\n",
      "Scaler trained on training data.\n",
      "Scaler applied to all testing data.\n",
      "Final testing data combined: X_test_final=(50015, 10), y_test_final=(50015,)\n",
      "\n",
      "--- [Step 4/5] Finalizing Training Data (Split & SMOTE) ---\n",
      "Creating validation set from training data (80/20)...\n",
      "Applying SMOTE only to the training fold...\n",
      "Training class distribution before SMOTE: Counter({np.int64(0): 36684, np.int64(1): 3030, np.int64(2): 755, np.int64(3): 332, np.int64(4): 7})\n",
      "Training class distribution after SMOTE: Counter({np.int64(0): 36684, np.int64(1): 36684, np.int64(2): 36684, np.int64(3): 36684, np.int64(4): 36684})\n",
      "\n",
      "--- [Step 5/5] Preparing Final Dataset for the Model ---\n",
      "\n",
      "============================================================\n",
      "âœ… DATA PREPARATION COMPLETE âœ…\n",
      "The following variables are ready to be used for training and evaluation:\n",
      "============================================================\n",
      "\n",
      "--- For MLP ---\n",
      "  Training:   X_train_mlp: (183420, 10), y_train_mlp: (183420, 5)\n",
      "  Validation: X_val_mlp: (10203, 10), y_val_mlp: (10203, 5)\n",
      "  Testing:    X_test_mlp: (50015, 10), y_test_mlp: (50015, 5)\n",
      "\n",
      "--- For 1D-CNN ---\n",
      "  Training:   X_train_cnn: (183420, 10, 1), y_train_cnn: (183420, 5)\n",
      "  Validation: X_val_cnn: (10203, 10, 1), y_val_cnn: (10203, 5)\n",
      "  Testing:    X_test_cnn: (50015, 10, 1), y_test_cnn: (50015, 5)\n",
      "\n",
      "--- For RandomForest ---\n",
      "  Training:   X_train_rf: (183420, 10), y_train_rf: (183420,)\n",
      "  Validation: X_val_rf: (10203, 10), y_val_rf: (10203,)\n",
      "  Testing:    X_test_rf: (50015, 10), y_test_rf: (50015,)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # --- 0. INITIAL PARAMETERS FOR .BIN AND MIT-BIH DATA PREPARATION ---\n",
    "    mitbih_dir = '../data/raw/MIT-BIH/mit-bih-arrhythmia-database-1.0.0/mit-bih-arrhythmia-database-1.0.0/'\n",
    "    window_size = 10\n",
    "    # Excluding 102, 104, 107, and 217\n",
    "    # all_records = ['100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '111',\n",
    "    #                '112', '113', '114', '115', '116', '117', '118', '119', '121', '122', '123',\n",
    "    #                '124', '200', '201', '202', '203', '205', '207', '208', '209', '210', '212',\n",
    "    #                '213', '214', '215', '217', '219', '220', '221', '222', '223', '228', '230',\n",
    "    #                '231', '232', '233', '234']\n",
    "    ds1 = ['101', '106', '108', '109', '112', '114', '115', '116', '118', '119',\n",
    "           '122', '124', '201', '203', '205', '207', '208', '209', '215', '220',\n",
    "           '223', '230'] # Used for training\n",
    "    ds2 = ['100', '103', '105', '111', '113', '117', '121', '123', '200', '202',\n",
    "           '210', '212', '213', '214', '219', '221', '222', '228', '231', '232',\n",
    "           '233', '234'] # Used for evaluation\n",
    "    FS_CUSTOM = 500  # IMPORTANT: Adjust according to the sampling frequency of your .bin data\n",
    "    custom_file_paths = {\n",
    "        'Arrhythmia': '../data/raw/Arrhythmia/ECG_WAVE.bin',\n",
    "        'Normal': '../data/raw/Normal/ecg_normal.bin'\n",
    "    }\n",
    "    custom_file_labels = {'Arrhythmia': 2, 'Normal': 0}\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"ðŸš€ STARTING DATASET PREPARATION PROCESS ðŸš€\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- 1. PROCESS TRAINING DATA (ds1) ---\n",
    "    print(\"\\n--- [Step 1/5] Processing Training Data (ds1) ---\")\n",
    "    signals_train, annotations_train = load_mitbih_data(ds1, mitbih_dir)\n",
    "    rr_train, labels_train = extract_rr_intervals_and_labels(annotations_train)\n",
    "    X_train, y_train = create_windowed_features(rr_train, labels_train, window_size)\n",
    "    print(f\"Raw training data ready: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "\n",
    "    # --- 2. PROCESS TESTING DATA (Combined ds2 and .bin) ---\n",
    "    print(\"\\n--- [Step 2/5] Processing Testing Data ---\")\n",
    "\n",
    "    # Part A: Process testing data from MIT-BIH (ds2)\n",
    "    print(\"\\nProcessing testing part 1 (ds2)...\")\n",
    "    signals_test_mitbih, annotations_test_mitbih = load_mitbih_data(ds2, mitbih_dir)\n",
    "    rr_test_mitbih, labels_test_mitbih = extract_rr_intervals_and_labels(annotations_test_mitbih)\n",
    "    X_test_mitbih, y_test_mitbih = create_windowed_features(rr_test_mitbih, labels_test_mitbih, window_size)\n",
    "    print(f\"MIT-BIH testing data ready: X_test_mitbih={X_test_mitbih.shape}, y_test_mitbih={y_test_mitbih.shape}\")\n",
    "\n",
    "    # Part B: Process testing data from .bin files\n",
    "    print(\"\\nProcessing testing part 2 (.bin)...\")\n",
    "    all_rr_custom = []\n",
    "    all_labels_custom = []\n",
    "    for category, path in custom_file_paths.items():\n",
    "        signal_custom = load_ecg_from_bin(path)\n",
    "        if signal_custom is not None:\n",
    "            r_peaks_custom = detect_r_peaks(signal_custom, fs=FS_CUSTOM)\n",
    "            rr_intervals_c, labels_c = extract_rr_and_apply_label_ecg_bin(\n",
    "                r_peaks_custom, fs=FS_CUSTOM, record_label=custom_file_labels[category]\n",
    "            )\n",
    "            all_rr_custom.append(rr_intervals_c)\n",
    "            all_labels_custom.append(labels_c)\n",
    "\n",
    "    rr_test_custom = np.concatenate(all_rr_custom)\n",
    "    labels_test_custom = np.concatenate(all_labels_custom)\n",
    "    X_test_custom, y_test_custom = create_windowed_features(rr_test_custom, labels_test_custom, window_size)\n",
    "\n",
    "    # --- 3. SCALING & COMBINING TESTING DATA ---\n",
    "    print(\"\\n--- [Step 3/5] Scaling and Finalizing Data ---\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    print(\"Scaler trained on training data.\")\n",
    "\n",
    "    # Apply the scaler to all parts of the testing data\n",
    "    X_test_mitbih_scaled = scaler.transform(X_test_mitbih)\n",
    "    X_test_custom_scaled = scaler.transform(X_test_custom)\n",
    "    print(\"Scaler applied to all testing data.\")\n",
    "\n",
    "    # Combine all scaled testing data\n",
    "    X_test_final = np.concatenate((X_test_mitbih_scaled, X_test_custom_scaled), axis=0)\n",
    "    y_test_final = np.concatenate((y_test_mitbih, y_test_custom), axis=0)\n",
    "    print(f\"Final testing data combined: X_test_final={X_test_final.shape}, y_test_final={y_test_final.shape}\")\n",
    "\n",
    "    # --- 4. TRAINING SET SPLITTING & OVERSAMPLING (SMOTE) ---\n",
    "    print(\"\\n--- [Step 4/5] Finalizing Training Data (Split & SMOTE) ---\")\n",
    "    output_dim = len(np.unique(y_train))\n",
    "    \n",
    "    print(\"Creating validation set from training data (80/20)...\")\n",
    "    X_train_fold, X_val, y_train_fold, y_val = train_test_split(\n",
    "        X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    print(\"Applying SMOTE only to the training fold...\")\n",
    "    print(\"Training class distribution before SMOTE:\", Counter(y_train_fold))\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "    print(\"Training class distribution after SMOTE:\", Counter(y_train_resampled))\n",
    "\n",
    "    # --- 5. FINAL DATA PREPARATION FOR THE MODEL ---\n",
    "    print(\"\\n--- [Step 5/5] Preparing Final Dataset for the Model ---\")\n",
    "\n",
    "    # One-hot encoding labels for Keras\n",
    "    y_train_encoded = to_categorical(y_train_resampled, num_classes=output_dim)\n",
    "    y_val_encoded = to_categorical(y_val, num_classes=output_dim)\n",
    "    y_test_final_encoded = to_categorical(y_test_final, num_classes=output_dim)\n",
    "\n",
    "    # ðŸ§  Data for MLP\n",
    "    X_train_mlp, y_train_mlp = X_train_resampled, y_train_encoded\n",
    "    X_val_mlp, y_val_mlp = X_val, y_val_encoded\n",
    "    X_test_mlp, y_test_mlp = X_test_final, y_test_final_encoded\n",
    "\n",
    "    # âš¡ Data for 1D-CNN\n",
    "    X_train_cnn = X_train_mlp.reshape((X_train_mlp.shape[0], X_train_mlp.shape[1], 1))\n",
    "    X_val_cnn = X_val_mlp.reshape((X_val_mlp.shape[0], X_val_mlp.shape[1], 1))\n",
    "    X_test_cnn = X_test_mlp.reshape((X_test_mlp.shape[0], X_test_mlp.shape[1], 1))\n",
    "    y_train_cnn, y_val_cnn, y_test_cnn = y_train_mlp, y_val_mlp, y_test_mlp\n",
    "\n",
    "    # ðŸ“Š Data for RandomForest\n",
    "    X_train_rf, y_train_rf = X_train_resampled, y_train_resampled\n",
    "    X_val_rf, y_val_rf = X_val, y_val\n",
    "    X_test_rf, y_test_rf = X_test_final, y_test_final\n",
    "\n",
    "    # --- FINAL RESULTS ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… DATA PREPARATION COMPLETE âœ…\")\n",
    "    print(\"The following variables are ready to be used for training and evaluation:\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"\\n--- For MLP ---\")\n",
    "    print(f\"  Training:   X_train_mlp: {X_train_mlp.shape}, y_train_mlp: {y_train_mlp.shape}\")\n",
    "    print(f\"  Validation: X_val_mlp: {X_val_mlp.shape}, y_val_mlp: {y_val_mlp.shape}\")\n",
    "    print(f\"  Testing:    X_test_mlp: {X_test_mlp.shape}, y_test_mlp: {y_test_mlp.shape}\")\n",
    "\n",
    "    print(\"\\n--- For 1D-CNN ---\")\n",
    "    print(f\"  Training:   X_train_cnn: {X_train_cnn.shape}, y_train_cnn: {y_train_cnn.shape}\")\n",
    "    print(f\"  Validation: X_val_cnn: {X_val_cnn.shape}, y_val_cnn: {y_val_cnn.shape}\")\n",
    "    print(f\"  Testing:    X_test_cnn: {X_test_cnn.shape}, y_test_cnn: {y_test_cnn.shape}\")\n",
    "\n",
    "    print(\"\\n--- For RandomForest ---\")\n",
    "    print(f\"  Training:   X_train_rf: {X_train_rf.shape}, y_train_rf: {y_train_rf.shape}\")\n",
    "    print(f\"  Validation: X_val_rf: {X_val_rf.shape}, y_val_rf: {y_val_rf.shape}\")\n",
    "    print(f\"  Testing:    X_test_rf: {X_test_rf.shape}, y_test_rf: {y_test_rf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162fbb6-179a-483f-b663-69c1b64dc040",
   "metadata": {},
   "source": [
    "## **MACHINE LEARNING MODEL TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04aa916-ef83-4e8f-ae5d-80d0a49a2129",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL FUNCTIONS\n",
    "There are 3 models that will be trained:\n",
    "1. MLP Model (TA242501010)\n",
    "2. 1D-CNN\n",
    "3. RandomForest\n",
    "\n",
    "There is also an additional function in order to do automatic hyperparameter tuning, but the function is only made for the MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28664a07-7fce-4c07-b780-2ffac2105947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the MLP model for automatic hyperparameter tuning\n",
    "def build_model(hp):\n",
    "    \"\"\"Function that builds a Keras model and defines the hyperparameters to be tuned.\"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tune the number of units in the first hidden layer\n",
    "    hp_units_1 = hp.Int('units_1', min_value=32, max_value=256, step=32)\n",
    "    model.add(Dense(units=hp_units_1, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    # Tune the dropout rate\n",
    "    hp_dropout_1 = hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(rate=hp_dropout_1))\n",
    "\n",
    "    # Tune the number of units in the second hidden layer\n",
    "    hp_units_2 = hp.Int('units_2', min_value=32, max_value=256, step=32)\n",
    "    model.add(Dense(units=hp_units_2, activation='relu'))\n",
    "\n",
    "    # Tune the dropout rate\n",
    "    hp_dropout_2 = hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(hp_dropout_2))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "    # Tune the learning rate for the Adam optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc_roc'),\n",
    "            tf.keras.metrics.AUC(name='auc_pr', curve='PR'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Function to create the MLP model for cross-validation\n",
    "def create_mlp_model(input_dim, output_dim):\n",
    "    \"\"\"Creates and compiles a Keras MLP model.\"\"\"\n",
    "    model = Sequential([\n",
    "        # Hyperparameters tuning\n",
    "        Dense(1024, input_dim=input_dim, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(output_dim, activation='softmax') # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy', # Suitable for one-hot labels\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "# Function to create the 1D-CNN model\n",
    "def create_cnn_model(input_shape, output_dim):\n",
    "    \"\"\"Creates and compiles a Keras 1D-CNN model.\"\"\"\n",
    "    # Input shape for CNN must be 3D: (samples, steps, features)\n",
    "    # Example: (10000, 187, 1)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=448, kernel_size=6, activation='relu',\n",
    "               input_shape=input_shape),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        Conv1D(filters=448, kernel_size=3, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Flatten(), # Flatten the data to connect to the Dense layer\n",
    "\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Function to create the RandomForest model\n",
    "def create_rf_model():\n",
    "    \"\"\"Creates an instance of the RandomForestClassifier model.\"\"\"\n",
    "    # Hyperparameters can be adjusted here\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=300,  # Number of trees in the forest\n",
    "        random_state=42,\n",
    "        n_jobs=-1,         # Use all CPU cores\n",
    "        max_depth=50\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16e19d-a20a-4308-9848-37de070b26a3",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL TRAINING EXECUTION\n",
    "1. Targeted metrics: Precision, Recall, F1-Score, and Specificity\n",
    "2. There are 3 executions: multiple models training, MLP model specific training, and MLP model automatic hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2cd67-b661-4d6b-a3fc-d87dd454416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/OSOTNAS/Documents/Kean/Others/KP_Xirka-Darma-Persada/machine-learning/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1754468119.395005    5765 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3620 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Training Multiple Models\n",
    "input_shape_cnn = (X_train_cnn.shape[1], X_train_cnn.shape[2])\n",
    "input_dim = X_train_mlp.shape[1]\n",
    "# output_dim already defined from the DATA PREPARATION section\n",
    "\n",
    "models = {\n",
    "    \"1D-CNN\": create_cnn_model(input_shape_cnn, output_dim),\n",
    "    \"RandomForest\": create_rf_model(),\n",
    "    \"MLP\": create_mlp_model(input_dim, output_dim)\n",
    "}\n",
    "\n",
    "# Dictionary to store the final results\n",
    "results = {}\n",
    "\n",
    "# --- TRAINING AND EVALUATING EACH MODEL ---\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*20} TRAINING MODEL: {name} {'='*20}\")\n",
    "\n",
    "    # ðŸ§  Training\n",
    "    if name == \"1D-CNN\":\n",
    "        model.fit(\n",
    "            X_train_cnn, y_train_cnn,\n",
    "            epochs=20,\n",
    "            batch_size=16,\n",
    "            verbose=1,\n",
    "            validation_data=(X_val_cnn, y_val_cnn) # Using the existing validation set\n",
    "        )\n",
    "    elif name == \"MLP\":\n",
    "        model.fit(\n",
    "            X_train_mlp, y_train_mlp,\n",
    "            epochs=20,\n",
    "            batch_size=16,\n",
    "            verbose=1,\n",
    "            validation_data=(X_val_mlp, y_val_mlp) # Using the existing validation set\n",
    "        )\n",
    "    else: # ðŸ“Š RandomForest\n",
    "        model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "    # âš¡ Prediction on the Test Set\n",
    "    print(f\"Evaluating model {name}...\")\n",
    "    if name == \"MLP\":\n",
    "        y_pred_raw = model.predict(X_test_mlp)\n",
    "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "    elif name == \"1D-CNN\":\n",
    "        y_pred_raw = model.predict(X_test_cnn)\n",
    "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "    else: # RandomForest\n",
    "        y_pred = model.predict(X_test_rf)\n",
    "\n",
    "    # Save prediction results for final evaluation\n",
    "    results[name] = {'y_pred': y_pred}\n",
    "\n",
    "# --- PRINT ALL RESULTS SIMULTANEOUSLY ---\n",
    "\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)', 'Unknown (Q)']\n",
    "\n",
    "print(f\"\\n{'='*25} FINAL EVALUATION RESULTS {'='*25}\")\n",
    "\n",
    "for name, result_data in results.items():\n",
    "    y_pred_test = result_data['y_pred']\n",
    "\n",
    "    print(f\"\\n\\n--- REPORT FOR MODEL: {name} ---\")\n",
    "\n",
    "    # Standard Classification Report (using y_test_rf, which are the original integer labels)\n",
    "    print(\"\\nClassification Report on the Test Set:\")\n",
    "    print(classification_report(y_test_rf, y_pred_test, target_names=class_names))\n",
    "\n",
    "    # Additional Metrics Report\n",
    "    print(\"Additional Metrics Report:\")\n",
    "    cm = confusion_matrix(y_test_rf, y_pred_test)\n",
    "    for i in range(len(class_names)):\n",
    "        tn = cm.sum() - (cm[i,:].sum() + cm[:,i].sum() - cm[i,i])\n",
    "        tp = cm[i,i]\n",
    "        fp = cm[:,i].sum() - cm[i,i]\n",
    "        fn = cm[i,:].sum() - cm[i,i]\n",
    "\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "        print(f\"  Class: {class_names[i]}\")\n",
    "        print(f\"    - Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "        print(f\"    - Specificity         : {specificity:.4f}\")\n",
    "        print(f\"    - False Positive Rate : {fpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f6a7b-b10a-4e43-89be-2dcaa76b0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training MLP Model - TA242501010\n",
    "# Initialize AI model\n",
    "model = create_mlp_model(input_dim, output_dim)\n",
    "\n",
    "# Prepare EarlyStopping callback for F1 score validation\n",
    "early_stopping_1 = EarlyStopping(monitor='val_f1_score', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Prepare class weights\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Start AI model training\n",
    "history = model.fit(\n",
    "    X_train_resampled,\n",
    "    y_train_resampled_encoded,\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_fold, y_val_fold_encoded),\n",
    "    verbose=1,\n",
    "    # class_weight=class_weight_dict,\n",
    "    validation_split=0.2\n",
    "    # callbacks=[early_stopping_1]\n",
    ")\n",
    "\n",
    "# Evaluate on the untouched set\n",
    "print(\"Evaluating on the untouched test set...\")\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred_test_raw = model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions back to labels if they are one-hot encoded\n",
    "if hasattr(y_pred_test_raw, 'shape') and len(y_pred_test_raw.shape) > 1:\n",
    "      y_pred_test = np.argmax(y_pred_test_raw, axis=1)\n",
    "else:\n",
    "      y_pred_test = y_pred_test_raw\n",
    "\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)', 'Unknown (Q)']\n",
    "\n",
    "print(\"\\nClassification Report on the Test Set:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=class_names))\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate metrics for each class (one-vs-rest)\n",
    "print(\"\\nAdditional Metrics Report:\")\n",
    "print(\"=\"*55)\n",
    "for i in range(len(class_names)):\n",
    "    tn = cm.sum() - (cm[i,:].sum() + cm[:,i].sum() - cm[i,i])\n",
    "    tp = cm[i,i]\n",
    "    fp = cm[:,i].sum() - cm[i,i]\n",
    "    fn = cm[i,:].sum() - cm[i,i]\n",
    "\n",
    "    # Sensitivity (Recall)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    # Specificity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    # False Positive Rate\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "    print(f\"Class: {class_names[i]}\")\n",
    "    print(f\"  Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"  Specificity         : {specificity:.4f}\")\n",
    "    print(f\"  False Positive Rate : {fpr:.4f}\")\n",
    "    print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547cc6c-40fd-4467-9684-fde9c688514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Hyperparameter Tuning\n",
    "print(\"\\n--- Starting Automatic Hyperparameter Tuning with KerasTuner ---\")\n",
    "\n",
    "# Calculate class_weight only once\n",
    "# class_weights = compute_class_weight(\n",
    "#     'balanced',\n",
    "#     classes=np.unique(y_train),\n",
    "#     y=y_train\n",
    "# )\n",
    "# class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Defining tuner objectives\n",
    "multi_objectives = [\n",
    "    kt.Objective(\"val_f1_score\", direction=\"max\")\n",
    "    #kt.Objective(\"val_specificity\", direction=\"max\")\n",
    "    #kt.Objective(\"val_precision\", direction=\"max\")\n",
    "    #kt.Objective(\"val_recall\", direction=\"max\")\n",
    "    #kt.Objective(\"val_auc_roc\", direction=\"max\")\n",
    "    #kt.Objective(\"val_auc_pr\", direction=\"max\")\n",
    "]\n",
    "# Initialize Tuner with RandomSearch\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=multi_objectives, # Target: maximize validation F1 score\n",
    "    max_trials=20,              # Total number of hyperparameter combinations to be tried\n",
    "    executions_per_trial=1,     # Number of models trained per combination (for stability)\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='ecg_classification_0834' # Can change the name to find the latest parameters with the latest code\n",
    ")\n",
    "\n",
    "# Prepare EarlyStopping callback for F1 score validation\n",
    "early_stopping = EarlyStopping(monitor='val_f1_score', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Run the search\n",
    "print(\"\\nStarting the search for the best hyperparameters...\")\n",
    "tuner.search(\n",
    "    X_train_resampled,              # Training data that has been processed with SMOTE\n",
    "    y_train_resampled_encoded,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val_fold, y_val_fold_encoded), # 1. Use validation_data\n",
    "    # class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "--- Search Complete ---\n",
    "Best hyperparameters found:\n",
    "- Units 1: {best_hps.get('units_1')}\n",
    "- Dropout 1: {best_hps.get('dropout_1'):.2f}\n",
    "- Units 2: {best_hps.get('units_2')}\n",
    "- Dropout 2: {best_hps.get('dropout_2'):.2f}\n",
    "- Learning Rate: {best_hps.get('learning_rate')}\n",
    "\"\"\")\n",
    "\n",
    "# --- Final Evaluation on the Test Set ---\n",
    "print(\"\\n--- Evaluating the Best Model on the Test Set ---\")\n",
    "X_test_scaled = scaler.transform(X_test) # Use the same scaler from training\n",
    "y_pred_test_raw = best_model.predict(X_test_scaled)\n",
    "y_pred_test = np.argmax(y_pred_test_raw, axis=1)\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)', 'Unknown (Q)']\n",
    "\n",
    "# --- 1. Classification Report ----\n",
    "print(\"\\nClassification Report on the Test Set:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=class_names))\n",
    "\n",
    "# --- 2. Confusion Matrix ---\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Specific Metric Calculation per Class ---\n",
    "print(\"\\n--- Detailed Performance Metrics per Class ---\")\n",
    "metrics_data = []\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Specificity calculation\n",
    "    tn = cm.sum() - (cm[i,:].sum() + cm[:,i].sum() - cm[i,i])\n",
    "    fp = cm[:,i].sum() - cm[i,i]\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    metrics_data.append({\n",
    "        \"Class\": class_name,\n",
    "        \"Precision\": precision_score(y_test, y_pred_test, average=None)[i],\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, y_pred_test, average=None)[i],\n",
    "        \"F1-Score\": f1_score(y_test, y_pred_test, average=None)[i],\n",
    "        \"Specificity\": specificity\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(metrics_df.to_string())\n",
    "\n",
    "\n",
    "# --- 4. Calculation of AUC-ROC and AUC-PR (One-vs-Rest) ---\n",
    "y_test_encoded = to_categorical(y_test, num_classes=output_dim)\n",
    "\n",
    "# Add this line to define y_pred_proba\n",
    "y_pred_proba = best_model.predict(X_test_scaled)\n",
    "\n",
    "# AUC-ROC\n",
    "auc_roc_ovr = roc_auc_score(y_test_encoded, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "print(f\"\\nAUC-ROC (One-vs-Rest, Weighted): {auc_roc_ovr:.4f}\")\n",
    "\n",
    "# AUC-PR\n",
    "# Calculate for each class and average\n",
    "precision_curves = dict()\n",
    "recall_curves = dict()\n",
    "auc_pr_scores = []\n",
    "for i in range(output_dim):\n",
    "    precision_curves[i], recall_curves[i], _ = precision_recall_curve(y_test_encoded[:, i], y_pred_proba[:, i])\n",
    "    auc_pr_scores.append(auc(recall_curves[i], precision_curves[i]))\n",
    "\n",
    "# Weighted average for AUC-PR\n",
    "support = np.bincount(y_test)\n",
    "avg_auc_pr = np.average(auc_pr_scores, weights=support)\n",
    "print(f\"AUC-PR (One-vs-Rest, Weighted): {avg_auc_pr:.4f}\")\n",
    "\n",
    "# Plotting PR Curves for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.plot(recall_curves[i], precision_curves[i], lw=2, label=f'{class_name} (AUC-PR = {auc_pr_scores[i]:.2f})')\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve per Class\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
