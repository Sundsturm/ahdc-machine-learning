{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e32954",
   "metadata": {},
   "source": [
    "## **LIBRARY IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acf0b717-46b3-4967-962f-6d5f2eae8848",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "dtype: int64\n",
      "TensorFlow has detected 1 GPU(s):\n",
      "- PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import cudf\n",
    "import os\n",
    "import pywt\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb  # For reading MIT-BIH data\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from scipy.signal import find_peaks, resample\n",
    "\n",
    "# Scikit-learn and Imbalanced-learn imports\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Model imports\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Dropout, Add, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Additional setups\n",
    "# Checking cUML\n",
    "print(cudf.Series([1, 2, 3]))\n",
    "\n",
    "# Setting TensorFlow flags\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Checking GPU\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"TensorFlow has detected {len(gpu_devices)} GPU(s):\")\n",
    "    for device in gpu_devices:\n",
    "        print(f\"- {device}\")\n",
    "else:\n",
    "    print(\"TensorFlow did not detect any GPUs. Training will run on the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348ca32-352e-4791-a764-9a1dedd1af04",
   "metadata": {},
   "source": [
    "## **DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb05f79-645a-4ff3-b8f5-44e867b01a39",
   "metadata": {},
   "source": [
    "### DATA PREPARATION FUNCTIONS\n",
    "There are 3 types of functions:\n",
    "1. Labels and windowed features of the RR intervals\n",
    "2. Data preparation of MIT-BIH dataset as training and validation set\n",
    "3. Data preparation of additional ECG data with a format of .bin as the final testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8a11cb1-fc53-445e-933c-49f337cb0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LABELS & ADDITIONAL FUNCTIONS\n",
    "label_map = { 'N': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0,  # Normal Beats (N)\n",
    "              'V': 1, 'E': 1,                          # Ventricular Ectopic (VEB)\n",
    "              'S': 2, 'A': 2, 'a': 2, 'J': 2,          # Supraventricular Ectopic (SVEB)\n",
    "              'F': 3                                  # Fusion Beat (F)\n",
    "            }\n",
    "# Helper function for resampling signals to a new length\n",
    "def resample_data(X, original_len, new_len):\n",
    "    \"\"\"Resamples all windows in X from original_len to new_len.\"\"\"\n",
    "    # Ensure X is 3D (num_samples, time_steps, channels)\n",
    "    if X.ndim != 3:\n",
    "        raise ValueError(\"Input array X must be 3-dimensional.\")\n",
    "    \n",
    "    # Remove the channel dimension for resampling\n",
    "    X_reshaped = X.squeeze(axis=-1)\n",
    "    \n",
    "    # Resample each signal window\n",
    "    resampled_X = np.apply_along_axis(\n",
    "        lambda row: resample(row, new_len),\n",
    "        axis=1,\n",
    "        arr=X_reshaped\n",
    "    )\n",
    "    \n",
    "    # Add the channel dimension back\n",
    "    return np.expand_dims(resampled_X, axis=-1)\n",
    "\n",
    "# Extract features of wavelet for 1D-CNN\n",
    "def extract_wavelet_features(window, wavelet='db4', level=4):\n",
    "    \"\"\"\n",
    "    Extracts statistical and entropy-based features from the wavelet coefficients of an ECG window.\n",
    "    \n",
    "    Args:\n",
    "        window (np.ndarray): A 1D numpy array representing a single ECG beat window.\n",
    "        wavelet (str): The type of wavelet to use.\n",
    "        level (int): The level of wavelet decomposition.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A 1D numpy array containing the extracted features.\n",
    "    \"\"\"\n",
    "    # Decompose the signal\n",
    "    coeffs = pywt.wavedec(window, wavelet, level=level)\n",
    "    \n",
    "    features = []\n",
    "    for c in coeffs:\n",
    "        # Basic statistical features\n",
    "        features.append(np.mean(c))\n",
    "        features.append(np.std(c))\n",
    "        features.append(np.var(c))\n",
    "        \n",
    "        # Energy of the coefficients\n",
    "        features.append(np.sum(np.square(c)))\n",
    "        \n",
    "        # Shannon Entropy of the coefficients\n",
    "        # We use the squared coeffs to represent energy distribution for entropy calculation\n",
    "        # Adding a small epsilon to avoid log(0)\n",
    "        features.append(entropy(np.square(c) + 1e-9))\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "# Wavelet data from MIT-BIH\n",
    "def prepare_wavelet_data(signals, annotations, window_size, fs=360, wavelet='db4', level=4):\n",
    "    \"\"\"\n",
    "    Prepares data by extracting wavelet-based features from ECG signal windows\n",
    "    centered around each annotated R-peak.\n",
    "\n",
    "    Args:\n",
    "        signals (list): List of raw ECG signal arrays.\n",
    "        annotations (list): List of wfdb Annotation objects.\n",
    "        window_size (int): The total number of samples in each window.\n",
    "        fs (int): The sampling frequency of the signals.\n",
    "        wavelet (str): The type of wavelet to use for feature extraction.\n",
    "        level (int): The level of wavelet decomposition.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (X, y) where:\n",
    "        - X is a 2D numpy array of wavelet features (num_beats, num_features).\n",
    "        - y is a 1D numpy array of corresponding integer labels.\n",
    "    \"\"\"\n",
    "    samples_before = window_size // 3\n",
    "    samples_after = window_size - samples_before\n",
    "\n",
    "    all_feature_vectors = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i, signal in enumerate(signals):\n",
    "        ann = annotations[i]\n",
    "        r_peaks = ann.sample\n",
    "        symbols = ann.symbol\n",
    "\n",
    "        if ann.fs!= fs:\n",
    "            print(f\"Warning: Record {ann.record_name} has fs={ann.fs}, but expected fs={fs}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for j, r_peak_loc in enumerate(r_peaks):\n",
    "            symbol = symbols[j]\n",
    "            if symbol in label_map:\n",
    "                start = r_peak_loc - samples_before\n",
    "                end = r_peak_loc + samples_after\n",
    "\n",
    "                if start >= 0 and end < len(signal):\n",
    "                    window = signal[start:end]\n",
    "                    label = label_map[symbol]\n",
    "                    \n",
    "                    # Extract features from the window\n",
    "                    features = extract_wavelet_features(window, wavelet=wavelet, level=level)\n",
    "                    \n",
    "                    all_feature_vectors.append(features)\n",
    "                    all_labels.append(label)\n",
    "\n",
    "    X = np.array(all_feature_vectors)\n",
    "    y = np.array(all_labels)\n",
    "\n",
    "    print(f\"Successfully created feature matrix with shape: {X.shape}\")\n",
    "    return X, y\n",
    "\n",
    "# Wavelet data from raw ECG data with format of .bin\n",
    "def prepare_wavelet_data_from_bin(signal, r_peaks, window_size, record_label, wavelet='db4', level=4):\n",
    "    \"\"\"\n",
    "    Prepares data by extracting wavelet features from windows around R-peaks from a.bin file.\n",
    "\n",
    "    Args:\n",
    "        signal (numpy.ndarray): The raw ECG signal array.\n",
    "        r_peaks (numpy.ndarray): The indices of the detected R-peaks.\n",
    "        window_size (int): The total number of samples in each window.\n",
    "        record_label (int): The integer label to apply to all windows.\n",
    "        wavelet (str): The type of wavelet to use for feature extraction.\n",
    "        level (int): The level of wavelet decomposition.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (X, y) where:\n",
    "        - X is a 2D numpy array of wavelet features.\n",
    "        - y is a 1D numpy array of corresponding integer labels.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Step 2: Extracting wavelet features from signal windows ---\")\n",
    "    samples_before = window_size // 3\n",
    "    samples_after = window_size - samples_before\n",
    "\n",
    "    all_feature_vectors = []\n",
    "    all_labels = []\n",
    "\n",
    "    for r_peak_loc in r_peaks:\n",
    "        start = r_peak_loc - samples_before\n",
    "        end = r_peak_loc + samples_after\n",
    "\n",
    "        if start >= 0 and end < len(signal):\n",
    "            window = signal[start:end]\n",
    "            \n",
    "            # Extract features from the window\n",
    "            features = extract_wavelet_features(window, wavelet=wavelet, level=level)\n",
    "            \n",
    "            all_feature_vectors.append(features)\n",
    "            all_labels.append(record_label)\n",
    "\n",
    "    X = np.array(all_feature_vectors)\n",
    "    y = np.array(all_labels)\n",
    "\n",
    "    print(f\"Successfully created {len(X)} feature vectors.\")\n",
    "    print(f\"Final feature matrix shape (X): {X.shape}\")\n",
    "    print(f\"Final labels shape (y): {y.shape}\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03deef09-ac69-442e-96e2-a2d354872eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATA LOADING UTILITY OF MIT-BIH DATA\n",
    "# This function efficiently loads the specified records and their annotations.\n",
    "def load_mitbih_records(db_path, record_names):\n",
    "    \"\"\"\n",
    "    Loads raw ECG signals and annotations for specified records.\n",
    "\n",
    "    Args:\n",
    "        db_path (str): The path to the database directory (e.g., 'mit-bih-arrhythmia-database-1.0.0').\n",
    "        record_names (list): A list of record names as strings (e.g., ['100', '101']).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two lists: (signals, annotations).\n",
    "        - signals: A list of raw ECG signal arrays for channel 0.\n",
    "        - annotations: A list of wfdb Annotation objects.\n",
    "    \"\"\"\n",
    "    all_signals = []\n",
    "    all_annotations = []\n",
    "    print(f\"Loading records: {', '.join(record_names)}...\")\n",
    "    for rec_name in record_names:\n",
    "        record_path = f'{db_path}/{rec_name}'\n",
    "        try:\n",
    "            # Read the signal from the first channel (.dat file)\n",
    "            signal = wfdb.rdrecord(record_path, channels=[0]).p_signal.flatten()\n",
    "            # Read the corresponding annotations (.atr file)\n",
    "            annotation = wfdb.rdann(record_path, 'atr')\n",
    "\n",
    "            all_signals.append(signal)\n",
    "            all_annotations.append(annotation)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing record {rec_name}: {e}\")\n",
    "    print(\"Loading complete.\")\n",
    "    return all_signals, all_annotations\n",
    "\n",
    "# A function to extract raw signal windows (beats) for model training.\n",
    "def prepare_raw_data(signals, annotations, window_size, fs=360):\n",
    "    \"\"\"\n",
    "    Prepares data by extracting fixed-size raw ECG signal windows\n",
    "    centered around each annotated R-peak.\n",
    "\n",
    "    Args:\n",
    "        signals (list): List of raw ECG signal arrays.\n",
    "        annotations (list): List of wfdb Annotation objects.\n",
    "        window_size (int): The total number of samples in each window (e.g., 288).\n",
    "        fs (int): The sampling frequency of the signals (default is 360 for MIT-BIH).\n",
    "\n",
    "    Returns:\n",
    "        A tuple (X, y) where:\n",
    "        - X is a numpy array of ECG signal windows, ready for a CNN.\n",
    "        - y is a numpy array of corresponding integer labels.\n",
    "    \"\"\"\n",
    "    # A common practice is to place the R-peak off-center (e.g., at 1/3)\n",
    "    # for better feature extraction by the model.\n",
    "    samples_before = window_size // 3\n",
    "    samples_after = window_size - samples_before\n",
    "\n",
    "    all_windows = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Process each signal and its corresponding annotations\n",
    "    for i, signal in enumerate(signals):\n",
    "        ann = annotations[i]\n",
    "        r_peaks = ann.sample\n",
    "        symbols = ann.symbol\n",
    "\n",
    "        # Verify the record's sampling frequency\n",
    "        if ann.fs != fs:\n",
    "            print(f\"Warning: Record {ann.record_name} has fs={ann.fs}, but expected fs={fs}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Extract a window for each valid R-peak annotation\n",
    "        for j, r_peak_loc in enumerate(r_peaks):\n",
    "            symbol = symbols[j]\n",
    "            # Classify only the beats defined in our label_map\n",
    "            if symbol in label_map:\n",
    "                start = r_peak_loc - samples_before\n",
    "                end = r_peak_loc + samples_after\n",
    "\n",
    "                # Ensure the window is fully within the signal's bounds\n",
    "                if start >= 0 and end < len(signal):\n",
    "                    window = signal[start:end]\n",
    "                    label = label_map[symbol]\n",
    "                    all_windows.append(window)\n",
    "                    all_labels.append(label)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(all_windows)\n",
    "    y = np.array(all_labels)\n",
    "\n",
    "    # Add a \"channel\" dimension for compatibility with deep learning models (e.g., CNNs)\n",
    "    # The shape becomes (number_of_beats, window_size, 1)\n",
    "    X = np.expand_dims(X, axis=-1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f269e46-0c80-4702-b7e2-bfd77c68fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DATA LOADING FOR .bin FILES\n",
    "# A function to load ECG data\n",
    "def load_ecg_from_bin(file_path, dtype=np.int16):\n",
    "    \"\"\"\n",
    "    Loading raw ECG signals from binary files.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .bin file.\n",
    "        dtype (numpy.dtype): Data type of the signal in the .bin file.\n",
    "\n",
    "    Return:\n",
    "        numpy.ndarray: ECG signals as a numpy array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        signal = np.fromfile(file_path, dtype=dtype)\n",
    "        print(f\"Completed reading {len(signal)} samples from {file_path}\")\n",
    "        return signal\n",
    "    except IOError as e:\n",
    "        print(f\"An error has occurred while reading: {e}\")\n",
    "        return None\n",
    "\n",
    "# A function to detect R-peaks for labelling\n",
    "def detect_r_peaks(signal, fs):\n",
    "    \"\"\"\n",
    "    Detecting R-peaks from the ECG signal. This serves as our substitute\n",
    "    for reading an annotation file.\n",
    "\n",
    "    Args:\n",
    "        signal (numpy.ndarray): Raw ECG signal in a numpy array.\n",
    "        fs (int): Sampling frequency of the ECG signal.\n",
    "\n",
    "    Return:\n",
    "        numpy.ndarray: An array of indices of the detected R-peaks.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 1: Detecting R-Peaks to locate heartbeats ---\")\n",
    "    height_threshold = np.max(signal) * 0.6\n",
    "    distance_threshold = fs * 0.4\n",
    "    r_peaks, _ = find_peaks(signal, height=height_threshold, distance=distance_threshold)\n",
    "    print(f\"Detected {len(r_peaks)} R-peaks.\")\n",
    "    return r_peaks\n",
    "\n",
    "# A function to prepare raw ECG data with its R-peaks\n",
    "def prepare_raw_data_from_bin(signal, r_peaks, window_size, record_label):\n",
    "    \"\"\"\n",
    "    Prepares data by extracting fixed-size raw ECG signal windows\n",
    "    centered around each detected R-peak from a .bin file.\n",
    "\n",
    "    Args:\n",
    "        signal (numpy.ndarray): The raw ECG signal array.\n",
    "        r_peaks (numpy.ndarray): The indices of the detected R-peaks.\n",
    "        window_size (int): The total number of samples in each window.\n",
    "        record_label (int): The integer label to apply to all windows from this record.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (X, y) where:\n",
    "        - X is a numpy array of ECG signal windows, ready for a CNN.\n",
    "        - y is a numpy array of corresponding integer labels.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Step 2: Extracting raw signal windows ---\")\n",
    "    samples_before = window_size // 3\n",
    "    samples_after = window_size - samples_before\n",
    "\n",
    "    all_windows = []\n",
    "    all_labels = []\n",
    "\n",
    "    for r_peak_loc in r_peaks:\n",
    "        start = r_peak_loc - samples_before\n",
    "        end = r_peak_loc + samples_after\n",
    "\n",
    "        # Ensure the window is fully within the signal's bounds\n",
    "        if start >= 0 and end < len(signal):\n",
    "            window = signal[start:end]\n",
    "            all_windows.append(window)\n",
    "            all_labels.append(record_label)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(all_windows)\n",
    "    y = np.array(all_labels)\n",
    "\n",
    "    # Add a \"channel\" dimension for compatibility with deep learning models (e.g., CNNs)\n",
    "    # The shape becomes (number_of_beats, window_size, 1)\n",
    "    X = np.expand_dims(X, axis=-1)\n",
    "\n",
    "    print(f\"Successfully created {len(X)} windows of size {window_size}.\")\n",
    "    print(f\"Final data shape (X): {X.shape}\")\n",
    "    print(f\"Final labels shape (y): {y.shape}\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# A function to visualize the data\n",
    "def plot_beat_windows(X, y, n=5):\n",
    "    \"\"\"\n",
    "    Plots a few sample beat windows.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): The data windows.\n",
    "        y (numpy.ndarray): The labels.\n",
    "        n (int): Number of samples to plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(n, 1, i + 1)\n",
    "        plt.plot(X[i].flatten()) # flatten to remove the channel dimension for plotting\n",
    "        plt.title(f\"Sample Beat Window {i+1} - Label: {y[i]}\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "    plt.xlabel(\"Sample Number within Window\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c0215-637d-49cd-8049-379035e45e3f",
   "metadata": {},
   "source": [
    "### DATA PREPARATION EXECUTION\n",
    "1. Reading all ECG datasets\n",
    "2. Divide all datasets into training dataset and testing dataset\n",
    "3. Standard scaling and combining datasets\n",
    "4. Splitting training dataset into training split and validation split then applying SMOTE algorithm into the training split\n",
    "5. Preparing all the datasets for each machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea82e85c-06ca-4dd4-9d1b-cb0b177484bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Step 1/4] Extracting Wavelet Features from All Datasets ---\n",
      "\n",
      "Processing MIT-BIH Training set (DS1)...\n",
      "Loading records: 101, 106, 108, 109, 112, 114, 115, 116, 118, 119, 122, 124, 201, 203, 205, 207, 208, 209, 215, 220, 223, 230...\n",
      "Loading complete.\n",
      "Successfully created feature matrix with shape: (50993, 25)\n",
      "\n",
      "Processing MIT-BIH Testing set (DS2)...\n",
      "Loading records: 100, 103, 105, 111, 113, 117, 121, 123, 200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233, 234...\n",
      "Loading complete.\n",
      "Successfully created feature matrix with shape: (49683, 25)\n",
      "\n",
      "Processing custom .bin files for the TESTING set...\n",
      "Completed reading 2380000 samples from ../data/raw/Arrhythmia/ECG_WAVE.bin\n",
      "\n",
      "--- Step 1: Detecting R-Peaks to locate heartbeats ---\n",
      "Detected 137 R-peaks.\n",
      "\n",
      "--- Step 2: Extracting wavelet features from signal windows ---\n",
      "Successfully created 136 feature vectors.\n",
      "Final feature matrix shape (X): (136, 25)\n",
      "Final labels shape (y): (136,)\n",
      "Completed reading 2135000 samples from ../data/raw/Normal/ecg_normal.bin\n",
      "\n",
      "--- Step 1: Detecting R-Peaks to locate heartbeats ---\n",
      "Detected 123 R-peaks.\n",
      "\n",
      "--- Step 2: Extracting wavelet features from signal windows ---\n",
      "Successfully created 122 feature vectors.\n",
      "Final feature matrix shape (X): (122, 25)\n",
      "Final labels shape (y): (122,)\n",
      "\n",
      "--- [Step 2/4] Scaling and Finalizing Data ---\n",
      "Scaler trained on training data.\n",
      "Scaler applied to all testing data.\n",
      "Final training data created: X_train_scaled=(50993, 25), y_train=(50993,)\n",
      "Final testing data combined: X_test_final=(49941, 25), y_test_final=(49941,)\n",
      "\n",
      "--- [Step 3/4] Finalizing Training Data (Split & Hybrid Sample) ---\n",
      "Creating validation set from training data (80/20)...\n",
      "Applying HYBRID SAMPLING (SMOTEENN) only to the training fold...\n",
      "Training class distribution before sampling: Counter({np.int64(0): 36677, np.int64(1): 3031, np.int64(2): 755, np.int64(3): 331})\n",
      "Training class distribution after SMOTEENN sampling: Counter({np.int64(3): 36675, np.int64(2): 36659, np.int64(1): 36656, np.int64(0): 33054})\n",
      "\n",
      "--- [Step 4/4] Preparing Final Datasets for Models ---\n",
      "\n",
      "============================================================\n",
      "✅ WAVELET DATA PREPARATION COMPLETE ✅\n",
      "The following variables are ready for training and evaluation:\n",
      "============================================================\n",
      "\n",
      "--- For MLP ---\n",
      "  Training:   X_train_mlp: (143044, 25), y_train_mlp: (143044, 4)\n",
      "  Validation: X_val_mlp:   (10199, 25), y_val_mlp:   (10199, 4)\n",
      "  Testing:    X_test_mlp:  (49941, 25), y_test_mlp:  (49941, 4)\n",
      "\n",
      "--- For 1D-CNN ---\n",
      "  Training:   X_train_cnn: (143044, 25, 1), y_train_cnn: (143044, 4)\n",
      "  Validation: X_val_cnn:   (10199, 25, 1), y_val_cnn:   (10199, 4)\n",
      "  Testing:    X_test_cnn:  (49941, 25, 1), y_test_cnn:  (49941, 4)\n",
      "\n",
      "--- For RandomForest ---\n",
      "  Training:   X_train_rf: (143044, 25), y_train_rf: (143044,)\n",
      "  Validation: X_val_rf:   (10199, 25), y_val_rf:   (10199,)\n",
      "  Testing:    X_test_rf:  (49941, 25), y_test_rf:  (49941,)\n"
     ]
    }
   ],
   "source": [
    "    # --- Configuration Section ---\n",
    "    DB_PATH_MIT = '../data/raw/MIT-BIH/mit-bih-arrhythmia-database-1.0.0/mit-bih-arrhythmia-database-1.0.0/'\n",
    "    FS_MIT = 360\n",
    "    WINDOW_SIZE_MIT = 288  # 800ms window -> 0.8s * 360Hz\n",
    "\n",
    "    # Wavelet Feature Configuration\n",
    "    WAVELET_TYPE = 'db4'\n",
    "    WAVELET_LEVEL = 4\n",
    "\n",
    "    # Split MIT-BIH records into training and testing sets to prevent patient data leakage\n",
    "    RECORDS_TRAIN = ['101', '106', '108', '109', '112', '114', '115', '116', '118', '119',\n",
    "                     '122', '124', '201', '203', '205', '207', '208', '209', '215', '220',\n",
    "                     '223', '230'] # DS1\n",
    "    RECORDS_TEST = ['100', '103', '105', '111', '113', '117', '121', '123', '200', '202',\n",
    "                    '210', '212', '213', '214', '219', '221', '222', '228', '231', '232',\n",
    "                    '233', '234'] # DS2\n",
    "\n",
    "    FS_CUSTOM = 500\n",
    "    WINDOW_SIZE_CUSTOM = 400  # 800ms window -> 0.8s * 500Hz\n",
    "    custom_file_paths = {\n",
    "        'Arrhythmia': '../data/raw/Arrhythmia/ECG_WAVE.bin',\n",
    "        'Normal': '../data/raw/Normal/ecg_normal.bin' \n",
    "    }\n",
    "    custom_file_labels = {'Arrhythmia': 2, 'Normal': 0} # SVEB and Normal\n",
    "\n",
    "    # ===================================================================\n",
    "    # Data Preparation Pipeline (Using Wavelet Features)\n",
    "    # ===================================================================\n",
    "\n",
    "    # --- [Step 1/4] Extract Wavelet Features ---\n",
    "    print(\"--- [Step 1/4] Extracting Wavelet Features from All Datasets ---\")\n",
    "    \n",
    "    # Load MIT-BIH training set (DS1) and extract features\n",
    "    print(\"\\nProcessing MIT-BIH Training set (DS1)...\")\n",
    "    train_signals_mit, train_anns_mit = load_mitbih_records(DB_PATH_MIT, RECORDS_TRAIN)\n",
    "    X_train_mit, y_train_mit = prepare_wavelet_data(\n",
    "        train_signals_mit, train_anns_mit, WINDOW_SIZE_MIT, fs=FS_MIT, wavelet=WAVELET_TYPE, level=WAVELET_LEVEL\n",
    "    )\n",
    "\n",
    "    # Load MIT-BIH testing set (DS2) and extract features\n",
    "    print(\"\\nProcessing MIT-BIH Testing set (DS2)...\")\n",
    "    test_signals_mit, test_anns_mit = load_mitbih_records(DB_PATH_MIT, RECORDS_TEST)\n",
    "    X_test_mitbih, y_test_mitbih = prepare_wavelet_data(\n",
    "        test_signals_mit, test_anns_mit, WINDOW_SIZE_MIT, fs=FS_MIT, wavelet=WAVELET_TYPE, level=WAVELET_LEVEL\n",
    "    )\n",
    "    \n",
    "    # Load custom data, extract features, and designate it for testing\n",
    "    print(\"\\nProcessing custom .bin files for the TESTING set...\")\n",
    "    X_test_custom_list, y_test_custom_list = [], []\n",
    "\n",
    "    for name, path in custom_file_paths.items():\n",
    "        label = custom_file_labels[name]\n",
    "        signal = load_ecg_from_bin(path)\n",
    "        if signal is not None:\n",
    "            r_peaks = detect_r_peaks(signal, fs=FS_CUSTOM)\n",
    "            X_single, y_single = prepare_wavelet_data_from_bin(\n",
    "                signal, r_peaks, WINDOW_SIZE_CUSTOM, label, wavelet=WAVELET_TYPE, level=WAVELET_LEVEL\n",
    "            )\n",
    "            X_test_custom_list.append(X_single)\n",
    "            y_test_custom_list.append(y_single)\n",
    "            \n",
    "    # Combine all custom data into a single feature matrix\n",
    "    X_test_custom = np.vstack(X_test_custom_list)\n",
    "    y_test_custom = np.concatenate(y_test_custom_list)\n",
    "\n",
    "    # --- [Step 2/4] Scaling & Combining Testing Data ---\n",
    "    print(\"\\n--- [Step 2/4] Scaling and Finalizing Data ---\")\n",
    "    \n",
    "    # The training data is ONLY from MIT-BIH DS1.\n",
    "    X_train = X_train_mit\n",
    "    y_train = y_train_mit\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    print(\"Scaler trained on training data.\")\n",
    "    \n",
    "    X_test_mitbih_scaled = scaler.transform(X_test_mitbih)\n",
    "    X_test_custom_scaled = scaler.transform(X_test_custom)\n",
    "    print(\"Scaler applied to all testing data.\")\n",
    "    \n",
    "    X_test_final = np.concatenate((X_test_mitbih_scaled, X_test_custom_scaled), axis=0)\n",
    "    y_test_final = np.concatenate((y_test_mitbih, y_test_custom), axis=0)\n",
    "    print(f\"Final training data created: X_train_scaled={X_train_scaled.shape}, y_train={y_train.shape}\")\n",
    "    print(f\"Final testing data combined: X_test_final={X_test_final.shape}, y_test_final={y_test_final.shape}\")\n",
    "\n",
    "    # --- [Step 3/4] Training Set Splitting & Hybrid Sampling (SMOTEENN) ---\n",
    "    print(\"\\n--- [Step 3/4] Finalizing Training Data (Split & Hybrid Sample) ---\")\n",
    "    # Determine number of classes from all available labels\n",
    "    output_dim = len(np.unique(np.concatenate((y_train, y_test_final))))\n",
    "    \n",
    "    print(\"Creating validation set from training data (80/20)...\")\n",
    "    X_train_fold, X_val, y_train_fold, y_val = train_test_split(\n",
    "        X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    print(\"Applying HYBRID SAMPLING (SMOTEENN) only to the training fold...\")\n",
    "    print(\"Training class distribution before sampling:\", Counter(y_train_fold))\n",
    "    \n",
    "    sampler = SMOTEENN(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = sampler.fit_resample(X_train_fold, y_train_fold)\n",
    "    \n",
    "    print(\"Training class distribution after SMOTEENN sampling:\", Counter(y_train_resampled))\n",
    "\n",
    "    # --- [Step 4/4] Final Data Preparation for Models ---\n",
    "    print(\"\\n--- [Step 4/4] Preparing Final Datasets for Models ---\")\n",
    "\n",
    "    # One-hot encoding labels for Keras/TensorFlow models\n",
    "    y_train_encoded = to_categorical(y_train_resampled, num_classes=output_dim)\n",
    "    y_val_encoded = to_categorical(y_val, num_classes=output_dim)\n",
    "    y_test_final_encoded = to_categorical(y_test_final, num_classes=output_dim)\n",
    "\n",
    "    # 🧠 Data for MLP (requires 2D input)\n",
    "    X_train_mlp, y_train_mlp = X_train_resampled, y_train_encoded\n",
    "    X_val_mlp, y_val_mlp = X_val, y_val_encoded\n",
    "    X_test_mlp, y_test_mlp = X_test_final, y_test_final_encoded\n",
    "\n",
    "    # ⚡ Data for 1D-CNN (requires 3D input: samples, feature_vector_length, channels)\n",
    "    X_train_cnn = X_train_mlp.reshape((X_train_mlp.shape[0], X_train_mlp.shape[1], 1))\n",
    "    X_val_cnn = X_val_mlp.reshape((X_val_mlp.shape[0], X_val_mlp.shape[1], 1))\n",
    "    X_test_cnn = X_test_mlp.reshape((X_test_mlp.shape[0], X_test_mlp.shape[1], 1))\n",
    "    y_train_cnn, y_val_cnn, y_test_cnn = y_train_mlp, y_val_mlp, y_test_mlp\n",
    "\n",
    "    # 🌲 Data for RandomForest (requires 2D input and integer labels)\n",
    "    X_train_rf, y_train_rf = X_train_resampled, y_train_resampled\n",
    "    X_val_rf, y_val_rf = X_val, y_val\n",
    "    X_test_rf, y_test_rf = X_test_final, y_test_final\n",
    "\n",
    "    # --- FINAL RESULTS ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ WAVELET DATA PREPARATION COMPLETE ✅\")\n",
    "    print(\"The following variables are ready for training and evaluation:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n--- For MLP ---\")\n",
    "    print(f\"  Training:   X_train_mlp: {X_train_mlp.shape}, y_train_mlp: {y_train_mlp.shape}\")\n",
    "    print(f\"  Validation: X_val_mlp:   {X_val_mlp.shape}, y_val_mlp:   {y_val_mlp.shape}\")\n",
    "    print(f\"  Testing:    X_test_mlp:  {X_test_mlp.shape}, y_test_mlp:  {y_test_mlp.shape}\")\n",
    "\n",
    "    print(\"\\n--- For 1D-CNN ---\")\n",
    "    print(f\"  Training:   X_train_cnn: {X_train_cnn.shape}, y_train_cnn: {y_train_cnn.shape}\")\n",
    "    print(f\"  Validation: X_val_cnn:   {X_val_cnn.shape}, y_val_cnn:   {y_val_cnn.shape}\")\n",
    "    print(f\"  Testing:    X_test_cnn:  {X_test_cnn.shape}, y_test_cnn:  {y_test_cnn.shape}\")\n",
    "\n",
    "    print(\"\\n--- For RandomForest ---\")\n",
    "    print(f\"  Training:   X_train_rf: {X_train_rf.shape}, y_train_rf: {y_train_rf.shape}\")\n",
    "    print(f\"  Validation: X_val_rf:   {X_val_rf.shape}, y_val_rf:   {y_val_rf.shape}\")\n",
    "    print(f\"  Testing:    X_test_rf:  {X_test_rf.shape}, y_test_rf:  {y_test_rf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162fbb6-179a-483f-b663-69c1b64dc040",
   "metadata": {},
   "source": [
    "## **MACHINE LEARNING MODEL TRAINING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04aa916-ef83-4e8f-ae5d-80d0a49a2129",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL FUNCTIONS\n",
    "There are 3 models that will be trained:\n",
    "1. MLP Model (TA242501010)\n",
    "2. 1D-CNN\n",
    "3. RandomForest\n",
    "\n",
    "There is also an additional function in order to do automatic hyperparameter tuning, but the function is only made for the MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28664a07-7fce-4c07-b780-2ffac2105947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the MLP model for automatic hyperparameter tuning\n",
    "def build_model(hp):\n",
    "    \"\"\"Function that builds a Keras model and defines the hyperparameters to be tuned.\"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tune the number of units in the first hidden layer\n",
    "    hp_units_1 = hp.Int('units_1', min_value=32, max_value=256, step=32)\n",
    "    model.add(Dense(units=hp_units_1, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "    # Tune the dropout rate\n",
    "    hp_dropout_1 = hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(rate=hp_dropout_1))\n",
    "\n",
    "    # Tune the number of units in the second hidden layer\n",
    "    hp_units_2 = hp.Int('units_2', min_value=32, max_value=256, step=32)\n",
    "    model.add(Dense(units=hp_units_2, activation='relu'))\n",
    "\n",
    "    # Tune the dropout rate\n",
    "    hp_dropout_2 = hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(hp_dropout_2))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "    # Tune the learning rate for the Adam optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc_roc'),\n",
    "            tf.keras.metrics.AUC(name='auc_pr', curve='PR'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Function to create the MLP model for cross-validation\n",
    "def create_mlp_model(input_dim, output_dim):\n",
    "    \"\"\"Creates and compiles a Keras MLP model.\"\"\"\n",
    "    model = Sequential([\n",
    "        # Hyperparameters tuning\n",
    "        Dense(512, input_dim=input_dim, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(output_dim, activation='softmax') # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy', # Suitable for one-hot labels\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "# Function to create the 1D-CNN model\n",
    "def create_cnn_model(input_shape, output_dim):\n",
    "    \"\"\"Creates and compiles a Keras 1D-CNN model.\"\"\"\n",
    "    # Input shape for CNN must be 3D: (samples, steps, features)\n",
    "    # Example: (10000, 187, 1)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=256, kernel_size=6, activation='relu', # Reduced filters\n",
    "               input_shape=input_shape),\n",
    "        Dropout(0.1),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        Conv1D(filters=256, kernel_size=3, activation='relu'), # Reduced filters\n",
    "        Dropout(0.2),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        Flatten(), # Now flattens a much smaller tensor\n",
    "\n",
    "        Dense(256, activation='relu'), # Reduced dense units\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_cnn_model_optimized(input_shape, output_dim):\n",
    "    \"\"\"\n",
    "    Creates and compiles an optimized 1D-CNN model using modern architectural patterns\n",
    "    like Batch Normalization, Global Pooling, and Residual Connections.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): The shape of the input data (e.g., (num_features, 1)).\n",
    "        output_dim (int): The number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        A compiled Keras Model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Define a reusable Residual Block ---\n",
    "    def residual_block(x, filters, kernel_size):\n",
    "        \"\"\"A residual block with two convolutional layers.\"\"\"\n",
    "        # Main path\n",
    "        y = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Activation('relu')(y)\n",
    "        \n",
    "        y = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        \n",
    "        # Shortcut connection: Add the input 'x' to the output of the block\n",
    "        # The input and output must have the same dimensions for addition.\n",
    "        # If not, a 1x1 convolution is used to match them.\n",
    "        if x.shape[-1] != filters:\n",
    "            shortcut = Conv1D(filters=filters, kernel_size=1, padding='same')(x)\n",
    "        else:\n",
    "            shortcut = x\n",
    "            \n",
    "        res_output = Add()([shortcut, y])\n",
    "        res_output = Activation('relu')(res_output)\n",
    "        return res_output\n",
    "\n",
    "    # --- Build the Model ---\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial Convolutional Layer\n",
    "    # This layer processes the raw input features\n",
    "    x = Conv1D(filters=128, kernel_size=7, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # --- Stack of Residual Blocks ---\n",
    "    # Each block learns progressively more complex features\n",
    "    x = residual_block(x, filters=128, kernel_size=5)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x) # Dropout between blocks for regularization\n",
    "\n",
    "    x = residual_block(x, filters=256, kernel_size=3)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # --- Transition to Classifier Head ---\n",
    "    # Recommendation 2: Use Global Pooling instead of Flatten\n",
    "    # This drastically reduces parameters and helps prevent overfitting.\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # --- Dense Classifier Head ---\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # --- Output Layer ---\n",
    "    outputs = Dense(output_dim, activation='softmax')(x)\n",
    "\n",
    "    # Create and compile the final model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            # Note: F1Score might require a different setup in some TF versions.\n",
    "            # If it causes issues, consider a custom callback to calculate it.\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Function to create the RandomForest model\n",
    "def create_rf_model():\n",
    "    \"\"\"Creates an instance of the GPU-accelerated RandomForestClassifier model using cuML.\"\"\"\n",
    "    # Hyperparameters are similar to scikit-learn's\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=180,  # Number of trees in the forest\n",
    "        random_state=42,\n",
    "        max_depth=30       # Note: The 'n_jobs' parameter is not needed as it runs on the GPU\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16e19d-a20a-4308-9848-37de070b26a3",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL TRAINING EXECUTION\n",
    "1. Targeted metrics: Precision, Recall, F1-Score, and Specificity\n",
    "2. There are 3 executions: multiple models training, MLP model specific training, and MLP model automatic hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2cd67-b661-4d6b-a3fc-d87dd454416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TRAINING MODEL: 1D-CNN ====================\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/OSOTNAS/Documents/Kean/Others/KP_Xirka-Darma-Persada/machine-learning/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1755135277.454056    1902 service.cc:152] XLA service 0x72a3e00021f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1755135277.454134    1902 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 6GB Laptop GPU, Compute Capability 8.6\n",
      "2025-08-14 08:34:37.589767: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1755135278.143348    1902 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-08-14 08:34:39.577212: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3182', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-08-14 08:34:39.624841: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2385', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-08-14 08:34:39.754972: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3180', 12 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-08-14 08:34:39.769786: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3182', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2025-08-14 08:34:40.328790: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3182', 36 bytes spill stores, 44 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  22/4471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 5ms/step - accuracy: 0.2758 - f1_score: 0.2492 - loss: 2.1856 - precision: 0.2998 - recall: 0.2125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755135284.770392    1902 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7691 - f1_score: 0.7686 - loss: 0.6047 - precision: 0.8097 - recall: 0.7195"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 08:35:15.752031: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_298', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 8ms/step - accuracy: 0.8618 - f1_score: 0.8616 - loss: 0.3681 - precision: 0.8852 - recall: 0.8388 - val_accuracy: 0.8529 - val_f1_score: 0.8935 - val_loss: 0.3860 - val_precision: 0.8592 - val_recall: 0.8456\n",
      "Epoch 2/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.9412 - f1_score: 0.9412 - loss: 0.1611 - precision: 0.9448 - recall: 0.9376 - val_accuracy: 0.8242 - val_f1_score: 0.8711 - val_loss: 0.4849 - val_precision: 0.8271 - val_recall: 0.8194\n",
      "Epoch 3/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.9579 - f1_score: 0.9579 - loss: 0.1168 - precision: 0.9599 - recall: 0.9561 - val_accuracy: 0.8818 - val_f1_score: 0.9060 - val_loss: 0.3530 - val_precision: 0.8843 - val_recall: 0.8784\n",
      "Epoch 4/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 7ms/step - accuracy: 0.9673 - f1_score: 0.9672 - loss: 0.0915 - precision: 0.9684 - recall: 0.9662 - val_accuracy: 0.8690 - val_f1_score: 0.9033 - val_loss: 0.3939 - val_precision: 0.8713 - val_recall: 0.8668\n",
      "Epoch 5/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - accuracy: 0.9726 - f1_score: 0.9726 - loss: 0.0766 - precision: 0.9734 - recall: 0.9717 - val_accuracy: 0.8607 - val_f1_score: 0.8926 - val_loss: 0.4516 - val_precision: 0.8629 - val_recall: 0.8588\n",
      "Epoch 6/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 6ms/step - accuracy: 0.9772 - f1_score: 0.9772 - loss: 0.0646 - precision: 0.9779 - recall: 0.9766 - val_accuracy: 0.8933 - val_f1_score: 0.9166 - val_loss: 0.3466 - val_precision: 0.8944 - val_recall: 0.8922\n",
      "Epoch 7/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.9799 - f1_score: 0.9799 - loss: 0.0571 - precision: 0.9803 - recall: 0.9795 - val_accuracy: 0.8877 - val_f1_score: 0.9125 - val_loss: 0.3510 - val_precision: 0.8912 - val_recall: 0.8866\n",
      "Epoch 8/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5ms/step - accuracy: 0.9823 - f1_score: 0.9823 - loss: 0.0509 - precision: 0.9827 - recall: 0.9819 - val_accuracy: 0.8912 - val_f1_score: 0.9150 - val_loss: 0.3916 - val_precision: 0.8926 - val_recall: 0.8900\n",
      "Epoch 9/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 6ms/step - accuracy: 0.9840 - f1_score: 0.9840 - loss: 0.0456 - precision: 0.9843 - recall: 0.9837 - val_accuracy: 0.8894 - val_f1_score: 0.9114 - val_loss: 0.3907 - val_precision: 0.8910 - val_recall: 0.8884\n",
      "Epoch 10/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 6ms/step - accuracy: 0.9857 - f1_score: 0.9857 - loss: 0.0413 - precision: 0.9860 - recall: 0.9854 - val_accuracy: 0.8991 - val_f1_score: 0.9209 - val_loss: 0.3519 - val_precision: 0.9000 - val_recall: 0.8983\n",
      "Evaluating model 1D-CNN...\n",
      "\u001b[1m1561/1561\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step\n",
      "\n",
      "==================== TRAINING MODEL: RandomForest ====================\n",
      "Evaluating model RandomForest...\n",
      "\n",
      "==================== TRAINING MODEL: MLP ====================\n",
      "Epoch 1/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7924 - f1_score: 0.7915 - loss: 0.5506 - precision: 0.8593 - recall: 0.7007 - specificity: 0.8454"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 08:39:48.674708: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_132_0', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-08-14 08:39:48.775540: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_139', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-14 08:39:48.937550: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_132', 484 bytes spill stores, 484 bytes spill loads\n",
      "\n",
      "2025-08-14 08:39:49.175732: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_139', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-08-14 08:39:52.212998: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_132_0', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-08-14 08:39:52.438540: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_132', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-08-14 08:39:52.464202: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_132', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 6ms/step - accuracy: 0.8530 - f1_score: 0.8526 - loss: 0.4007 - precision: 0.8863 - recall: 0.8129 - specificity: 0.9256 - val_accuracy: 0.7687 - val_f1_score: 0.8320 - val_loss: 0.5274 - val_precision: 0.7809 - val_recall: 0.7498 - val_specificity: 0.8679\n",
      "Epoch 2/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.9174 - f1_score: 0.9173 - loss: 0.2314 - precision: 0.9262 - recall: 0.9086 - specificity: 0.9781 - val_accuracy: 0.8218 - val_f1_score: 0.8684 - val_loss: 0.4527 - val_precision: 0.8301 - val_recall: 0.8129 - val_specificity: 0.9026\n",
      "Epoch 3/10\n",
      "\u001b[1m4471/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9361 - f1_score: 0.9360 - loss: 0.1796 - precision: 0.9420 - recall: 0.9303 - specificity: 0.9875 - val_accuracy: 0.8464 - val_f1_score: 0.8843 - val_loss: 0.4082 - val_precision: 0.8530 - val_recall: 0.8383 - val_specificity: 0.9253\n",
      "Epoch 4/10\n",
      "\u001b[1m3873/4471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9438 - f1_score: 0.9437 - loss: 0.1544 - precision: 0.9482 - recall: 0.9399 - specificity: 0.9907"
     ]
    }
   ],
   "source": [
    "# Training Multiple Models\n",
    "input_shape_cnn = (X_train_cnn.shape[1], X_train_cnn.shape[2])\n",
    "input_dim = X_train_mlp.shape[1]\n",
    "# output_dim already defined from the DATA PREPARATION section\n",
    "\n",
    "models = {\n",
    "    # \"1D-CNN\": create_cnn_model(input_shape_cnn, output_dim),\n",
    "    \"1D-CNN\": create_cnn_model_optimized(input_shape_cnn, output_dim),\n",
    "    \"RandomForest\": create_rf_model(),\n",
    "    \"MLP\": create_mlp_model(input_dim, output_dim)\n",
    "}\n",
    "\n",
    "# Dictionary to store the final results\n",
    "results = {}\n",
    "\n",
    "# --- TRAINING AND EVALUATING EACH MODEL ---\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*20} TRAINING MODEL: {name} {'='*20}\")\n",
    "\n",
    "    # 🧠 Training\n",
    "    if name == \"1D-CNN\":\n",
    "        model.fit(\n",
    "            X_train_cnn, y_train_cnn,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=1,\n",
    "            validation_data=(X_val_cnn, y_val_cnn) # Using the existing validation set\n",
    "        )\n",
    "    elif name == \"MLP\":\n",
    "        model.fit(\n",
    "            X_train_mlp, y_train_mlp,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=1,\n",
    "            validation_data=(X_val_mlp, y_val_mlp) # Using the existing validation set\n",
    "        )\n",
    "    else: # 📊 RandomForest\n",
    "        model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "    # ⚡ Prediction on the Test Set\n",
    "    print(f\"Evaluating model {name}...\")\n",
    "    if name == \"MLP\":\n",
    "        y_pred_raw = model.predict(X_test_mlp)\n",
    "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "    elif name == \"1D-CNN\":\n",
    "        y_pred_raw = model.predict(X_test_cnn)\n",
    "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "    else: # RandomForest\n",
    "        y_pred = model.predict(X_test_rf)\n",
    "\n",
    "    # Save prediction results for final evaluation\n",
    "    results[name] = {'y_pred': y_pred}\n",
    "\n",
    "# --- PRINT ALL RESULTS SIMULTANEOUSLY ---\n",
    "\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)', 'Unknown (Q)']\n",
    "\n",
    "print(f\"\\n{'='*25} FINAL EVALUATION RESULTS {'='*25}\")\n",
    "\n",
    "for name, result_data in results.items():\n",
    "    y_pred_test = result_data['y_pred']\n",
    "\n",
    "    print(f\"\\n\\n--- REPORT FOR MODEL: {name} ---\")\n",
    "    # Standard Classification Report (using y_test_final, the original integer labels)\n",
    "    print(\"\\nClassification Report on the Test Set:\")\n",
    "    print(classification_report(y_test_final, y_pred_test, target_names=class_names))\n",
    "\n",
    "    # Additional Metrics Report\n",
    "    print(\"Additional Metrics Report:\")\n",
    "    cm = confusion_matrix(y_test_final, y_pred_test)\n",
    "    \n",
    "    for i in range(len(class_names)):\n",
    "        tn = cm.sum() - (cm[i,:].sum() + cm[:,i].sum() - cm[i,i])\n",
    "        tp = cm[i,i]\n",
    "        fp = cm[:,i].sum() - cm[i,i]\n",
    "        fn = cm[i,:].sum() - cm[i,i]\n",
    "\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "        print(f\"  Class: {class_names[i]}\")\n",
    "        print(f\"    - Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "        print(f\"    - Specificity         : {specificity:.4f}\")\n",
    "        print(f\"    - False Positive Rate : {fpr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f6a7b-b10a-4e43-89be-2dcaa76b0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training MLP Model - TA242501010\n",
    "# Initialize AI model\n",
    "model = create_mlp_model(input_dim, output_dim)\n",
    "\n",
    "# Prepare EarlyStopping callback for F1 score validation\n",
    "early_stopping_1 = EarlyStopping(monitor='val_f1_score', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Prepare class weights\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Start AI model training\n",
    "history = model.fit(\n",
    "    X_train_resampled,\n",
    "    y_train_resampled_encoded,\n",
    "    epochs=20,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_fold, y_val_fold_encoded),\n",
    "    verbose=1,\n",
    "    # class_weight=class_weight_dict,\n",
    "    validation_split=0.2\n",
    "    # callbacks=[early_stopping_1]\n",
    ")\n",
    "\n",
    "# Evaluate on the untouched set\n",
    "print(\"Evaluating on the untouched test set...\")\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_pred_test_raw = model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions back to labels if they are one-hot encoded\n",
    "if hasattr(y_pred_test_raw, 'shape') and len(y_pred_test_raw.shape) > 1:\n",
    "      y_pred_test = np.argmax(y_pred_test_raw, axis=1)\n",
    "else:\n",
    "      y_pred_test = y_pred_test_raw\n",
    "\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)', 'Unknown (Q)']\n",
    "\n",
    "print(\"\\nClassification Report on the Test Set:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=class_names))\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Calculate metrics for each class (one-vs-rest)\n",
    "print(\"\\nAdditional Metrics Report:\")\n",
    "print(\"=\"*55)\n",
    "for i in range(len(class_names)):\n",
    "    tn = cm.sum() - (cm[i,:].sum() + cm[:,i].sum() - cm[i,i])\n",
    "    tp = cm[i,i]\n",
    "    fp = cm[:,i].sum() - cm[i,i]\n",
    "    fn = cm[i,:].sum() - cm[i,i]\n",
    "\n",
    "    # Sensitivity (Recall)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    # Specificity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    # False Positive Rate\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "    print(f\"Class: {class_names[i]}\")\n",
    "    print(f\"  Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"  Specificity         : {specificity:.4f}\")\n",
    "    print(f\"  False Positive Rate : {fpr:.4f}\")\n",
    "    print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547cc6c-40fd-4467-9684-fde9c688514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Hyperparameter Tuning\n",
    "print(\"\\n--- Starting Automatic Hyperparameter Tuning with KerasTuner ---\")\n",
    "\n",
    "# Calculate class_weight only once\n",
    "# class_weights = compute_class_weight(\n",
    "#     'balanced',\n",
    "#     classes=np.unique(y_train),\n",
    "#     y=y_train\n",
    "# )\n",
    "# class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Defining tuner objectives\n",
    "multi_objectives = [\n",
    "    kt.Objective(\"val_f1_score\", direction=\"max\")\n",
    "    #kt.Objective(\"val_specificity\", direction=\"max\")\n",
    "    #kt.Objective(\"val_precision\", direction=\"max\")\n",
    "    #kt.Objective(\"val_recall\", direction=\"max\")\n",
    "    #kt.Objective(\"val_auc_roc\", direction=\"max\")\n",
    "    #kt.Objective(\"val_auc_pr\", direction=\"max\")\n",
    "]\n",
    "# Initialize Tuner with RandomSearch\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=multi_objectives, # Target: maximize validation F1 score\n",
    "    max_trials=20,              # Total number of hyperparameter combinations to be tried\n",
    "    executions_per_trial=1,     # Number of models trained per combination (for stability)\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='ecg_classification_0834' # Can change the name to find the latest parameters with the latest code\n",
    ")\n",
    "\n",
    "# Prepare EarlyStopping callback for F1 score validation\n",
    "early_stopping = EarlyStopping(monitor='val_f1_score', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Run the search\n",
    "print(\"\\nStarting the search for the best hyperparameters...\")\n",
    "tuner.search(\n",
    "    X_train_resampled,              # Training data that has been processed with SMOTE\n",
    "    y_train_resampled_encoded,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val_fold, y_val_fold_encoded), # 1. Use validation_data\n",
    "    # class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters and the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "--- Search Complete ---\n",
    "Best hyperparameters found:\n",
    "- Units 1: {best_hps.get('units_1')}\n",
    "- Dropout 1: {best_hps.get('dropout_1'):.2f}\n",
    "- Units 2: {best_hps.get('units_2')}\n",
    "- Dropout 2: {best_hps.get('dropout_2'):.2f}\n",
    "- Learning Rate: {best_hps.get('learning_rate')}\n",
    "\"\"\")\n",
    "\n",
    "# --- Final Evaluation on the Test Set ---\n",
    "print(\"\\n--- Evaluating the Best Model on the Test Set ---\")\n",
    "X_test_scaled = scaler.transform(X_test) # Use the same scaler from training\n",
    "y_pred_test_raw = best_model.predict(X_test_scaled)\n",
    "y_pred_test = np.argmax(y_pred_test_raw, axis=1)\n",
    "class_names = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)', 'Unknown (Q)']\n",
    "\n",
    "# --- 1. Classification Report ----\n",
    "print(\"\\nClassification Report on the Test Set:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=class_names))\n",
    "\n",
    "# --- 2. Confusion Matrix ---\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Specific Metric Calculation per Class ---\n",
    "print(\"\\n--- Detailed Performance Metrics per Class ---\")\n",
    "metrics_data = []\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Specificity calculation\n",
    "    tn = cm.sum() - (cm[i,:].sum() + cm[:,i].sum() - cm[i,i])\n",
    "    fp = cm[:,i].sum() - cm[i,i]\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    metrics_data.append({\n",
    "        \"Class\": class_name,\n",
    "        \"Precision\": precision_score(y_test, y_pred_test, average=None)[i],\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, y_pred_test, average=None)[i],\n",
    "        \"F1-Score\": f1_score(y_test, y_pred_test, average=None)[i],\n",
    "        \"Specificity\": specificity\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(metrics_df.to_string())\n",
    "\n",
    "\n",
    "# --- 4. Calculation of AUC-ROC and AUC-PR (One-vs-Rest) ---\n",
    "y_test_encoded = to_categorical(y_test, num_classes=output_dim)\n",
    "\n",
    "# Add this line to define y_pred_proba\n",
    "y_pred_proba = best_model.predict(X_test_scaled)\n",
    "\n",
    "# AUC-ROC\n",
    "auc_roc_ovr = roc_auc_score(y_test_encoded, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "print(f\"\\nAUC-ROC (One-vs-Rest, Weighted): {auc_roc_ovr:.4f}\")\n",
    "\n",
    "# AUC-PR\n",
    "# Calculate for each class and average\n",
    "precision_curves = dict()\n",
    "recall_curves = dict()\n",
    "auc_pr_scores = []\n",
    "for i in range(output_dim):\n",
    "    precision_curves[i], recall_curves[i], _ = precision_recall_curve(y_test_encoded[:, i], y_pred_proba[:, i])\n",
    "    auc_pr_scores.append(auc(recall_curves[i], precision_curves[i]))\n",
    "\n",
    "# Weighted average for AUC-PR\n",
    "support = np.bincount(y_test)\n",
    "avg_auc_pr = np.average(auc_pr_scores, weights=support)\n",
    "print(f\"AUC-PR (One-vs-Rest, Weighted): {avg_auc_pr:.4f}\")\n",
    "\n",
    "# Plotting PR Curves for each class\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.plot(recall_curves[i], precision_curves[i], lw=2, label=f'{class_name} (AUC-PR = {auc_pr_scores[i]:.2f})')\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve per Class\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
