{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374dbb81-b280-4e45-ae8a-74ad54e76152",
   "metadata": {},
   "source": [
    "# training-C.ipynb\n",
    "1. This code is intended for training three models with raw data for 1D-CNN and DWT data for MLP and Random Forest\n",
    "2. The dataset are MIT-BIH Arrhythmia Database and additional ECG data with a format of .bin\n",
    "3. Classes/labels: N, S, V, F, and/without Q\n",
    "4. The data utilizes dual-lead based on MIT-BIH Arrhythmia Database so that the ECG data from .bin is duplicated onto the other lead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e32954",
   "metadata": {},
   "source": [
    "## **LIBRARY IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0b717-46b3-4967-962f-6d5f2eae8848",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import cudf\n",
    "import os\n",
    "import joblib\n",
    "import pywt\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb  # For reading MIT-BIH data\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import neurokit2 as nk\n",
    "\n",
    "# Scikit-learn and Imbalanced-learn imports\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "from scipy.signal import find_peaks, resample, butter, filtfilt, iirnotch, spectrogram\n",
    "from sklearn.utils import class_weight\n",
    "from glob import glob\n",
    "\n",
    "# Model imports\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Dropout, Add, GlobalAveragePooling1D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Additional setups\n",
    "# Checking cUML\n",
    "print(cudf.Series([1, 2, 3]))\n",
    "\n",
    "# Setting TensorFlow flags\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Checking GPU\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print(f\"TensorFlow has detected {len(gpu_devices)} GPU(s):\")\n",
    "    for device in gpu_devices:\n",
    "        print(f\"- {device}\")\n",
    "else:\n",
    "    print(\"TensorFlow did not detect any GPUs. Training will run on the CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348ca32-352e-4791-a764-9a1dedd1af04",
   "metadata": {},
   "source": [
    "## **DATA PREPARATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb05f79-645a-4ff3-b8f5-44e867b01a39",
   "metadata": {},
   "source": [
    "### DATA PREPARATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a11cb1-fc53-445e-933c-49f337cb0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'N': 0, '.': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0,  # Class 0: Normal Beats (N)\n",
    "    'V': 1, 'E': 1,                                  # Class 1: Ventricular Ectopic (VEB)\n",
    "    'S': 2, 'A': 2, 'a': 2, 'J': 2,                  # Class 2: Supraventricular Ectopic (SVEB)\n",
    "    'F': 3                                           # Class 3: Fusion Beat (F)\n",
    "}\n",
    "\n",
    "DB_PATH_MIT = '../data/raw/MIT-BIH/mit-bih-arrhythmia-database-1.0.0/mit-bih-arrhythmia-database-1.0.0/'\n",
    "\n",
    "FS_MIT = 360\n",
    "FS_TARGET = 500\n",
    "WINDOW_SIZE = int(FS_TARGET*0.8)  # 400 samples -> 0.8s * 500Hz\n",
    "# Split MIT-BIH records into training and testing sets to prevent patient data leakage\n",
    "RECORDS_TRAIN = ['101', '106', '108', '109', '112', '114', '115', '116', '118', '119',\n",
    "                 '122', '124', '201', '203', '205', '207', '208', '209', '215', '220',\n",
    "                 '223', '230'] # DS1\n",
    "RECORDS_TEST = ['100', '103', '105', '111', '113', '117', '121', '123', '200', '202',\n",
    "                '210', '212', '213', '214', '219', '221', '222', '228', '231', '232',\n",
    "                '233', '234'] # DS2\n",
    "custom_file_paths = {\n",
    "    'Arrhythmia': '../data/raw/Arrhythmia/ECG_WAVE.bin',\n",
    "    'Normal': '../data/raw/Normal/ecg_normal.bin'\n",
    "}\n",
    "custom_file_labels = {'Arrhythmia': 2, 'Normal': 0} # SVEB and Normal\n",
    "\n",
    "# Wavelet Feature Configuration\n",
    "WAVELET_TYPE = 'db4'\n",
    "WAVELET_LEVEL = 4\n",
    "\n",
    "# Define the output directory\n",
    "output_dir_prepare_data = '../data/processed'\n",
    "os.makedirs(output_dir_prepare_data, exist_ok=True)\n",
    "\n",
    "def preprocess_signal(signal, fs):\n",
    "    \"\"\"Applies a multi-stage denoising pipeline to a raw ECG signal.\"\"\"\n",
    "    signal = np.array(signal)\n",
    "    nyq = 0.5 * fs\n",
    "    low_cutoff = 0.6\n",
    "    b, a = butter(2, low_cutoff / nyq, btype='high')\n",
    "    signal_bw_removed = filtfilt(b, a, signal)\n",
    "    powerline_freq = 50\n",
    "    b, a = iirnotch(powerline_freq / nyq, Q=30)\n",
    "    signal_pl_removed = filtfilt(b, a, signal_bw_removed)\n",
    "    high_cutoff = 100\n",
    "    b, a = butter(4, high_cutoff / nyq, btype='low')\n",
    "    cleaned_signal = filtfilt(b, a, signal_pl_removed)\n",
    "    return cleaned_signal\n",
    "\n",
    "def extract_dwt_features(window, wavelet, level):\n",
    "    \"\"\"Extracts statistical features from the DWT coefficients.\"\"\"\n",
    "    coeffs = pywt.wavedec(window, wavelet, level=level) # Use wavedec for DWT\n",
    "    features = []\n",
    "    for c in coeffs:\n",
    "        features.append(np.mean(c))\n",
    "        features.append(np.std(c))\n",
    "        features.append(np.var(c))\n",
    "        features.append(np.sum(np.square(c)))\n",
    "        features.append(entropy(np.square(c) + 1e-9))\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_rr_interval_features(r_peaks, current_beat_index, fs):\n",
    "    \"\"\"\n",
    "    Calculates RR interval features for a given heartbeat.\n",
    "    \"\"\"\n",
    "    pre_rr_samples = r_peaks[current_beat_index] - r_peaks[current_beat_index - 1]\n",
    "    pre_rr_seconds = pre_rr_samples / fs\n",
    "\n",
    "    post_rr_samples = r_peaks[current_beat_index + 1] - r_peaks[current_beat_index]\n",
    "    post_rr_seconds = post_rr_samples / fs\n",
    "\n",
    "    local_window_start = max(0, current_beat_index - 10)\n",
    "    local_r_peaks = r_peaks[local_window_start:current_beat_index + 1]\n",
    "    \n",
    "    if len(local_r_peaks) > 1:\n",
    "        local_rr_intervals = np.diff(local_r_peaks) / fs\n",
    "        local_avg_rr = np.mean(local_rr_intervals)\n",
    "    else:\n",
    "        local_avg_rr = pre_rr_seconds\n",
    "\n",
    "    ratio_rr = pre_rr_seconds / (local_avg_rr + 1e-9)\n",
    "\n",
    "    return np.array([pre_rr_seconds, post_rr_seconds, local_avg_rr, ratio_rr])\n",
    "\n",
    "def calculate_specificity(y_true, y_pred, classes):\n",
    "    \"\"\"Calculates macro-averaged specificity from a confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    num_classes = len(classes)\n",
    "    specificities = []\n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        tn = cm.sum() - (tp + fp + fn)\n",
    "        \n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        specificities.append(specificity)\n",
    "        \n",
    "    return np.mean(specificities)\n",
    "\n",
    "# Visualization Function\n",
    "def visualize_preprocessing_result(raw_signal, fs, record_name=\"\"):\n",
    "    \"\"\"\n",
    "    Applies preprocessing and plots the raw vs. cleaned signal for comparison.\n",
    "    \"\"\"\n",
    "    print(f\"Visualizing preprocessing for record: {record_name} (first 10 seconds)...\")\n",
    "    \n",
    "    duration_in_seconds = 10\n",
    "    snippet_length = int(duration_in_seconds * fs)\n",
    "    raw_snippet = raw_signal[:snippet_length]\n",
    "\n",
    "    cleaned_snippet = preprocess_signal(raw_snippet, fs=fs)\n",
    "\n",
    "    time_axis = np.arange(len(raw_snippet)) / fs\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(time_axis, raw_snippet, label='Raw Signal', color='blue', alpha=0.7)\n",
    "    plt.plot(time_axis, cleaned_snippet, label='Cleaned Signal', color='red', linewidth=1.5)\n",
    "    plt.title(f'ECG Signal Preprocessing Result for Record {record_name}')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Amplitude (mV)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2280f36-f477-42a6-ac58-db67d3b4157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATA LOADING UTILITIES\n",
    "def load_mitbih_records(db_path, record_names):\n",
    "    \"\"\"REVERTED: Loads raw ECG signals (two leads) and annotations.\"\"\"\n",
    "    signals_leadA, signals_leadB, all_annotations = [], [], []\n",
    "    print(f\"Loading MIT-BIH records (two leads): {', '.join(record_names)}...\")\n",
    "    for rec_name in record_names:\n",
    "        record_path = os.path.join(db_path, rec_name)\n",
    "        try:\n",
    "            # Load both channels\n",
    "            record = wfdb.rdrecord(record_path, channels=[0, 1])\n",
    "            signals_leadA.append(record.p_signal[:, 0].flatten())\n",
    "            signals_leadB.append(record.p_signal[:, 1].flatten())\n",
    "            annotation = wfdb.rdann(record_path, 'atr')\n",
    "            all_annotations.append(annotation)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing record {rec_name}: {e}\")\n",
    "    print(\"Loading complete.\")\n",
    "    return signals_leadA, signals_leadB, all_annotations\n",
    "\n",
    "def load_ecg_from_bin(file_path, dtype=np.int16):\n",
    "    \"\"\"Loading raw ECG signals from binary files.\"\"\"\n",
    "    try:\n",
    "        signal = np.fromfile(file_path, dtype=dtype)\n",
    "        print(f\"Completed reading {len(signal)} samples from {file_path}\")\n",
    "        return signal\n",
    "    except IOError as e:\n",
    "        print(f\"An error has occurred while reading: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_r_peaks_robust(signal, fs):\n",
    "    \"\"\"Detects R-peaks using a robust algorithm from NeuroKit2.\"\"\"\n",
    "    try:\n",
    "        _, rpeaks_dict = nk.ecg_peaks(signal, sampling_rate=fs)\n",
    "        r_peaks = rpeaks_dict['ECG_R_Peaks']\n",
    "        return r_peaks\n",
    "    except Exception as e:\n",
    "        return np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d55906-2b36-4dd9-b9db-dc6663f9a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DATA PREPARATION FUNCTIONS\n",
    "def prepare_features_and_raw_data(signals_A, signals_B, annotations, window_size, fs=360, target_fs=500):\n",
    "    \"\"\"REVERTED: Processes dual-lead MIT-BIH data to generate features and raw windows.\"\"\"\n",
    "    all_combined_features, all_raw_windows, all_labels = [], [], []\n",
    "    samples_before = window_size // 3\n",
    "    samples_after = window_size - samples_before\n",
    "\n",
    "    for i, (raw_signal_A, raw_signal_B) in enumerate(zip(signals_A, signals_B)):\n",
    "        ann = annotations[i]\n",
    "        try:\n",
    "            cleaned_A_orig_fs = preprocess_signal(raw_signal_A, fs=fs)\n",
    "            cleaned_B_orig_fs = preprocess_signal(raw_signal_B, fs=fs)\n",
    "\n",
    "            cleaned_A = resample(cleaned_A_orig_fs, int(len(cleaned_A_orig_fs) * (target_fs / fs)))\n",
    "            cleaned_B = resample(cleaned_B_orig_fs, int(len(cleaned_B_orig_fs) * (target_fs / fs)))\n",
    "            \n",
    "            r_peaks_resampled = np.round(ann.sample * (target_fs / fs)).astype(int)\n",
    "\n",
    "            for j in range(1, len(r_peaks_resampled) - 1):\n",
    "                r_peak_loc = r_peaks_resampled[j]\n",
    "                symbol = ann.symbol[j]\n",
    "                if symbol in label_map:\n",
    "                    start, end = r_peak_loc - samples_before, r_peak_loc + samples_after\n",
    "                    if start >= 0 and end < len(cleaned_A):\n",
    "                        window_A = cleaned_A[start:end]\n",
    "                        window_B = cleaned_B[start:end]\n",
    "                        \n",
    "                        dwt_A = extract_dwt_features(window_A, WAVELET_TYPE, WAVELET_LEVEL)\n",
    "                        dwt_B = extract_dwt_features(window_B, WAVELET_TYPE, WAVELET_LEVEL)\n",
    "                        morph_features = np.concatenate((dwt_A, dwt_B))\n",
    "\n",
    "                        rr_features = extract_rr_interval_features(r_peaks_resampled, j, fs=target_fs)\n",
    "                        \n",
    "                        combined_features = np.concatenate((morph_features, rr_features))\n",
    "                        all_combined_features.append(combined_features)\n",
    "                        # Stack the two leads for the raw data\n",
    "                        all_raw_windows.append(np.stack((window_A, window_B), axis=-1))\n",
    "                        all_labels.append(label_map[symbol])\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process record {ann.record_name}: {e}\")\n",
    "    return np.array(all_combined_features), np.array(all_raw_windows), np.array(all_labels)\n",
    "\n",
    "def prepare_features_and_raw_data_from_bin(signal, r_peaks, window_size, label, target_fs=500):\n",
    "    \"\"\"REVERTED: Processes a single-lead .bin file, duplicating channels for dual-lead models.\"\"\"\n",
    "    all_combined_features, all_raw_windows, all_labels = [], [], []\n",
    "    samples_before = window_size // 3\n",
    "    samples_after = window_size - samples_before\n",
    "    cleaned_signal = preprocess_signal(signal, fs=target_fs)\n",
    "\n",
    "    for j in range(1, len(r_peaks) - 1):\n",
    "        r_peak_loc = r_peaks[j]\n",
    "        start, end = r_peak_loc - samples_before, r_peak_loc + samples_after\n",
    "        if start >= 0 and end < len(cleaned_signal):\n",
    "            window = cleaned_signal[start:end]\n",
    "            \n",
    "            dwt_features = extract_dwt_features(window, WAVELET_TYPE, WAVELET_LEVEL)\n",
    "            # Duplicate features for dual-lead format\n",
    "            morph_features = np.concatenate((dwt_features, dwt_features))\n",
    "            rr_features = extract_rr_interval_features(r_peaks, j, fs=target_fs)\n",
    "            \n",
    "            all_combined_features.append(np.concatenate((morph_features, rr_features)))\n",
    "            # Duplicate and stack window for dual-lead format\n",
    "            all_raw_windows.append(np.stack((window, window), axis=-1))\n",
    "            all_labels.append(label)\n",
    "            \n",
    "    return np.array(all_combined_features), np.array(all_raw_windows), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c0215-637d-49cd-8049-379035e45e3f",
   "metadata": {},
   "source": [
    "### DATA PREPARATION EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82e85c-06ca-4dd4-9d1b-cb0b177484bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # --- [Step 1] Process MIT-BIH Data ---\n",
    "    print(\"--- [Step 1] Processing Dual-Lead MIT-BIH Data ---\")\n",
    "    train_signals_A, train_signals_B, train_anns = load_mitbih_records(DB_PATH_MIT, RECORDS_TRAIN)\n",
    "    \n",
    "    if train_signals_A:\n",
    "        visualize_preprocessing_result(train_signals_A[0], fs=FS_MIT, record_name=RECORDS_TRAIN[0])\n",
    "\n",
    "    X_train_features_base, X_train_raw_base, y_train = prepare_features_and_raw_data(train_signals_A, train_signals_B, train_anns, WINDOW_SIZE, fs=FS_MIT, target_fs=FS_TARGET)\n",
    "\n",
    "    test_signals_A, test_signals_B, test_anns = load_mitbih_records(DB_PATH_MIT, RECORDS_TEST)\n",
    "    X_test_features_mit, X_test_raw_mit, y_test_mitbih = prepare_features_and_raw_data(test_signals_A, test_signals_B, test_anns, WINDOW_SIZE, fs=FS_MIT, target_fs=FS_TARGET)\n",
    "\n",
    "    # --- [Step 2] Process Single-Lead Custom Data for TESTING ---\n",
    "    print(\"\\n--- [Step 2] Processing Single-Lead Custom Data for Testing ---\")\n",
    "    X_test_custom_features_list, X_test_custom_raw_list, y_test_custom_list = [], [], []\n",
    "    for name, path in custom_file_paths.items():\n",
    "        signal = load_ecg_from_bin(path)\n",
    "        if signal is not None:\n",
    "            r_peaks = detect_r_peaks_robust(signal, fs=FS_TARGET)\n",
    "            features_single, raw_single, y_single = prepare_features_and_raw_data_from_bin(\n",
    "                signal, r_peaks, WINDOW_SIZE, custom_file_labels[name], target_fs=FS_TARGET\n",
    "            )\n",
    "            X_test_custom_features_list.append(features_single)\n",
    "            X_test_custom_raw_list.append(raw_single)\n",
    "            y_test_custom_list.append(y_single)\n",
    "            \n",
    "    X_test_custom_features = np.vstack(X_test_custom_features_list)\n",
    "    X_test_custom_raw = np.vstack(X_test_custom_raw_list)\n",
    "    y_test_custom = np.concatenate(y_test_custom_list)\n",
    "\n",
    "    # --- [Step 3] Finalize Datasets and Scale Feature data ---\n",
    "    print(\"\\n--- [Step 3] Finalizing Datasets and Scaling ---\")\n",
    "    print(f\"Training data shapes before splitting:\")\n",
    "    print(f\"Features: {X_train_features_base.shape}, Raw: {X_train_raw_base.shape}, Labels: {y_train.shape}\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_features_scaled = scaler.fit_transform(X_train_features_base)\n",
    "    print(\"Scaler trained on MIT-BIH training features.\")\n",
    "    \n",
    "    if X_test_features_mit.size > 0:\n",
    "        X_test_features_mit_scaled = scaler.transform(X_test_features_mit)\n",
    "    else:\n",
    "        X_test_features_mit_scaled = X_test_features_mit\n",
    "\n",
    "    if X_test_custom_features.size > 0:\n",
    "        X_test_features_custom_scaled = scaler.transform(X_test_custom_features)\n",
    "    else:\n",
    "        X_test_features_custom_scaled = X_test_custom_features\n",
    "    \n",
    "    X_test_features_final = np.concatenate((X_test_features_mit_scaled, X_test_features_custom_scaled), axis=0)\n",
    "    X_test_raw_final = np.concatenate((X_test_raw_mit, X_test_custom_raw), axis=0)\n",
    "    y_test_final = np.concatenate((y_test_mitbih, y_test_custom), axis=0)\n",
    "    \n",
    "    # --- [Step 4] Splitting & Sampling for Feature data ---\n",
    "    print(\"\\n--- [Step 4] Finalizing Training Data (Split & SMOTE Sample) ---\")\n",
    "    X_train_features_fold, X_val_features_fold, y_train_fold, y_val_fold = train_test_split(\n",
    "        X_train_features_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    X_train_raw_fold, X_val_raw_fold, _, _ = train_test_split(\n",
    "        X_train_raw_base, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    print(\"Applying SMOTEEN to feature training data...\")\n",
    "    print(\"Class distribution before sampling:\", Counter(y_train_fold))\n",
    "    sampler = SMOTEEN(random_state=42)\n",
    "    X_train_features_resampled, y_train_features_resampled = sampler.fit_resample(X_train_features_fold, y_train_fold)\n",
    "    print(\"Class distribution after sampling:\", Counter(y_train_features_resampled))\n",
    "\n",
    "    # --- [Step 5] Final Data Preparation for Models ---\n",
    "    print(\"\\n--- [Step 5] Preparing Final Datasets for Models ---\")\n",
    "    output_dim = len(np.unique(y_train))\n",
    "    \n",
    "    # Data for MLP\n",
    "    X_train_mlp, y_train_mlp = X_train_features_resampled, to_categorical(y_train_features_resampled, num_classes=output_dim)\n",
    "    X_val_mlp, y_val_mlp = X_val_features_fold, to_categorical(y_val_fold, num_classes=output_dim)\n",
    "    X_test_mlp, y_test_mlp = X_test_features_final, to_categorical(y_test_final, num_classes=output_dim)\n",
    "\n",
    "    # Data for 1D-CNN\n",
    "    X_train_cnn, y_train_cnn = X_train_raw_fold, to_categorical(y_train_fold, num_classes=output_dim)\n",
    "    X_val_cnn, y_val_cnn = X_val_raw_fold, to_categorical(y_val_fold, num_classes=output_dim)\n",
    "    X_test_cnn, y_test_cnn = X_test_raw_final, to_categorical(y_test_final, num_classes=output_dim)\n",
    "\n",
    "    # Data for RandomForest\n",
    "    X_train_rf, y_train_rf = X_train_features_resampled, y_train_features_resampled\n",
    "    X_val_rf, y_val_rf = X_val_features_fold, y_val_fold\n",
    "    X_test_rf, y_test_rf = X_test_features_final, y_test_final\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ DATA PREPARATION COMPLETE ✅\")\n",
    "    print(f\"Shapes for MLP -> Train: {X_train_mlp.shape}, Val: {X_val_mlp.shape}, Test: {X_test_mlp.shape}\")\n",
    "    print(f\"Shapes for 1D-CNN -> Train: {X_train_cnn.shape}, Val: {X_val_cnn.shape}, Test: {X_test_cnn.shape}\")\n",
    "    print(f\"Shapes for RandomForest -> Train: {X_train_rf.shape}, Val: {X_val_rf.shape}, Test: {X_test_rf.shape}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162fbb6-179a-483f-b663-69c1b64dc040",
   "metadata": {},
   "source": [
    "## **MACHINE LEARNING MODEL TRAINING & SAVING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04aa916-ef83-4e8f-ae5d-80d0a49a2129",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28664a07-7fce-4c07-b780-2ffac2105947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(input_dim, output_dim):\n",
    "    \"\"\"Creates and compiles a Keras MLP model.\"\"\"\n",
    "    model = Sequential([\n",
    "        # Hyperparameters tuning\n",
    "        Dense(512, input_dim=input_dim, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(output_dim, activation='softmax') # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy', # Suitable for one-hot labels\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            # tf.keras.metrics.Precision(name='precision'),\n",
    "            # tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "# 1st CNN model\n",
    "def create_cnn_model(input_shape, output_dim):\n",
    "    \"\"\"Creates and compiles a Keras 1D-CNN model.\"\"\"\n",
    "    # Input shape for CNN must be 3D: (samples, steps, features)\n",
    "    # Example: (10000, 187, 1)\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=512, kernel_size=6, activation='relu', # Reduced filters\n",
    "               input_shape=input_shape),\n",
    "        Dropout(0.1),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        Conv1D(filters=512, kernel_size=3, activation='relu'), # Reduced filters\n",
    "        Dropout(0.2),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "\n",
    "        Flatten(), # Now flattens a much smaller tensor\n",
    "\n",
    "        Dense(512, activation='relu'), # Reduced dense units\n",
    "        Dropout(0.4),\n",
    "\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "# 1D-CNN optimized based on paper\n",
    "def create_cnn_model_optimized(input_shape, output_dim, hp=None):\n",
    "    \"\"\"\n",
    "    Creates and compiles an optimized 1D-CNN model.\n",
    "    If 'hp' is provided, it builds a tunable model for KerasTuner.\n",
    "    Otherwise, it builds a model with default hyperparameters.\n",
    "    \"\"\"\n",
    "    # Define a default hyperparameter object if none is passed\n",
    "    if hp is None:\n",
    "        hp = kt.HyperParameters()\n",
    "        # Set default values for when not tuning\n",
    "        hp.values['conv4_filters'] = 100\n",
    "        hp.values['dense_units'] = 256\n",
    "        hp.values['learning_rate'] = 0.0001\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='valid', name='conv1d_1_freezed')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_1_freezed')(x)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', name='conv1d_2_freezed')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_2_freezed')(x)\n",
    "    x = Conv1D(filters=4, kernel_size=3, activation='relu', padding='same', name='conv1d_3_freezed')(x)\n",
    "    x = MaxPooling1D(pool_size=2, name='maxpool1d_3_freezed')(x)\n",
    "    # ===============================================\n",
    "    #           Trainable Layers\n",
    "    # ===============================================\n",
    "    x = Conv1D(filters=hp.values['conv4_filters'], kernel_size=3, activation='relu', padding='same', name='conv1d_4_trainable')(x)\n",
    "    x = Flatten(name='flatten_layer')(x)\n",
    "    x = Dense(units=hp.values['dense_units'], activation='relu', name='dense_1_trainable')(x)\n",
    "    outputs = Dense(units=output_dim, activation='softmax', name='output_layer_trainable')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.values['learning_rate']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "            tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 1D-ResNet\n",
    "# def create_cnn_model_optimized(input_shape, output_dim, hp=None):\n",
    "#     \"\"\"\n",
    "#     Creates and compiles an optimized 1D-CNN model.\n",
    "#     If 'hp' is provided, it builds a tunable model for KerasTuner.\n",
    "#     Otherwise, it builds a model with default hyperparameters.\n",
    "#     \"\"\"\n",
    "#     # Define a default hyperparameter object if none is passed\n",
    "#     if hp is None:\n",
    "#         hp = kt.HyperParameters()\n",
    "#         # Set default values for when not tuning\n",
    "#         hp.values['initial_filters'] = 384\n",
    "#         hp.values['res_block_1_filters'] = 384\n",
    "#         hp.values['res_block_2_filters'] = 384\n",
    "#         hp.values['kernel_size_initial'] = 7\n",
    "#         hp.values['kernel_size_res'] = 5\n",
    "#         hp.values['dropout_1'] = 0.1\n",
    "#         hp.values['dropout_2'] = 0.3\n",
    "#         hp.values['dense_units'] = 512\n",
    "#         hp.values['dense_dropout'] = 0.5\n",
    "#         hp.values['learning_rate'] = 0.0001\n",
    "\n",
    "#     def residual_block(x, filters, kernel_size):\n",
    "#         y = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "#         y = BatchNormalization()(y)\n",
    "#         y = Activation('relu')(y)\n",
    "#         y = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(y)\n",
    "#         y = BatchNormalization()(y)\n",
    "#         shortcut = Conv1D(filters=filters, kernel_size=1, padding='same')(x) if x.shape[-1] != filters else x\n",
    "#         res_output = Add()([shortcut, y])\n",
    "#         return Activation('relu')(res_output)\n",
    "\n",
    "#     inputs = Input(shape=input_shape)\n",
    "#     x = Conv1D(filters=hp.values['initial_filters'], kernel_size=hp.values['kernel_size_initial'], padding='same')(inputs)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "#     x = residual_block(x, filters=hp.values['res_block_1_filters'], kernel_size=hp.values['kernel_size_res'])\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "#     x = Dropout(hp.values['dropout_1'])(x)\n",
    "#     x = residual_block(x, filters=hp.values['res_block_2_filters'], kernel_size=hp.values['kernel_size_res'])\n",
    "#     x = MaxPooling1D(pool_size=2)(x)\n",
    "#     x = Dropout(hp.values['dropout_2'])(x)\n",
    "#     x = GlobalAveragePooling1D()(x)\n",
    "#     x = Dense(hp.values['dense_units'], activation='relu')(x)\n",
    "#     x = Dropout(hp.values['dense_dropout'])(x)\n",
    "#     outputs = Dense(output_dim, activation='softmax')(x)\n",
    "    \n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=hp.values['learning_rate']),\n",
    "#         loss='categorical_crossentropy',\n",
    "#         metrics=[\n",
    "#             # 'accuracy',\n",
    "#             tf.keras.metrics.Precision(name='precision'),\n",
    "#             tf.keras.metrics.Recall(name='recall'),\n",
    "#             # Note: F1Score might require a different setup in some TF versions.\n",
    "#             # If it causes issues, consider a custom callback to calculate it.\n",
    "#             tf.keras.metrics.F1Score(average='weighted', name='f1_score'),\n",
    "#             tf.keras.metrics.SpecificityAtSensitivity(0.9, name='specificity')\n",
    "#         ]\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# Function to create the RandomForest model\n",
    "def create_rf_model():\n",
    "    \"\"\"Creates an instance of the GPU-accelerated RandomForestClassifier model using cuML.\"\"\"\n",
    "    # Hyperparameters are similar to imblearn's\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=200, \n",
    "        max_depth=30, \n",
    "        random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16e19d-a20a-4308-9848-37de070b26a3",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING MODEL TRAINING EXECUTION\n",
    "1. Targeted metrics: Precision, Recall, F1-Score, and Specificity\n",
    "2. After training, all models immediately saved and exported into a certain folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ea05e-468d-4a2c-b58f-b144471e3287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Single-fold validation ---\n",
    "# Define the output directory for saving the models and artifacts\n",
    "OUTPUT_DIR = '../models'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define class names for reporting and plotting\n",
    "CLASS_NAMES = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)']\n",
    "# ==============================================================================\n",
    "# --- MANUAL CLASS WEIGHT CALCULATION ---\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*20 + \" MANUAL CLASS WEIGHT CALCULATION \" + \"=\"*20)\n",
    "\n",
    "# We use the y_train data designated for the CNN before it's combined for CV\n",
    "# This represents the original class distribution before any splits or sampling.\n",
    "y_full_train_cnn_for_weights = np.argmax(np.concatenate((y_train_cnn, y_val_cnn), axis=0), axis=1)\n",
    "\n",
    "n_samples = len(y_full_train_cnn_for_weights)\n",
    "class_counts = Counter(y_full_train_cnn_for_weights)\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "class_weights_dict = {}\n",
    "print(\"Manual Calculation Breakdown:\")\n",
    "for i in sorted(class_counts.keys()):\n",
    "    n_samples_in_class = class_counts[i]\n",
    "    weight = n_samples / (n_classes * n_samples_in_class)\n",
    "    class_weights_dict[i] = weight\n",
    "    print(f\"  Weight for Class {i} ({CLASS_NAMES[i]}): {n_samples} / ({n_classes} * {n_samples_in_class}) = {weight:.4f}\")\n",
    "\n",
    "print(\"\\nFinal Calculated Class Weights to be used for 1D-CNN:\", class_weights_dict)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- TRAINING CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "# Define model creation parameters\n",
    "input_shape_cnn = (X_train_cnn.shape[1], X_train_cnn.shape[2])\n",
    "input_dim_mlp = X_train_mlp.shape[1]\n",
    "output_dim = y_train_cnn.shape[1]\n",
    "\n",
    "# Create a dictionary of models to be trained\n",
    "models = {\n",
    "    \"1D-CNN\": create_cnn_model(input_shape_cnn, output_dim),\n",
    "    \"RandomForest\": create_rf_model(),\n",
    "    \"MLP\": create_mlp_model(input_dim_mlp, output_dim)\n",
    "}\n",
    "\n",
    "# Dictionary to store the final evaluation results\n",
    "results = {}\n",
    "\n",
    "# ==============================================================================\n",
    "# --- TRAINING AND EVALUATING EACH MODEL ---\n",
    "# ==============================================================================\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*20} TRAINING MODEL: {name} {'='*20}\")\n",
    "\n",
    "    # --- Training Step ---\n",
    "    if name == \"1D-CNN\":\n",
    "        model.fit(\n",
    "            X_train_cnn, y_train_cnn,\n",
    "            epochs=150,\n",
    "            batch_size=100,\n",
    "            verbose=1,\n",
    "            validation_data=(X_val_cnn, y_val_cnn),\n",
    "            class_weight=class_weights_dict\n",
    "        )\n",
    "    elif name == \"MLP\":\n",
    "        model.fit(\n",
    "            X_train_mlp, y_train_mlp,\n",
    "            epochs=100,\n",
    "            batch_size=128,\n",
    "            verbose=1,\n",
    "            validation_data=(X_val_mlp, y_val_mlp),\n",
    "            class_weight=class_weights_dict # Applying weights as per user example\n",
    "        )\n",
    "    else:  # RandomForest\n",
    "        model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "    # --- Prediction Step on the Test Set ---\n",
    "    print(f\"\\n--- Evaluating model {name} on the test set... ---\")\n",
    "    if name in [\"MLP\", \"1D-CNN\"]:\n",
    "        # Use appropriate test data for each model\n",
    "        X_test_data = X_test_mlp if name == \"MLP\" else X_test_cnn\n",
    "        y_pred_raw = model.predict(X_test_data)\n",
    "        y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "    else:  # RandomForest\n",
    "        y_pred = model.predict(X_test_rf)\n",
    "\n",
    "    # Store prediction results and ground truth for final reporting\n",
    "    results[name] = {'y_pred': y_pred, 'y_true': y_test_final, 'model_instance': model}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- [PART 3] FINAL REPORTING AND MODEL SAVING ---\n",
    "# ==============================================================================\n",
    "print(f\"\\n{'='*25} FINAL EVALUATION RESULTS {'='*25}\")\n",
    "\n",
    "for name, result_data in results.items():\n",
    "    y_true = result_data['y_true']\n",
    "    y_pred = result_data['y_pred']\n",
    "    model_to_save = result_data['model_instance']\n",
    "\n",
    "    print(f\"\\n\\n{'~'*15} REPORT FOR MODEL: {name} {'~'*15}\")\n",
    "\n",
    "    # --- Classification Report ---\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=CLASS_NAMES, zero_division=0)\n",
    "    print(report)\n",
    "\n",
    "    # --- Confusion Matrix Visualization ---\n",
    "    plot_confusion_matrix(y_true, y_pred, name)\n",
    "\n",
    "    # --- Saving the Trained Model ---\n",
    "    print(f\"--- Saving model: {name} ---\")\n",
    "    if name in [\"1D-CNN\", \"MLP\"]:\n",
    "        model_path = os.path.join(OUTPUT_DIR, f\"model_{name.lower()}_final.keras\")\n",
    "        model_to_save.save(model_path)\n",
    "        print(f\"✅ Model {name} has been saved to: {model_path}\")\n",
    "    else:  # RandomForest\n",
    "        model_path = os.path.join(OUTPUT_DIR, f\"model_{name.lower()}_final.joblib\")\n",
    "        joblib.dump(model_to_save, model_path)\n",
    "        print(f\"✅ Model {name} has been saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2cd67-b661-4d6b-a3fc-d87dd454416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ten-fold cross validation ---\n",
    "# --- [SETUP] Define common variables and helper functions ---\n",
    "# Define the output directory for saving the models and artifacts\n",
    "OUTPUT_DIR = '../models'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Define class names for reporting and plotting\n",
    "CLASS_NAMES = ['Normal (N)', 'Ventricular (V)', 'Supraventricular (S)', 'Fusion (F)']\n",
    "N_SPLITS = 10 # Number of folds for cross-validation\n",
    "# ==============================================================================\n",
    "# --- MANUAL CLASS WEIGHT CALCULATION ---\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*20 + \" MANUAL CLASS WEIGHT CALCULATION \" + \"=\"*20)\n",
    "\n",
    "# We use the y_train data designated for the CNN before it's combined for CV\n",
    "# This represents the original class distribution before any splits or sampling.\n",
    "y_full_train_cnn_for_weights = np.argmax(np.concatenate((y_train_cnn, y_val_cnn), axis=0), axis=1)\n",
    "\n",
    "n_samples = len(y_full_train_cnn_for_weights)\n",
    "class_counts = Counter(y_full_train_cnn_for_weights)\n",
    "n_classes = len(class_counts)\n",
    "\n",
    "class_weights_dict = {}\n",
    "print(\"Manual Calculation Breakdown:\")\n",
    "for i in sorted(class_counts.keys()):\n",
    "    n_samples_in_class = class_counts[i]\n",
    "    weight = n_samples / (n_classes * n_samples_in_class)\n",
    "    class_weights_dict[i] = weight\n",
    "    print(f\"  Weight for Class {i} ({CLASS_NAMES[i]}): {n_samples} / ({n_classes} * {n_samples_in_class}) = {weight:.4f}\")\n",
    "\n",
    "print(\"\\nFinal Calculated Class Weights to be used for 1D-CNN:\", class_weights_dict)\n",
    "\n",
    "# ==============================================================================\n",
    "# --- [PART 1] MLP MODEL TRAINING & 10-FOLD CROSS-VALIDATION ---\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*25 + \" MLP MODEL TRAINING \" + \"=\"*25)\n",
    "\n",
    "# --- [1.1] Prepare Full Dataset for Cross-Validation ---\n",
    "# Combine the initial training and validation sets created from SMOTE-resampled data\n",
    "X_full_train_mlp = np.concatenate((X_train_mlp, X_val_mlp), axis=0)\n",
    "y_full_train_mlp_cat = np.concatenate((y_train_mlp, y_val_mlp), axis=0)\n",
    "y_full_train_mlp = np.argmax(y_full_train_mlp_cat, axis=1) # 1D labels for StratifiedKFold\n",
    "\n",
    "input_dim_mlp = X_full_train_mlp.shape[1]\n",
    "output_dim_mlp = y_full_train_mlp_cat.shape[1]\n",
    "\n",
    "# --- [1.2] 10-Fold Cross-Validation ---\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "fold_metrics = {'loss': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
    "\n",
    "print(f\"\\n--- Starting {N_SPLITS}-Fold Cross-Validation for MLP ---\")\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X_full_train_mlp, y_full_train_mlp)):\n",
    "    print(f\"\\n--- FOLD {fold + 1}/{N_SPLITS} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train_fold, X_val_fold = X_full_train_mlp[train_index], X_full_train_mlp[val_index]\n",
    "    y_train_fold_cat, y_val_fold_cat = y_full_train_mlp_cat[train_index], y_full_train_mlp_cat[val_index]\n",
    "\n",
    "    # Create a new model instance for each fold to ensure independent training\n",
    "    model_mlp = create_mlp_model(input_dim=input_dim_mlp, output_dim=output_dim_mlp)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training...\")\n",
    "    model_mlp.fit(\n",
    "        X_train_fold, y_train_fold_cat,\n",
    "        epochs=100,\n",
    "        batch_size=128,\n",
    "        verbose=0,\n",
    "        validation_data=(X_val_fold, y_val_fold_cat)\n",
    "    )\n",
    "\n",
    "    # Evaluate the fold\n",
    "    results = model_mlp.evaluate(X_val_fold, y_val_fold_cat, verbose=0)\n",
    "    fold_metrics['loss'].append(results[0])\n",
    "    fold_metrics['precision'].append(results[1])\n",
    "    fold_metrics['recall'].append(results[2])\n",
    "    fold_metrics['f1_score'].append(results[3])\n",
    "    \n",
    "    print(f\"Fold {fold + 1} - Val Loss: {results[0]:.4f}, Val F1-Score: {results[3]:.4f}\")\n",
    "\n",
    "# --- [1.3] Average Cross-Validation Results ---\n",
    "print(f\"\\n{'='*20} MLP CV SUMMARY {'='*20}\")\n",
    "print(f\"Average Validation Loss: {np.mean(fold_metrics['loss']):.4f} (+/- {np.std(fold_metrics['loss']):.4f})\")\n",
    "print(f\"Average Validation F1-Score: {np.mean(fold_metrics['f1_score']):.4f} (+/- {np.std(fold_metrics['f1_score']):.4f})\")\n",
    "print(f\"Average Validation Precision: {np.mean(fold_metrics['precision']):.4f} (+/- {np.std(fold_metrics['precision']):.4f})\")\n",
    "print(f\"Average Validation Recall: {np.mean(fold_metrics['recall']):.4f} (+/- {np.std(fold_metrics['recall']):.4f})\")\n",
    "\n",
    "\n",
    "# --- [1.4] Final Model Training on ALL Data ---\n",
    "print(f\"\\n{'='*20} TRAINING FINAL MLP MODEL ON ALL DATA {'='*20}\")\n",
    "final_model_mlp = create_mlp_model(input_dim=input_dim_mlp, output_dim=output_dim_mlp)\n",
    "final_model_mlp.fit(\n",
    "    X_full_train_mlp, y_full_train_mlp_cat,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- [1.5] Final Evaluation on the Hold-Out Test Set ---\n",
    "print(f\"\\n{'='*20} FINAL MLP EVALUATION ON TEST SET {'='*20}\")\n",
    "y_pred_mlp_raw = final_model_mlp.predict(X_test_mlp)\n",
    "y_pred_mlp = np.argmax(y_pred_mlp_raw, axis=1)\n",
    "y_true_mlp = np.argmax(y_test_mlp, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_mlp, y_pred_mlp, target_names=CLASS_NAMES, zero_division=0))\n",
    "plot_confusion_matrix(y_true_mlp, y_pred_mlp, \"MLP\")\n",
    "\n",
    "# --- [1.6] Saving the Final Model ---\n",
    "model_path = os.path.join(OUTPUT_DIR, \"model_mlp_final.keras\")\n",
    "final_model_mlp.save(model_path)\n",
    "print(f\"✅ Final MLP Model has been saved to: {model_path}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- [PART 2] RANDOM FOREST MODEL TRAINING & 10-FOLD CROSS-VALIDATION ---\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" RANDOM FOREST MODEL TRAINING \" + \"=\"*20)\n",
    "\n",
    "# --- [2.1] Prepare Full Dataset for Cross-Validation ---\n",
    "# Combine initial training and validation sets. Labels are already 1D.\n",
    "X_full_train_rf = np.concatenate((X_train_rf, X_val_rf), axis=0)\n",
    "y_full_train_rf = np.concatenate((y_train_rf, y_val_rf), axis=0)\n",
    "\n",
    "# --- [2.2] 10-Fold Cross-Validation ---\n",
    "skf_rf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "fold_metrics_rf = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
    "\n",
    "print(f\"\\n--- Starting {N_SPLITS}-Fold Cross-Validation for RandomForest ---\")\n",
    "for fold, (train_index, val_index) in enumerate(skf_rf.split(X_full_train_rf, y_full_train_rf)):\n",
    "    print(f\"\\n--- FOLD {fold + 1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    # Split data for the current fold\n",
    "    X_train_fold, X_val_fold = X_full_train_rf[train_index], X_full_train_rf[val_index]\n",
    "    y_train_fold, y_val_fold = y_full_train_rf[train_index], y_full_train_rf[val_index]\n",
    "\n",
    "    # Create a new model instance for each fold\n",
    "    model_rf = create_rf_model()\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training...\")\n",
    "    model_rf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Evaluate the fold\n",
    "    y_pred_fold = model_rf.predict(X_val_fold)\n",
    "    accuracy = accuracy_score(y_val_fold, y_pred_fold)\n",
    "    precision = precision_score(y_val_fold, y_pred_fold, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_val_fold, y_pred_fold, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_val_fold, y_pred_fold, average='macro', zero_division=0)\n",
    "    \n",
    "    fold_metrics_rf['accuracy'].append(accuracy)\n",
    "    fold_metrics_rf['precision'].append(precision)\n",
    "    fold_metrics_rf['recall'].append(recall)\n",
    "    fold_metrics_rf['f1_score'].append(f1)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} - Val Accuracy: {accuracy:.4f}, Val Macro F1-Score: {f1:.4f}\")\n",
    "\n",
    "# --- [2.3] Average Cross-Validation Results ---\n",
    "print(f\"\\n{'='*20} RANDOM FOREST CV SUMMARY {'='*20}\")\n",
    "print(f\"Average Validation Accuracy: {np.mean(fold_metrics_rf['accuracy']):.4f} (+/- {np.std(fold_metrics_rf['accuracy']):.4f})\")\n",
    "print(f\"Average Validation F1-Score: {np.mean(fold_metrics_rf['f1_score']):.4f} (+/- {np.std(fold_metrics_rf['f1_score']):.4f})\")\n",
    "print(f\"Average Validation Precision: {np.mean(fold_metrics_rf['precision']):.4f} (+/- {np.std(fold_metrics_rf['precision']):.4f})\")\n",
    "print(f\"Average Validation Recall: {np.mean(fold_metrics_rf['recall']):.4f} (+/- {np.std(fold_metrics_rf['recall']):.4f})\")\n",
    "\n",
    "# --- [2.4] Final Model Training on ALL Data ---\n",
    "print(f\"\\n{'='*20} TRAINING FINAL RF MODEL ON ALL DATA {'='*20}\")\n",
    "final_model_rf = create_rf_model()\n",
    "final_model_rf.fit(X_full_train_rf, y_full_train_rf)\n",
    "\n",
    "# --- [2.5] Final Evaluation on the Hold-Out Test Set ---\n",
    "print(f\"\\n{'='*20} FINAL RF EVALUATION ON TEST SET {'='*20}\")\n",
    "y_pred_rf = final_model_rf.predict(X_test_rf)\n",
    "y_true_rf = y_test_rf # Test labels are already 1D\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_rf, y_pred_rf, target_names=CLASS_NAMES, zero_division=0))\n",
    "plot_confusion_matrix(y_true_rf, y_pred_rf, \"RandomForest\")\n",
    "\n",
    "# --- [2.6] Saving the Final Model ---\n",
    "model_path_rf = os.path.join(OUTPUT_DIR, \"model_rf_final.joblib\")\n",
    "joblib.dump(final_model_rf, model_path_rf)\n",
    "print(f\"✅ Final RandomForest Model has been saved to: {model_path_rf}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# --- [PART 3] 1D-CNN MODEL TRAINING & 10-FOLD CROSS-VALIDATION ---\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*23 + \" 1D-CNN MODEL TRAINING \" + \"=\"*23)\n",
    "\n",
    "# --- [3.1] Prepare Full Dataset for Cross-Validation ---\n",
    "# We use the raw, non-resampled data for the CNN and apply class weights\n",
    "X_full_train_cnn = np.concatenate((X_train_cnn, X_val_cnn), axis=0)\n",
    "y_full_train_cnn_cat = np.concatenate((y_train_cnn, y_val_cnn), axis=0)\n",
    "y_full_train_cnn = np.argmax(y_full_train_cnn_cat, axis=1) # 1D labels for splitting\n",
    "\n",
    "input_shape_cnn = (X_full_train_cnn.shape[1], X_full_train_cnn.shape[2])\n",
    "output_dim_cnn = y_full_train_cnn_cat.shape[1]\n",
    "\n",
    "# --- [3.2] 10-Fold Cross-Validation ---\n",
    "skf_cnn = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "fold_metrics_cnn = {'loss': [], 'f1_score': [], 'precision': [], 'recall': []}\n",
    "\n",
    "print(f\"\\n--- Starting {N_SPLITS}-Fold Cross-Validation for 1D-CNN ---\")\n",
    "for fold, (train_index, val_index) in enumerate(skf_cnn.split(X_full_train_cnn, y_full_train_cnn)):\n",
    "    print(f\"\\n--- FOLD {fold + 1}/{N_SPLITS} ---\")\n",
    "\n",
    "    # Split data for the current fold\n",
    "    X_train_fold, X_val_fold = X_full_train_cnn[train_index], X_full_train_cnn[val_index]\n",
    "    y_train_fold_cat, y_val_fold_cat = y_full_train_cnn_cat[train_index], y_full_train_cnn_cat[val_index]\n",
    "    \n",
    "    # NOTE: We are now using the pre-calculated global class weights\n",
    "    # No need to calculate weights inside the loop anymore.\n",
    "\n",
    "    # Create a new model instance for each fold\n",
    "    model_cnn = create_cnn_model(input_shape=input_shape_cnn, output_dim=output_dim_cnn)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training with pre-calculated class weights...\")\n",
    "    model_cnn.fit(\n",
    "        X_train_fold, y_train_fold_cat,\n",
    "        epochs=150,\n",
    "        batch_size=100,\n",
    "        verbose=0,\n",
    "        validation_data=(X_val_fold, y_val_fold_cat),\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "\n",
    "    # Evaluate the fold\n",
    "    results_cnn = model_cnn.evaluate(X_val_fold, y_val_fold_cat, verbose=0)\n",
    "    fold_metrics_cnn['loss'].append(results_cnn[0])\n",
    "    fold_metrics_cnn['precision'].append(results_cnn[1])\n",
    "    fold_metrics_cnn['recall'].append(results_cnn[2])\n",
    "    fold_metrics_cnn['f1_score'].append(results_cnn[3])\n",
    "    \n",
    "    print(f\"Fold {fold + 1} - Val Loss: {results_cnn[0]:.4f}, Val F1-Score: {results_cnn[3]:.4f}\")\n",
    "\n",
    "# --- [3.3] Average Cross-Validation Results ---\n",
    "print(f\"\\n{'='*20} 1D-CNN CV SUMMARY {'='*20}\")\n",
    "print(f\"Average Validation Loss: {np.mean(fold_metrics_cnn['loss']):.4f} (+/- {np.std(fold_metrics_cnn['loss']):.4f})\")\n",
    "print(f\"Average Validation F1-Score: {np.mean(fold_metrics_cnn['f1_score']):.4f} (+/- {np.std(fold_metrics_cnn['f1_score']):.4f})\")\n",
    "print(f\"Average Validation Precision: {np.mean(fold_metrics_cnn['precision']):.4f} (+/- {np.std(fold_metrics_cnn['precision']):.4f})\")\n",
    "print(f\"Average Validation Recall: {np.mean(fold_metrics_cnn['recall']):.4f} (+/- {np.std(fold_metrics_cnn['recall']):.4f})\")\n",
    "\n",
    "# --- [3.4] Final Model Training on ALL Data ---\n",
    "print(f\"\\n{'='*20} TRAINING FINAL 1D-CNN MODEL ON ALL DATA {'='*20}\")\n",
    "# We use the same globally calculated weights for the final training.\n",
    "print(\"Using Final Model Weights:\", class_weights_dict)\n",
    "\n",
    "final_model_cnn = create_cnn_model(input_shape=input_shape_cnn, output_dim=output_dim_cnn)\n",
    "final_model_cnn.fit(\n",
    "    X_full_train_cnn, y_full_train_cnn_cat,\n",
    "    epochs=150,\n",
    "    batch_size=100,\n",
    "    verbose=1,\n",
    "    class_weight=class_weights_dict\n",
    ")\n",
    "\n",
    "# --- [3.5] Final Evaluation on the Hold-Out Test Set ---\n",
    "print(f\"\\n{'='*20} FINAL 1D-CNN EVALUATION ON TEST SET {'='*20}\")\n",
    "y_pred_cnn_raw = final_model_cnn.predict(X_test_cnn)\n",
    "y_pred_cnn = np.argmax(y_pred_cnn_raw, axis=1)\n",
    "y_true_cnn = np.argmax(y_test_cnn, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_cnn, y_pred_cnn, target_names=CLASS_NAMES, zero_division=0))\n",
    "plot_confusion_matrix(y_true_cnn, y_pred_cnn, \"1D-CNN\")\n",
    "\n",
    "# --- [3.6] Saving the Final Model ---\n",
    "model_path_cnn = os.path.join(OUTPUT_DIR, \"model_1d-cnn_final.keras\")\n",
    "final_model_cnn.save(model_path_cnn)\n",
    "print(f\"✅ Final 1D-CNN Model has been saved to: {model_path_cnn}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
